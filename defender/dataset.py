import argparse
from collections import defaultdict
import pickle
import random
from pathlib import Path
from typing import List, Dict, Tuple, Union, Optional
import os
import warnings
import numpy as np
import torch
import pathlib
from torch.utils.data import random_split
import pandas as pd
import glob
from tqdm.auto import trange, tqdm
from scipy.sparse import csr_matrix
from defender.models import ember

from defender.models.feature_extractor import FeatureExtractor, Features
from defender.models.malware_torch import get_nonzero_idx
from defender.utils import load_pickle, save_pickle, split_arrays
from torch.utils.data import Dataset


def get_validation_data(gw_samples, mw_samples, *args,**kwargs):
    datasets = []
    for t, n in (('gw', np.arange(1,gw_samples+1)),('mw', np.arange(1,mw_samples+1))):
        for i in n:
            datasets.append(PEDataset(
                folders=[f"{t}{i}"],
                labels=[1 if t=='mw' else 0],
                *args,**kwargs
            ))
    return datasets


class PEDataset(Dataset):
    def __init__(self, data_path, extractor, use_features, textual_idx, folders=['gw', 'mw'], labels=[0,1], cache=False):
        self.tmp_file = f"tmp_calc/torch_{folders}_test"

        if cache and os.path.exists(self.tmp_file):
            warnings.warn("Loading from cache...")
            self.filenames, self.y = load_pickle(self.tmp_file)
            return

        x = []
        y = []
        for t in zip(folders, labels):
            files = list(glob.glob(os.path.join(data_path, f"{t[0]}*", "**", '*'), recursive=True))
            files = [f for f in files if os.path.isfile(f)]
            y.append(np.ones(len(files)) * t[1])
            x.extend(files)

        self.y=np.concatenate(y, axis=0)
        x, non_parsed = extractor(x, enable_extractor=use_features, train=False)
        x = [k.toarray() for k in x]
        # remove unparsed labels
        correct = ~np.isin(np.arange(self.y.shape[0]), non_parsed)
        self.y = self.y[correct]

        # check no nan
        assert np.sum([np.isnan(k).sum() for k in x]) == 0
        # check that shapes match
        assert len(self.y) == len(x[0]), "Shapes do not match"
        # check that no unlabeled data
        assert np.logical_and(self.y != 0, self.y !=1).sum() == 0, "Unknown labels, it should be 0 or 1"

        for idx, f_idx in enumerate(use_features):
            x[idx] = torch.from_numpy(x[idx])
            if f_idx not in textual_idx:
                continue
            x[idx] = get_nonzero_idx(x[idx])

        self.filenames = []
        for i in trange(len(self.y)):
            f = f"{self.tmp_file}_{i}"
            v = [k[i] for k in x]
            torch.save(v, f)
            self.filenames.append(f)
        
        save_pickle((self.filenames, self.y), self.tmp_file)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, index):
        return [k.float() for k in torch.load(self.filenames[index])], self.y[index]


def save_files(X, path):
    filenames = []
    for i in trange(X.shape[0]):
        f = f"{path}_{i}"
        v = torch.from_numpy(X[i,...])
        if v.isnan().sum() > 0:
            raise Exception("Input nan, no implemented solutions")
        torch.save(v, f)
        filenames.append(f)
    return filenames


def get_datasets(paths, seed, lengths=[0.8, 0.2], use_features=None, use_ember=False, use_bodmas=False, textual_idx=[]):
    self_x = []
    self_y = []
    indices = {}
    # max_nonzero = defaultdict(lambda: 0)
    max_nonzero = defaultdict(int)
    vocab_dim = {}
    for idx_path, path in enumerate(paths):
        SparseDataset.version += 1
        self_tmp_file = f"{SparseDataset.base_tmp_file}_{idx_path}"
        # load data
        list_sparse_x, y = load_pickle(path)
        if isinstance(y, list):
            y = y[0]
        # save to files to not fill memory
        for idx,sparse_x in enumerate(list_sparse_x):
            if use_features is not None and idx not in use_features:
                continue
            to_sparse = idx in textual_idx

            if idx_path == 0:
                indices[idx] = len(self_x)
                self_x.append([])

            filenames = []
            for i in trange(sparse_x.shape[0]):
                f = f"{self_tmp_file}_{idx}_{i}"
                v = torch.from_numpy(sparse_x[i,...].toarray()[0])
                if v.isnan().any():
                    raise Exception("Input nan, no implemented solutions")
                if to_sparse:
                    vocab_dim[idx] = v.shape[0]
                    v = (v>0).nonzero()[:,0] + 1
                    max_nonzero[idx] = max(len(v), max_nonzero[idx])
                torch.save(v, f)
                filenames.append(f)

            self_x[indices[idx]].extend(filenames)
        self_y.append(y)

    # load additional data
    if use_bodmas:
        assert use_features == [0]
        if len(self_x) == 0:
            indices[0] = len(self_x)
            self_x.append([])

        filenames = []

        # bodmas dataset
        filename = 'data/bodmas/bodmas.npz'
        data = np.load(filename)
        X = data['X']  # all the feature vectors
        y = data['y']  # labels, 0 as benign, 1 as malicious
        self_y.append(y)
        del y
        del data

        # save to files
        self_x[indices[0]].extend(save_files(X, f"{SparseDataset.base_tmp_file}_bodmas"))
        del X
    
    if use_ember:
        assert use_features == [0]
        if len(self_x) == 0:
            indices[0] = len(self_x)
            self_x.append([])

        X_train, y_train, X_test, y_test = ember.read_vectorized_features("data/ember2018/")
        self_y.append(y_train)
        self_y.append(y_test)
        del y_train
        del y_test
        
        # save to files
        self_x[indices[0]].extend(save_files(X_train, f"{SparseDataset.base_tmp_file}_emberTrain"))
        del X_train
        self_x[indices[0]].extend(save_files(X_test, f"{SparseDataset.base_tmp_file}_emberTest"))
        del X_test
    self_y = np.concatenate(self_y, axis=0)

    # Filter unlabeled data
    train_rows = (self_y != -1)
    self_y = self_y[train_rows]
    self_x = [np.asarray(x)[train_rows] for x in self_x]

    # # check no nan
    # assert torch.sum([np.isnan(k).sum() for k in self_x]) == 0
    # check that shapes match
    assert len(self_y) == len(self_x[0]), "Shapes do not match"
    # check that no unlabeled data
    assert np.logical_and(self_y != 0, self_y !=1).sum() == 0, "Unknown labels, it should be 0 or 1"

    # divide into train and validation
    xy1, xy2 = split_arrays(self_x + [self_y], lengths, seed=seed)

    return SparseDataset(xy1[:-1], xy1[-1], max_nonzero,vocab_dim, use_features), SparseDataset(xy2[:-1], xy2[-1], max_nonzero,vocab_dim, use_features)

class SparseDataset(Dataset):
    base_tmp_file = f"tmp_calc/torch_tmp_x"
    version = -1

    def __init__(self, x, y, max_nonzero, vocab_dim, use_features):
        self.x = x
        self.targets = y
        self.max_nonzero = max_nonzero
        self.vocab_dim = vocab_dim
        self.use_features = use_features

    def __len__(self):
        return len(self.x[0])

    def feature_dims(self):
        r = [torch.load(x[0]).shape[0] for x in self.x]
        r = [k if idx not in self.vocab_dim else self.vocab_dim[idx] for k,idx in zip(r, self.use_features)]
        return r

    def __getitem__(self, index):
        # return [x[index].astype(np.float32) for x in self.x], self.targets[index]
        if self.targets[index] == 1:
            index_p = (self.targets[:index+1] == 1).sum()-1
        else:
            index_p = -(self.targets[:index+1] == 0).sum()
        x = [torch.load(x[index]).float() for x in self.x]
        x = [torch.nn.functional.pad(k, (max(0, self.max_nonzero[self.use_features[idx]] - len(k)), 0)) for idx,k in enumerate(x)]
        
        return x, self.targets[index], index_p


class DikePEDataset:
    def __init__(
        self,
        data_path = 'data/DikeDataset',
        use_large_dataset = 'data/large_dataset',
        use_windows = True,
        use_programs = True,
        extra_malware = ['data/malware'],
        extra_benign = None,
        feature_extractors: List[Features] = None,
        train_ratio=0.8,
        save_splits='data',
        seed:int = 123456,
        train_test_extractor=False,
    ):
        """
        Args:
            data_path (str, optional): path of the Dike dataset. None if not used. Defaults to 'data/DikeDataset'.
            use_windows (bool, optional): whether to include windows system files. Defaults to True.
            use_programs (bool, optional): whether to include program files. Defaults to True.
            extra_malware (list, optional): list of malware folders to include. Defaults to ['data/malware'].
            extra_benign (_type_, optional): list of benign folders to include. Defaults to None.
            feature_extractors (List[Features], optional): list of feature extractors to use. If None, a default set is used. Defaults to None.
            train_ratio (float, optional): percentage of train samples. Defaults to 0.8.
            save_splits (str, optional): where to save the extracted data. Defaults to 'data'.
            seed (int, optional): seed for randomly splitting train/test. Defaults to 123456.
            train_test_extractor (bool, optional): whether to use the same extractor for train and test. Should be False to ensure there is no leak from train set to test set. Defaults to True.
        """
        self.data_path = data_path
        self.feature_extractor = FeatureExtractor(feature_extractors)
        self.save_splits=save_splits
        self.train_ratio = train_ratio
        self.seed = seed

        y = []
        filenames = []

        if use_large_dataset is not None:
            labels = pd.read_csv(os.path.join(use_large_dataset, "samples.csv"))
            y.append((labels.positives.to_numpy()/(labels.total.to_numpy() + 1e-5)>0.5).astype(int))
            filenames.extend([os.path.join(use_large_dataset,"samples",str(k)) for k in labels.id.tolist()])

        if extra_malware is not None and len(extra_malware) > 0:
            # get extra PE files
            extra_labels = []
            for d in extra_malware:
                for f in glob.glob(f"{d}/**/*", recursive=True):
                    if os.path.isfile(f):
                        filenames.append(f)
                        extra_labels.append(1)
            y.append(np.asarray(extra_labels))

        if extra_benign is not None and len(extra_benign) > 0:
            # get extra PE files
            extra_labels = []
            for d in extra_benign:
                for f in glob.glob(f"{d}/**/*", recursive=True):
                    if os.path.isfile(f):
                        filenames.append(f)
                        extra_labels.append(0)
            y.append(np.asarray(extra_labels))
            
        # get windows system files
        if use_windows:
            print("Searching for windows system files...")
            system_files = glob.glob('/mnt/c/Windows/system32/**/*.exe', recursive=True) \
            + glob.glob('/mnt/c/Windows/system32/**/*.dll', recursive=True) \
            + glob.glob('/mnt/c/Program Files/**/*.exe', recursive=True) \
            + glob.glob('/mnt/c/Program Files/**/*.dll', recursive=True)
            y.append(np.zeros(len(system_files)))
            filenames.extend(system_files)

        # get files from ProgramFiles
        if use_programs:
            print("Searching in program files files...")
            program_files = glob.glob('/mnt/c/Program Files/**/*.exe', recursive=True) \
            + glob.glob('/mnt/c/Program Files/**/*.dll', recursive=True) \
            + glob.glob('/mnt/c/Program Files (x86)/**/*.exe', recursive=True) \
            + glob.glob('/mnt/c/Program Files (x86)/**/*.dll', recursive=True) \
            + glob.glob('/mnt/c/Drivers/**/*.exe', recursive=True) \
            + glob.glob('/mnt/c/Drivers/**/*.dll', recursive=True)
            y.append(np.zeros(len(program_files)))
            filenames.extend(program_files)
        
        if data_path is not None:
            # load labels
            benign_labels = pd.read_csv(os.path.join(data_path, "labels", "benign.csv"))
            malware_labels = pd.read_csv(os.path.join(data_path, "labels", "malware.csv"))
            
            # label as malware or benign and concat
            benign_labels['malware'] = 0
            malware_labels['malware'] = 1
            labels = pd.concat([benign_labels, malware_labels], axis=0, ignore_index=True)

            # keep only PE files
            labels = labels[labels.type == 0]
            labels.drop('type', axis=1, inplace=True)
            # set hash as index
            labels.set_index('hash', inplace=True)

            # ignore all labels except whether it is malware
            y.append(labels.malware.to_numpy())
            # get filenames from dataset and benign files from system
            filenames.extend([os.path.join(self.data_path, 'files', 'malware' if k[1].malware else 'benign', f"{k[0]}.exe") for k in labels.iterrows()])

        # concatenate all labels
        y = np.concatenate(y, axis=0)
        filenames = np.asarray(filenames)

        if train_test_extractor:
            print("Extracting features...")
            x, unparsed = self.feature_extractor(filenames, train=True, y=y)
            y = y[~np.isin(np.arange(y.shape[0]), unparsed)]
            save_pickle((x,y,unparsed), 'tmp.pkl')

            # split train and test
            splits, splits_idx = split_arrays([y], [self.train_ratio, 1-self.train_ratio], seed=self.seed, return_idx=True)
            self.y_train, self.y_test = [k for k in splits]
            self.x_train, self.x_test = [[k[spl] for k in x] for spl in splits_idx]
        else:
            # split train and test
            splits, splits_idx = split_arrays([y], [self.train_ratio, 1-self.train_ratio], seed=self.seed, return_idx=True)
            self.y_train, self.y_test = [k[0] for k in splits]

            # extract features, remove unparsed, and save to make to save computation
            print("Extracting features...")
            self.x_train, unparsed = self.feature_extractor(filenames[splits_idx[0]], train=True, y=self.y_train)
            self.y_train = self.y_train[~np.isin(np.arange(self.y_train.shape[0]), unparsed)]
            self.x_test, unparsed = self.feature_extractor(filenames[splits_idx[1]], train=False)
            self.y_test = self.y_test[~np.isin(np.arange(self.y_test.shape[0]), unparsed)]

        ## save x,y
        self._save()
        
    def _save(self):
        names = ['train', 'test']
        # save feature extractor
        save_pickle(self.feature_extractor, f"{self.save_splits}_feature_extractor.pkl")

        datasets = []
        for d, name in zip([(self.x_train, self.y_train), (self.x_test, self.y_test)], names):
            if self.save_splits is not None:
                save_pickle(d, f"{self.save_splits}_{name}.pkl")
            datasets.append(d)

            # _, counts = np.unique(d[1], return_counts=True)
            # ratio = counts / counts.sum()
            # print(f"{name}:: Benign: {counts[0]}/{ratio[0]}%    Malware: {counts[1]}/{ratio[1]}%")

        return datasets


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-s', '--save', type=str, default='data', help='Saving directory')
    parser.add_argument('--dike', action='store_true')
    parser.add_argument('--large_dataset', type=str, default=None)
    parser.add_argument('--windows', action='store_true')
    parser.add_argument('--programs', action='store_true')
    parser.add_argument('--malware', type=str, nargs='+', default=[])
    parser.add_argument('--benign', type=str, nargs='+', default=[])
    parser.add_argument('--train_ratio', type=float, default=0.8)
    parser.add_argument('--combined', action='store_true')
    args = parser.parse_args()

    dataset = DikePEDataset(
        data_path = 'data/DikeDataset' if args.dike else None,
        use_windows = args.windows,
        use_large_dataset=args.large_dataset,
        use_programs = args.programs,
        extra_malware = args.malware,
        extra_benign = args.benign,
        train_ratio=args.train_ratio,
        save_splits=args.save,
        train_test_extractor=args.combined,
    )
