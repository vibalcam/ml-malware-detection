from typing import Any, List
import warnings
from sklearn.metrics import confusion_matrix, matthews_corrcoef, f1_score, roc_auc_score
import numpy as np
import torch
from defender.utils import save_pickle, load_pickle
import torchmetrics as metrics

from defender.models.feature_extractor import FeatureExtractor
from torchmetrics.functional.classification import binary_roc
from torchmetrics import Metric


def find_best_score(
        y_true, 
        y_pred, 
        return_threshold=False,
        return_full_scores=False,
        max_fpr=0.01, 
        min_tpr=0.95,
        n_tries=100,
    ):
    # thresholds = np.linspace(0, 100, n_tries+1)/100
    thresholds = np.unique(y_pred) # get the unique probabilities
    thresholds = thresholds[~np.isnan(thresholds)] # remove nan
    thresholds = np.insert(thresholds, 0, 0) # add 0 as the lower bound
    thresholds = np.append(thresholds, 1) # add 1 as the upper bound
    scores = [
        competition_scorer(
            y_true=y_true, 
            y_pred=y_pred,
            threshold=k,
            possible_thresholds=thresholds,
            max_fpr=max_fpr, 
            min_tpr=min_tpr,
        ) for k in thresholds
    ]
    best_idx = np.argmax([k[0] for k in scores])
    best_threshold = thresholds[best_idx]
    best_score = scores[best_idx][0]
    
    ret = [best_score]
    if return_threshold:
        ret += [best_threshold]
    if return_full_scores:
        ret += [scores[best_idx]]

    if len(ret)==1:
        return ret[0]
    return ret


def prediction_metrics(y_true, y_pred, epsilon=1e-5):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    tn = np.sum((y_true == 0) & (y_pred == 0))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    metric_score = matthews_corrcoef(y_true=y_true, y_pred=y_pred)
    f1 = f1_score(y_true=y_true, y_pred=y_pred)
    fpr = fp / (fp + tn + epsilon)
    tpr = tp / (tp + fn + epsilon)

    return metric_score, f1, fpr, tpr


def competition_scorer(y_true, y_pred, threshold, possible_thresholds, max_fpr = 0.01, min_tpr=0.95):
    '''
    Returns the challenge score, metric, f1-score, auc, fpr, tpr

    `y_pred` are the probabilities of class 1

    The default metric used is MCC

    The challenge score is the metric score if the `max_fpr` and `min_tpr` are accomplished
    '''
    # auc = roc_auc_score(y_true=y_true, y_score=y_pred, max_fpr=max_fpr)
    auc = metrics.functional.classification.binary_auroc(preds=torch.from_numpy(y_pred).float(), target=torch.from_numpy(y_true).int(), max_fpr=max_fpr, thresholds=torch.from_numpy(possible_thresholds).float())

    y_pred = (y_pred>=threshold).astype(int)
    metric_score, f1, fpr, tpr = prediction_metrics(y_true, y_pred)

    if fpr > max_fpr:
        score = ((tpr - min_tpr) + (max_fpr - fpr)) / (1 - min_tpr + max_fpr)
    else:
        score = (tpr - min_tpr) / (1 - min_tpr)
    
    return (score, metric_score, f1, auc, fpr, tpr)


class ThresholdSelector:
    # def __init__(self, threshold_range=None):
    #     self.threshold_range = threshold_range if threshold_range is not None else np.linspace(1, 10, 101)/100

    @torch.no_grad()
    def __call__(self, model, val_loader, n_tries=100):
        # Evaluate model on validation set
        model.eval()
        y_true = []
        y_pred = []
        with torch.no_grad():
            for batch in val_loader:
                x,y = batch[0], batch[1]
                y_hat = model(x)
                y_true.append(y)
                y_pred.append(torch.sigmoid(y_hat))
        y_true = torch.cat(y_true, dim=0)
        y_pred = torch.cat(y_pred, dim=0)

        # best_score, best_threshold = find_best_score(
        #     y_true=y_true.numpy(), 
        #     y_pred=y_pred.numpy(), 
        #     return_threshold=True,
        #     n_tries=n_tries,
        #     # max_fpr=0.02,
        # )

        fpr, tpr, thresholds = binary_roc(y_pred.float(), y_true.int())
        idx = (fpr < 0.02).nonzero()[-1]
        best_threshold = thresholds[idx]
        best_score = tpr[idx]

        return best_score, best_threshold


class Model:
    def __init__(self, *args, **kwargs) -> None:
        self.best_threshold = 0.5
        self._train = True
        warnings.warn(f"Set train to True for training and False for inference")

    @property
    def train(self):
        return self._train

    @train.setter
    def train(self,v):
        self._train = v

    def extract_features(self, pe_files: List[str]) -> List[np.array]:
        '''
        Given a list of pe_files (n_samples), returns a list of matrices of features (n_samples, features)
        '''
        raise NotImplementedError("Extract features needs to be implemented")

    def __call__(self, x: List[np.array]) -> np.array:
        '''
        Given a list of matrices of features `x`, returns the probabilities of being malware
        '''
        raise NotImplementedError("Call/predict_proba needs to be implemented")
        
    def predict(self, x: List[np.array]):
        '''
        Given a list of matrices of features `x`, returns the predictions (1 malware, 0 benign)

        By default it calls `self(x)` and then compares the probabilities with `self.best_threshold`
        '''
        p = self(x)
        return (p >= self.best_threshold).astype(int)
    
    def predict_files(self, pe_files: List[str], get_probs=False) -> np.array:
        '''
        Given a list of pe_files, returns the predictions (1 malware, 0 benign)

        By default it calls `extract features` and then `predict`
        '''
        features = self.extract_features(pe_files)
        if get_probs:
            return self(features)
        else:
            return self.predict(features)
    
    def fit(self, *args: Any, **kwds: Any):
        '''
        Trains a model given the `x,y` data
        '''
        if not self.train:
            raise Exception("Not in training mode: set train to True")
    
    def evaluate(self, x, y, needs_extractor=False, *args, **kwargs):
        '''
        Evaluates the model on the given `x,y` data.

        If `needs_extractor` is true, then `x` is a list of pe_files,
            otherwise, `x` is a list of feature matrices
        '''
        self.train = False

        if needs_extractor:
            x = self.extract_features(x)
        y_pred = self(x)

        self.last_evaluation = competition_scorer(y_true=y, y_pred=y_pred, threshold=self.best_threshold)
        return self.last_evaluation
    
    def save(self, path):
        save_pickle((self.__class__, self.__dict__), path)

    @staticmethod
    def _init_parameters(d):
        '''
        Used for the default loading
        
        Returns a dictionary of parameters used to initialize models while loading
        '''
        return {}
    
    @staticmethod
    def load(path, cls=None):
        if cls is not None:
            return cls.load(path, cls)

        cls, model_dict = load_pickle(path)
        model = cls.__new__(cls)
        model.__init__(**cls._init_parameters(model_dict))
        model.__dict__.update(model_dict)
        model.train = False
        return model
