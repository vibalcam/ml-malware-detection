from typing import Any, List
import warnings
from sklearn.metrics import confusion_matrix, matthews_corrcoef, f1_score, roc_auc_score
import numpy as np
from defender.utils import save_pickle, load_pickle

from defender.models.feature_extractor import FeatureExtractor


def find_best_score(
        y_true, 
        y_pred, 
        return_threshold=False,
        metric=matthews_corrcoef, 
        max_fpr = 0.01, 
        min_tpr=0.95
    ):
    scores = []
    thresholds = np.linspace(1, 10, 101)/100
    for k in thresholds:
        score = competition_scorer(
            y_true=y_true, 
            y_pred=y_pred,
            threshold=k,
            metric=metric, 
            max_fpr=max_fpr, 
            min_tpr=min_tpr,
        )[0]
        scores.append(score)
    best_idx = np.argmax(scores)
    best_threshold = thresholds[best_idx]
    best_score = scores[best_idx]
    return (best_score, best_threshold) if return_threshold else best_score


def competition_scorer(y_true, y_pred, threshold, metric=matthews_corrcoef, max_fpr = 0.01, min_tpr=0.95):
    '''
    Returns the challenge score, metric, f1-score, auc, fpr, tpr

    `y_pred` are the probabilities of class 1

    The default metric used is MCC

    The challenge score is the metric score if the `max_fpr` and `min_tpr` are accomplished
    '''
    auc = roc_auc_score(y_true=y_true, y_score=y_pred)

    y_pred = (y_pred>=threshold).astype(int)

    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    metric_score = metric(y_true=y_true, y_pred=y_pred)
    f1 = f1_score(y_true=y_true, y_pred=y_pred)
    fpr = fp / (fp + tn)
    tpr = tp / (tp + fn)

    if fpr > max_fpr or tpr < min_tpr:
        score = 0.0
    else:
        score = metric_score

    return (score, metric_score, f1, auc, fpr, tpr)


class Model:
    def __init__(self, *args, **kwargs) -> None:
        self.best_threshold = 0.5
        self.train = True
        warnings.warn(f"Set train to True for training and False for inference")

    def extract_features(self, pe_files: List[str]) -> List[np.array]:
        '''
        Given a list of pe_files (n_samples), returns a list of matrices of features (n_samples, features)
        '''
        raise NotImplementedError("Extract features needs to be implemented")

    def __call__(self, x: List[np.array]) -> np.array:
        '''
        Given a list of matrices of features `x`, returns the probabilities of being malware
        '''
        raise NotImplementedError("Call/predict_proba needs to be implemented")
        
    def predict(self, x: List[np.array]):
        '''
        Given a list of matrices of features `x`, returns the predictions (1 malware, 0 benign)

        By default it calls `self(x)` and then compares the probabilities with `self.best_threshold`
        '''
        p = self(x)
        return (p >= self.best_threshold).astype(int)
    
    def predict_files(self, pe_files: List[str]) -> np.array:
        '''
        Given a list of pe_files, returns the predictions (1 malware, 0 benign)

        By default it calls `extract features` and then `predict`
        '''
        features = self.extract_features(pe_files)
        return self.predict(features)
    
    def fit(self, *args: Any, **kwds: Any):
        '''
        Trains a model given the `x,y` data
        '''
        if not self.train:
            raise Exception("Not in training mode: set train to True")
    
    def evaluate(self, x, y, needs_extractor=False, *args, **kwargs):
        '''
        Evaluates the model on the given `x,y` data.

        If `needs_extractor` is true, then `x` is a list of pe_files,
            otherwise, `x` is a list of feature matrices
        '''
        self.train = False

        if needs_extractor:
            x = self.extract_features(x)
        y_pred = self(x)

        self.last_evaluation = competition_scorer(y_true=y, y_pred=y_pred, threshold=self.best_threshold)
        return self.last_evaluation
    
    def save(self, path):
        save_pickle((self.__class__, self.__dict__), path)

    @staticmethod
    def _init_parameters(d):
        '''
        Used for the default loading
        
        Returns a dictionary of parameters used to initialize models while loading
        '''
        return {}
    
    @staticmethod
    def load(path, cls=None):
        cls_loaded, model_dict = load_pickle(path)
        if cls is None:
            cls = cls_loaded
        model = cls.__new__(cls)
        model.__init__(**cls._init_parameters(model_dict))
        model.__dict__.update(model_dict)
        model.train = False
        return model
