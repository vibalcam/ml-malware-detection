from typing import Callable, List, Union
import pandas as pd
import warnings
from tqdm.auto import tqdm 
import numpy as np
from sklearn.decomposition import IncrementalPCA
from scipy.sparse import csr_matrix
from scipy import sparse

from defender.models.ember import PEFeatureExtractor
from defender.models.mlsec import PEAttributeExtractor
from defender.utils import load_pickle, save_pickle
import lief
from sklearn.feature_extraction.text import TfidfVectorizer

'''
https://github.com/erocarrera/pefile/blob/wiki/UsageExamples.md#introduction
Retrieving the bytes at the entry point and analyze
'''


def get_raw_bytes(filepath):
    with open(filepath, 'rb') as infile:
        return infile.read()


class Features:
    def __init__(self) -> None:
        self.version = 1
        
    def __call__(self, pe_filenames, train:bool=True, **kwargs) -> List:
        '''
        Extracts a list of feature matrices from the given PE files, which are given by filename or bytes

        If file `i` is not able to be parsed, the `i` will be added to the returned list and the ith row of the sparse matrices will be invalid

        Returns the list of sparse feature matrices and a list of invalid row indices
        '''
        raise NotImplementedError()
    
    def output_length(self):
        return NotImplementedError()


class FeatureExtractor(Features):
    tmp_file = 'tmp_calc/features'

    def __init__(self, feature_extractors: List[Features] = None):
        if feature_extractors is None:
            feature_extractors = [
                # EmberFeature(),
                MLSecFeature(),
                BasicFeatures(),
                TfIdfFuncs(only_malware=True),
                # TfIdfFuncs(),
            ]
        self.feature_extractors = feature_extractors

    def __call__(self, pe_filenames, enable_extractor=None, train=True, **kwargs):
        features = []
        non_parsed = set()
        # run each feature extractor
        idx_out = 0
        if enable_extractor is not None:
            enable_extractor = np.asarray(enable_extractor.copy())
            enable_extractor.sort()
        else:
            enable_extractor = np.arange(np.sum([k.output_length() for k in self.feature_extractors]))
        for f in self.feature_extractors:
            # check if extractor is disabled
            if enable_extractor is not None and idx_out not in enable_extractor:
                enable_extractor[idx_out:] -= 1
                continue
            feat, idx_err = f(pe_filenames, train, **kwargs)
            non_parsed.update(idx_err)
            # features.extend(feat)
            v = np.arange(idx_out, idx_out+f.output_length())
            for k in v[np.in1d(v, enable_extractor)]:
                file = f"{self.tmp_file}_{k}"
                save_pickle(feat[k-idx_out], file)
                features.append(file)
                # features.append(feat[k-idx_out])
            idx_out += f.output_length()

        non_parsed = list(non_parsed)
        if len(non_parsed) > 0:
            warnings.warn(f"Total {len(non_parsed)} parsing errors for {non_parsed}")
        # remove unparsed and return list
        x = []
        for f in features:
            k = load_pickle(f)
            correct = ~np.isin(np.arange(k.shape[0]), non_parsed)
            x.append(k[correct])
        return x, non_parsed
        # return [k[~np.isin(np.arange(k.shape[0]), non_parsed)] for k in features], non_parsed


class EmberFeature(Features):
    def __init__(self) -> None:
        super().__init__()
        self.extractor = PEFeatureExtractor()

    def __call__(self, pe_filenames, train: bool = True, **kwargs) -> List:
        data_list = []
        non_parsed = []
        for idx,f in tqdm(enumerate(pe_filenames), desc="Ember"):
            if isinstance(f, str):
                bytez = get_raw_bytes(f)
            else:
                bytez = f

            features = self.extractor.feature_vector(bytez)
            if np.isnan(features).any():
                # if not able to parse, set to empty so it will be returned as nan
                data_list.append(csr_matrix(np.full((1,2381), np.nan)))
                # log
                non_parsed.append(idx)
                warnings.warn(f"{idx}: Binary {f} not able to parse")
                continue  
            
            data_list.append(csr_matrix(features))

        return [sparse.vstack(data_list)], non_parsed
    
    def output_length(self):
        return 1


class MLSecFeature(Features):
    # numerical attributes
    NUMERICAL_ATTRIBUTES = [
        #'string_paths', 'string_urls', 'string_registry', 'string_MZ', 'size',
        'virtual_size', 'has_debug', 'imports', 'exports', 'has_relocations',
        'has_resources', 'has_signature', 'has_tls', 'symbols', 'timestamp', 
        'numberof_sections', 'major_image_version', 'minor_image_version', 
        'major_linker_version', 'minor_linker_version', 'major_operating_system_version',
        'minor_operating_system_version', 'major_subsystem_version', 
        'minor_subsystem_version', 'sizeof_code', 'sizeof_headers', 'sizeof_heap_commit',
    ]

    # categorical attributes
    CATEGORICAL_ATTRIBUTES = [
        'machine', 'magic',
    ]

    # textual attributes
    TEXTUAL_ATTRIBUTES = [
        'libraries', 'functions', 'exports_list',
        'dll_characteristics_list', 'characteristics_list',
    ]
    
    def __call__(self, pe_filenames, train: bool = True, **kwargs) -> List:
        data_list = []
        non_parsed = []
        for idx,f in tqdm(enumerate(pe_filenames), desc="MLSec"):
            try:
                if isinstance(f, str):
                    bytez = get_raw_bytes(f)
                else:
                    bytez = f

                # initialize feature extractor with bytez
                pe_att_ext = PEAttributeExtractor(bytez)
                # extract PE attributes
                atts = pe_att_ext.extract()

                atts = csr_matrix(pd.DataFrame([atts])[self.NUMERICAL_ATTRIBUTES].values)

                data_list.append(atts)
            except Exception as e:
                # if not able to parse, set to empty so it will be returned as nan
                # data_list.append({})
                data_list.append(csr_matrix(np.full((1,22), np.nan)))
                # log
                non_parsed.append(idx)
                warnings.warn(f"{idx}: Binary {f} not able to parse")
                continue  

        # data = pd.DataFrame(data_list)
        # ret = [data[k] for k in [
        #     self.NUMERICAL_ATTRIBUTES,
        #     self.CATEGORICAL_ATTRIBUTES,
        #     self.TEXTUAL_ATTRIBUTES,
        # ]]
        # ret[0] = csr_matrix(ret[0].values)

        # return ret, non_parsed

        return [sparse.vstack(data_list)], non_parsed

    def output_length(self):
        # return 3
        return 1


class TfIdfFuncs(Features):
    def __init__(self, only_malware=True, decode_error='ignore') -> None:
        self.vectorizer_imports = None
        self.only_malware = only_malware
        self.decode_error = decode_error

    def _tokenizer(self, x):
        return x.split(' ')
    
    # @property
    # def decode_error(self):
    #     return 'ignore'

    def __call__(self, pe_filenames: List[str], train=True, y=None, **kwargs) -> List:
        if not train and self.vectorizer_imports is None:
            raise Exception("Have to first call train")
        
        if train and self.only_malware and y is not None:
            # sort with malware (1) first
            t = y * (-1)
            indices = t.argsort()
        else:
            # labels are not used
            y = None
            indices = np.arange(len(pe_filenames))

        # List to store the imported function lists for each PE file
        func_matrix = []
        library_matrix = []
        sec_matrix = []

        # only used when limit to malware
        func_set = set()
        library_set = set()
        sec_set = set()

        # Iterate through the PE files and get the list of imported functions for each file
        non_parsed = []
        for idx in tqdm(indices, desc=f"TFIDF"):
            f = pe_filenames[idx]

            if isinstance(f, str):
                binary = lief.PE.parse(f)
            else:
                binary = lief.PE.parse(list(f))

            # if unable to parse ignore
            if binary is None:
                func_matrix.append([])
                library_matrix.append([])
                sec_matrix.append([])
                non_parsed.append(idx)
                warnings.warn(f"{idx}: Binary {f} not able to parse")
                continue

            # get list of imported libraries and functions
            func_list = []
            library_list = []
            for imported_library in binary.imports:
                library_list.append(imported_library.name)
                for func in imported_library.entries:
                    func_list.append(f"{imported_library.name}/{func.name}")

            sec_list = []
            # Iterate through the sections and print their information
            for section in binary.sections:
                sec_list.append(section.name)

            # if malware, add to set of checked
            # since sorted, all malware is seen first
            if y is not None:
                if y[idx] == 1:
                    # add to set
                    func_set.update(func_list)
                    library_set.update(library_list)
                    sec_set.update(sec_list)
                else:
                    func_list = [k for k in func_list if k in func_set]
                    library_list = [k for k in library_list if k in library_set]
                    sec_list = [k for k in sec_list if k in sec_set]
                
            library_matrix.append(library_list)
            func_matrix.append(func_list)
            sec_matrix.append(sec_list)
        
        if train:
            # Create a TF-IDF vectorizer
            self.vectorizer_imports = TfidfVectorizer(lowercase=False, tokenizer=self._tokenizer, decode_error=self.decode_error)
            self.vectorizer_libs = TfidfVectorizer(lowercase=False, tokenizer=self._tokenizer, decode_error=self.decode_error)
            self.vectorizer_sect = TfidfVectorizer(lowercase=False, tokenizer=self._tokenizer, decode_error=self.decode_error)

            # Calculate the TF-IDF scores for the imported functions across all the documents
            tf_idf_imports = self.vectorizer_imports.fit_transform([" ".join(doc) for doc in func_matrix])
            tf_idf_libs = self.vectorizer_libs.fit_transform([" ".join(doc) for doc in library_matrix])
            tf_idf_sect = self.vectorizer_sect.fit_transform([" ".join(doc) for doc in sec_matrix])
        else:

            self.vectorizer_imports.decode_error = self.decode_error if hasattr(self, 'decode_error') else 'ignore'
            self.vectorizer_libs.decode_error = self.decode_error if hasattr(self, 'decode_error') else 'ignore'
            self.vectorizer_sect.decode_error = self.decode_error if hasattr(self, 'decode_error') else 'ignore'
            
            tf_idf_imports = self.vectorizer_imports.transform([" ".join(doc) for doc in func_matrix])
            tf_idf_libs = self.vectorizer_libs.transform([" ".join(doc) for doc in library_matrix])
            tf_idf_sect = self.vectorizer_sect.transform([" ".join(doc) for doc in sec_matrix])

        return [tf_idf_sect, tf_idf_libs, tf_idf_imports], non_parsed
    
    def output_length(self):
        return 3


class BasicFeatures(Features):
    def __init__(self) -> None:
        super().__init__()
        self.version = 2

    def __call__(self, pe_filenames, train=True, **kwargs) -> List:
        import pefile
        # https://github.com/amauricio/sklearn-antimalware/blob/master/test.py

        data_list = []
        non_parsed = []
        for idx,f in tqdm(enumerate(pe_filenames), desc="Basic"):
            try:
                if isinstance(f, str):
                    pe = pefile.PE(f)
                else:
                    pe = pefile.PE(data=f)
            except Exception as e:
                warnings.warn(str(e))
                # if not able to parse, set to empty so it will be returned as nan
                data_list.append({})
                # log
                non_parsed.append(idx)
                warnings.warn(f"{idx}: Binary {f} not able to parse")
                continue            

            count_suspicious_functions = 0
            number_packers = 0
            
            name_packers = [
                'UPX',
                'MPRESS',
                'ExeStealth',
                'Morphine',
                'Themida',
            ]
            suspicious_functions_list = [
                'CreateProcess',
                'CreateProcessA',
                'ConnectNamedPipe',
                'CreateFileMapping',
                'CreateRemoteThread',
                'DeviceIoControl',
                'GetAsyncKeyState',
                'GetModuleFilename',
                'GetThreadContext',
                'InternetOpen',
            ]
            if hasattr(self, 'version') and self.version == 2:
                suspicious_functions_list += [
                    "LoadLibraryA",
                    "GetProcAddress",
                    "VirtualAlloc",
                    "VirtualProtect",
                    "WriteProcessMemory",
                    "ReadProcessMemory",
                    "GetModuleHandle",
                    "ExitProcess",
                ]
            entropy = map(lambda x:x.get_entropy(), pe.sections)
            raw_sizes = map(lambda x:x.SizeOfRawData, pe.sections)
            virtual_sizes = map(lambda x:x.Misc_VirtualSize, pe.sections)
            physical_address = map(lambda x:x.Misc_PhysicalAddress, pe.sections)
            virtual_address = map(lambda x:x.VirtualAddress, pe.sections)
            pointer_raw_data = map(lambda x:x.PointerToRawData, pe.sections)
            characteristics = map(lambda x:x.Characteristics, pe.sections)

            data = {
                    'e_magic':pe.DOS_HEADER.e_magic,
                    'e_cblp':pe.DOS_HEADER.e_cblp,
                    'e_cp':pe.DOS_HEADER.e_cp,
                    'e_crlc':pe.DOS_HEADER.e_crlc,
                    'e_cparhdr':pe.DOS_HEADER.e_cparhdr,
                    'e_minalloc':pe.DOS_HEADER.e_minalloc,
                    'e_maxalloc':pe.DOS_HEADER.e_maxalloc,
                    'e_ss':pe.DOS_HEADER.e_ss,
                    'e_sp':pe.DOS_HEADER.e_sp,
                    'e_csum':pe.DOS_HEADER.e_csum,
                    'e_ip':pe.DOS_HEADER.e_ip,
                    'e_cs':pe.DOS_HEADER.e_cs,
                    'e_lfarlc':pe.DOS_HEADER.e_lfarlc,
                    'e_ovno':pe.DOS_HEADER.e_ovno,
                    'e_oemid':pe.DOS_HEADER.e_oemid,
                    'e_oeminfo':pe.DOS_HEADER.e_oeminfo,
                    'e_lfanew':pe.DOS_HEADER.e_lfanew,
                    'Machine':pe.FILE_HEADER.Machine,
                    'NumberOfSections':pe.FILE_HEADER.NumberOfSections,
                    'TimeDateStamp':pe.FILE_HEADER.TimeDateStamp,
                    'PointerToSymbolTable':pe.FILE_HEADER.PointerToSymbolTable,
                    'NumberOfSymbols':pe.FILE_HEADER.NumberOfSymbols,
                    'SizeOfOptionalHeader':pe.FILE_HEADER.SizeOfOptionalHeader,
                    'Characteristics':pe.FILE_HEADER.Characteristics,
                    'Magic':pe.OPTIONAL_HEADER.Magic,
                    'MajorLinkerVersion':pe.OPTIONAL_HEADER.MajorLinkerVersion,
                    'MinorLinkerVersion':pe.OPTIONAL_HEADER.MinorLinkerVersion,
                    'SizeOfCode':pe.OPTIONAL_HEADER.SizeOfCode,
                    'SizeOfInitializedData':pe.OPTIONAL_HEADER.SizeOfInitializedData,
                    'SizeOfUninitializedData':pe.OPTIONAL_HEADER.SizeOfUninitializedData,
                    'AddressOfEntryPoint':pe.OPTIONAL_HEADER.AddressOfEntryPoint,
                    'BaseOfCode':pe.OPTIONAL_HEADER.BaseOfCode,
                    'ImageBase':pe.OPTIONAL_HEADER.ImageBase,
                    'SectionAlignment':pe.OPTIONAL_HEADER.SectionAlignment,
                    'FileAlignment':pe.OPTIONAL_HEADER.FileAlignment,
                    'MajorOperatingSystemVersion':pe.OPTIONAL_HEADER.MajorOperatingSystemVersion,
                    'MinorOperatingSystemVersion':pe.OPTIONAL_HEADER.MinorOperatingSystemVersion,
                    'MajorImageVersion':pe.OPTIONAL_HEADER.MajorImageVersion,
                    'MinorImageVersion':pe.OPTIONAL_HEADER.MinorImageVersion,
                    'MajorSubsystemVersion':pe.OPTIONAL_HEADER.MajorSubsystemVersion,
                    'MinorSubsystemVersion':pe.OPTIONAL_HEADER.MinorSubsystemVersion,
                    'SizeOfHeaders':pe.OPTIONAL_HEADER.SizeOfHeaders,
                    'CheckSum':pe.OPTIONAL_HEADER.CheckSum,
                    'SizeOfImage':pe.OPTIONAL_HEADER.SizeOfImage,
                    'Subsystem':pe.OPTIONAL_HEADER.Subsystem,
                    'DllCharacteristics':pe.OPTIONAL_HEADER.DllCharacteristics,
                    'SizeOfStackReserve':pe.OPTIONAL_HEADER.SizeOfStackReserve,
                    'SizeOfStackCommit':pe.OPTIONAL_HEADER.SizeOfStackCommit,
                    'SizeOfHeapReserve':pe.OPTIONAL_HEADER.SizeOfHeapReserve,
                    'SizeOfHeapCommit':pe.OPTIONAL_HEADER.SizeOfHeapCommit,
                    'LoaderFlags':pe.OPTIONAL_HEADER.LoaderFlags,
                    'NumberOfRvaAndSizes':pe.OPTIONAL_HEADER.NumberOfRvaAndSizes,
                }

            try:
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    for func in entry.imports:
                        if func.name.decode('utf-8') in suspicious_functions_list:
                            count_suspicious_functions+=1
                data['SuspiciousImportFunctions'] = count_suspicious_functions
            except AttributeError:
                data['SuspiciousImportFunctions'] = 0

            try:
                for entry in pe.sections:
                    try:
                        entry.Name.decode('utf-8')
                    except Exception:
                        number_packers+=1
                    if entry.Name in name_packers:
                        number_packers+=1
                    
                data['SuspiciousNameSection'] = number_packers
            except AttributeError as e:
                data['SuspiciousNameSection'] = -1
            try:
                data['SectionsLength'] = len(pe.sections)
            except (ValueError, TypeError):
                data['SectionsLength'] = -1
            try:
                data['SectionMinEntropy'] = min(entropy)
            except (ValueError, TypeError):
                data['SectionMinEntropy'] = -1
            try:
                data['SectionMaxEntropy'] = max(entropy)
            except (ValueError, TypeError):
                data['SectionMaxEntropy'] = -1
            try:
                data['SectionMinRawsize'] = min(raw_sizes)
            except (ValueError, TypeError):
                data['SectionMinRawsize'] = -1
            try:
                data['SectionMaxRawsize'] = max(raw_sizes)
            except (ValueError, TypeError):
                data['SectionMaxRawsize'] = -1
            try:
                data['SectionMinVirtualsize'] = min(virtual_sizes)
            except (ValueError, TypeError):
                data['SectionMinVirtualsize'] = -1
            try:
                data['SectionMaxVirtualsize'] = max(virtual_sizes)
            except (ValueError, TypeError):
                data['SectionMaxVirtualsize'] = -1
            try:
                data['SectionMaxVirtualsize'] = max(virtual_sizes)
            except (ValueError, TypeError):
                data['SectionMaxVirtualsize'] = -1

            try:
                data['SectionMaxPhysical'] = max(physical_address)
            except (ValueError, TypeError):
                data['SectionMaxPhysical'] = -1
            try:
                data['SectionMinPhysical'] = min(physical_address)
            except (ValueError, TypeError):
                data['SectionMinPhysical'] = -1

            try:
                data['SectionMaxVirtual'] = max(virtual_address)
            except (ValueError, TypeError):
                data['SectionMaxVirtual'] = -1
            try:
                data['SectionMinVirtual'] = min(virtual_address)
            except (ValueError, TypeError):
                data['SectionMinVirtual'] = -1

            try:
                data['SectionMaxPointerData'] = max(pointer_raw_data)
            except (ValueError, TypeError):
                data['SectionMaxPointerData'] = -1

            try:
                data['SectionMinPointerData'] = min(pointer_raw_data)
            except (ValueError, TypeError):
                data['SectionMinPointerData'] = -1

            try:
                data['SectionMaxChar'] = max(characteristics)
            except (ValueError, TypeError):
                data['SectionMaxChar'] = -1

            try:
                data['SectionMinChar'] = min(characteristics)
            except (ValueError, TypeError):
                data['SectionMainChar'] = -1

            try:
                data['DirectoryEntryImport'] = (len(pe.DIRECTORY_ENTRY_IMPORT))
            except AttributeError:
                data['DirectoryEntryImport'] = -1
            try:
                imports = sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], [])
                data['DirectoryEntryImportSize'] = (len(imports))
            except AttributeError:
                data['DirectoryEntryImportSize'] =-1
            #Exports
            try:
                data['DirectoryEntryExport']  = (len(pe.DIRECTORY_ENTRY_EXPORT.symbols))
            except AttributeError:
                # No export
                data['DirectoryEntryExport']  = -1

            try:
                data[ 'ImageDirectoryEntryExport' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXPORT']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntryExport' ] = -1
            try:
                data[ 'ImageDirectoryEntryImport' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntryImport' ] = -1
            try:
                data[ 'ImageDirectoryEntryResource' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_RESOURCE']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntryResource' ] = -1
            try:
                data[ 'ImageDirectoryEntryException' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXCEPTION']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntryException' ] = -1
            try:
                data[ 'ImageDirectoryEntrySecurity' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_SECURITY']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntrySecurity' ] = -1
        
            data_list.append(data)

        return [csr_matrix(pd.DataFrame(data_list).to_numpy())], non_parsed
    
    def output_length(self):
        return 1
