from typing import Callable, List, Union
import pandas as pd
import warnings
from tqdm.auto import tqdm 
import numpy as np
from sklearn.decomposition import IncrementalPCA
from scipy.sparse import csr_matrix

'''
todo
https://github.com/erocarrera/pefile/blob/wiki/UsageExamples.md#introduction
Retrieving the bytes at the entry point and analyze

'''


class Features:
    def __call__(self, pe_filenames, train:bool=True) -> Union[List[csr_matrix], List[int]]:
        '''
        Extracts a list of feature matrices from the given PE files, which are given by filename or bytes

        If file `i` is not able to be parsed, the `i` will be added to the returned list and the ith row of the sparse matrices will be invalid

        Returns the list of sparse feature matrices and a list of invalid row indices
        '''
        raise NotImplementedError()


class FeatureExtractor(Features):
    def __init__(self, feature_extractors: List[Features] = None):
        if feature_extractors is None:
            feature_extractors = [
                BasicFeatures(),
                TfIdfFuncs(),
            ]
        self.feature_extractors = feature_extractors

    def __call__(self, pe_filenames, enable_extractor=None, train=True):
        features = []
        non_parsed = set()
        # run each feature extractor
        for idx,f in enumerate(self.feature_extractors):
            # check if extractor is disabled
            if enable_extractor is not None and idx not in enable_extractor:
                continue
            feat, idx_err = f(pe_filenames, train)
            non_parsed.update(idx_err)
            features.extend(feat)

        non_parsed = list(non_parsed)
        warnings.warn(f"Total {len(non_parsed)} parsing errors for {non_parsed}")
        # remove unparsed and return list
        return [k[~np.isin(np.arange(k.shape[0]), non_parsed)] for k in features], non_parsed


# todo! do tfidft but only over libraries, imports and sections that you can find in malware
class TfIdfFuncs(Features):
    def __init__(self) -> None:
        self.vectorizer_imports = None

    def _tokenizer(self, x):
        return x.split(' ')

    def __call__(self, pe_filenames: List[str], train=True):
        if not train and self.vectorizer_imports is None:
            raise Exception("Have to first call train")

        import lief
        from sklearn.feature_extraction.text import TfidfVectorizer

        # List to store the imported function lists for each PE file
        func_matrix = []
        library_matrix = []
        sec_matrix = []

        # Iterate through the PE files and get the list of imported functions for each file
        non_parsed = []
        for idx,f in tqdm(enumerate(pe_filenames)):
            if isinstance(f, str):
                binary = lief.PE.parse(f)
            else:
                binary = lief.PE.parse(list(f))

            # if unable to parse ignore
            if binary is None:
                func_matrix.append([])
                library_matrix.append([])
                sec_matrix.append([])
                non_parsed.append(idx)
                warnings.warn(f"{idx}: Binary {f} not able to parse")
                continue

            # get list of imported libraries and functions
            func_list = []
            library_list = []
            for imported_library in binary.imports:
                library_list.append(imported_library.name)
                for func in imported_library.entries:
                    func_list.append(f"{imported_library.name}/{func.name}")

            sec_list = []
            # Iterate through the sections and print their information
            for section in binary.sections:
                sec_list.append(section.name)

            library_matrix.append(library_list)
            func_matrix.append(func_list)
            sec_matrix.append(sec_list)
        
        if train:
            # Create a TF-IDF vectorizer
            self.vectorizer_imports = TfidfVectorizer(lowercase=False, tokenizer=self._tokenizer)
            self.vectorizer_libs = TfidfVectorizer(lowercase=False, tokenizer=self._tokenizer)
            self.vectorizer_sect = TfidfVectorizer(lowercase=False, tokenizer=self._tokenizer)

            # Calculate the TF-IDF scores for the imported functions across all the documents
            tf_idf_imports = self.vectorizer_imports.fit_transform([" ".join(doc) for doc in func_matrix])
            tf_idf_libs = self.vectorizer_libs.fit_transform([" ".join(doc) for doc in library_matrix])
            tf_idf_sect = self.vectorizer_sect.fit_transform([" ".join(doc) for doc in sec_matrix])
        else:
            tf_idf_imports = self.vectorizer_imports.transform([" ".join(doc) for doc in func_matrix])
            tf_idf_libs = self.vectorizer_libs.transform([" ".join(doc) for doc in library_matrix])
            tf_idf_sect = self.vectorizer_sect.transform([" ".join(doc) for doc in sec_matrix])

        return [tf_idf_imports, tf_idf_libs, tf_idf_sect], non_parsed


class BasicFeatures(Features):
    def __call__(self, pe_filenames, train=True):
        import pefile
        # https://github.com/amauricio/sklearn-antimalware/blob/master/test.py

        data_list = []
        non_parsed = []
        for idx,f in tqdm(enumerate(pe_filenames)):
            try:
                if isinstance(f, str):
                    pe = pefile.PE(f)
                else:
                    pe = pefile.PE(data=f)
            except:
                # if not able to parse, set to empty so it will be returned as nan
                data_list.append({})
                # log
                non_parsed.append(idx)
                warnings.warn(f"{idx}: Binary {f} not able to parse")
                continue            

            count_suspicious_functions = 0
            number_packers = 0
            
            name_packers = [
                'UPX',
                'MPRESS',
                'ExeStealth',
                'Morphine',
                'Themida',
            ]
            suspicious_functions_list = [
                'CreateProcess',
                'CreateProcessA',
                'ConnectNamedPipe',
                'CreateFileMapping',
                'CreateRemoteThread',
                'DeviceIoControl',
                'GetAsyncKeyState',
                'GetModuleFilename',
                'GetThreadContext',
                'InternetOpen',
            ]
            entropy = map(lambda x:x.get_entropy(), pe.sections)
            raw_sizes = map(lambda x:x.SizeOfRawData, pe.sections)
            virtual_sizes = map(lambda x:x.Misc_VirtualSize, pe.sections)
            physical_address = map(lambda x:x.Misc_PhysicalAddress, pe.sections)
            virtual_address = map(lambda x:x.VirtualAddress, pe.sections)
            pointer_raw_data = map(lambda x:x.PointerToRawData, pe.sections)
            characteristics = map(lambda x:x.Characteristics, pe.sections)

            data = {
                    'e_magic':pe.DOS_HEADER.e_magic,
                    'e_cblp':pe.DOS_HEADER.e_cblp,
                    'e_cp':pe.DOS_HEADER.e_cp,
                    'e_crlc':pe.DOS_HEADER.e_crlc,
                    'e_cparhdr':pe.DOS_HEADER.e_cparhdr,
                    'e_minalloc':pe.DOS_HEADER.e_minalloc,
                    'e_maxalloc':pe.DOS_HEADER.e_maxalloc,
                    'e_ss':pe.DOS_HEADER.e_ss,
                    'e_sp':pe.DOS_HEADER.e_sp,
                    'e_csum':pe.DOS_HEADER.e_csum,
                    'e_ip':pe.DOS_HEADER.e_ip,
                    'e_cs':pe.DOS_HEADER.e_cs,
                    'e_lfarlc':pe.DOS_HEADER.e_lfarlc,
                    'e_ovno':pe.DOS_HEADER.e_ovno,
                    'e_oemid':pe.DOS_HEADER.e_oemid,
                    'e_oeminfo':pe.DOS_HEADER.e_oeminfo,
                    'e_lfanew':pe.DOS_HEADER.e_lfanew,
                    'Machine':pe.FILE_HEADER.Machine,
                    'NumberOfSections':pe.FILE_HEADER.NumberOfSections,
                    'TimeDateStamp':pe.FILE_HEADER.TimeDateStamp,
                    'PointerToSymbolTable':pe.FILE_HEADER.PointerToSymbolTable,
                    'NumberOfSymbols':pe.FILE_HEADER.NumberOfSymbols,
                    'SizeOfOptionalHeader':pe.FILE_HEADER.SizeOfOptionalHeader,
                    'Characteristics':pe.FILE_HEADER.Characteristics,
                    'Magic':pe.OPTIONAL_HEADER.Magic,
                    'MajorLinkerVersion':pe.OPTIONAL_HEADER.MajorLinkerVersion,
                    'MinorLinkerVersion':pe.OPTIONAL_HEADER.MinorLinkerVersion,
                    'SizeOfCode':pe.OPTIONAL_HEADER.SizeOfCode,
                    'SizeOfInitializedData':pe.OPTIONAL_HEADER.SizeOfInitializedData,
                    'SizeOfUninitializedData':pe.OPTIONAL_HEADER.SizeOfUninitializedData,
                    'AddressOfEntryPoint':pe.OPTIONAL_HEADER.AddressOfEntryPoint,
                    'BaseOfCode':pe.OPTIONAL_HEADER.BaseOfCode,
                    'ImageBase':pe.OPTIONAL_HEADER.ImageBase,
                    'SectionAlignment':pe.OPTIONAL_HEADER.SectionAlignment,
                    'FileAlignment':pe.OPTIONAL_HEADER.FileAlignment,
                    'MajorOperatingSystemVersion':pe.OPTIONAL_HEADER.MajorOperatingSystemVersion,
                    'MinorOperatingSystemVersion':pe.OPTIONAL_HEADER.MinorOperatingSystemVersion,
                    'MajorImageVersion':pe.OPTIONAL_HEADER.MajorImageVersion,
                    'MinorImageVersion':pe.OPTIONAL_HEADER.MinorImageVersion,
                    'MajorSubsystemVersion':pe.OPTIONAL_HEADER.MajorSubsystemVersion,
                    'MinorSubsystemVersion':pe.OPTIONAL_HEADER.MinorSubsystemVersion,
                    'SizeOfHeaders':pe.OPTIONAL_HEADER.SizeOfHeaders,
                    'CheckSum':pe.OPTIONAL_HEADER.CheckSum,
                    'SizeOfImage':pe.OPTIONAL_HEADER.SizeOfImage,
                    'Subsystem':pe.OPTIONAL_HEADER.Subsystem,
                    'DllCharacteristics':pe.OPTIONAL_HEADER.DllCharacteristics,
                    'SizeOfStackReserve':pe.OPTIONAL_HEADER.SizeOfStackReserve,
                    'SizeOfStackCommit':pe.OPTIONAL_HEADER.SizeOfStackCommit,
                    'SizeOfHeapReserve':pe.OPTIONAL_HEADER.SizeOfHeapReserve,
                    'SizeOfHeapCommit':pe.OPTIONAL_HEADER.SizeOfHeapCommit,
                    'LoaderFlags':pe.OPTIONAL_HEADER.LoaderFlags,
                    'NumberOfRvaAndSizes':pe.OPTIONAL_HEADER.NumberOfRvaAndSizes
                }

            try:
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    for func in entry.imports:
                        if func.name.decode('utf-8') in suspicious_functions_list:
                            count_suspicious_functions+=1
                data['SuspiciousImportFunctions'] = count_suspicious_functions
            except AttributeError:
                data['SuspiciousImportFunctions'] = 0

            try:
                for entry in pe.sections:
                    try:
                        entry.Name.decode('utf-8')
                    except Exception:
                        number_packers+=1
                    if entry.Name in name_packers:
                        number_packers+=1
                    
                data['SuspiciousNameSection'] = number_packers
            except AttributeError as e:
                data['SuspiciousNameSection'] = 0
            try:
                data['SectionsLength'] = len(pe.sections)
            except (ValueError, TypeError):
                data['SectionsLength'] = 0
            try:
                data['SectionMinEntropy'] = min(entropy)
            except (ValueError, TypeError):
                data['SectionMinEntropy'] = 0
            try:
                data['SectionMaxEntropy'] = max(entropy)
            except (ValueError, TypeError):
                data['SectionMaxEntropy'] = 0
            try:
                data['SectionMinRawsize'] = min(raw_sizes)
            except (ValueError, TypeError):
                data['SectionMinRawsize'] = 0
            try:
                data['SectionMaxRawsize'] = max(raw_sizes)
            except (ValueError, TypeError):
                data['SectionMaxRawsize'] = 0
            try:
                data['SectionMinVirtualsize'] = min(virtual_sizes)
            except (ValueError, TypeError):
                data['SectionMinVirtualsize'] = 0
            try:
                data['SectionMaxVirtualsize'] = max(virtual_sizes)
            except (ValueError, TypeError):
                data['SectionMaxVirtualsize'] = 0
            try:
                data['SectionMaxVirtualsize'] = max(virtual_sizes)
            except (ValueError, TypeError):
                data['SectionMaxVirtualsize'] = 0

            try:
                data['SectionMaxPhysical'] = max(physical_address)
            except (ValueError, TypeError):
                data['SectionMaxPhysical'] = 0
            try:
                data['SectionMinPhysical'] = min(physical_address)
            except (ValueError, TypeError):
                data['SectionMinPhysical'] = 0

            try:
                data['SectionMaxVirtual'] = max(virtual_address)
            except (ValueError, TypeError):
                data['SectionMaxVirtual'] = 0
            try:
                data['SectionMinVirtual'] = min(virtual_address)
            except (ValueError, TypeError):
                data['SectionMinVirtual'] = 0

            try:
                data['SectionMaxPointerData'] = max(pointer_raw_data)
            except (ValueError, TypeError):
                data['SectionMaxPointerData'] = 0

            try:
                data['SectionMinPointerData'] = min(pointer_raw_data)
            except (ValueError, TypeError):
                data['SectionMinPointerData'] = 0

            try:
                data['SectionMaxChar'] = max(characteristics)
            except (ValueError, TypeError):
                data['SectionMaxChar'] = 0

            try:
                data['SectionMinChar'] = min(characteristics)
            except (ValueError, TypeError):
                data['SectionMainChar'] = 0

            try:
                data['DirectoryEntryImport'] = (len(pe.DIRECTORY_ENTRY_IMPORT))
                imports = sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], [])
                data['DirectoryEntryImportSize'] = (len(imports))
            except AttributeError:
                data['DirectoryEntryImport'] = 0
                data['DirectoryEntryImportSize'] =0
            #Exports
            try:
                data['DirectoryEntryExport']  = (len(pe.DIRECTORY_ENTRY_EXPORT.symbols))
            except AttributeError:
                # No export
                data['DirectoryEntryExport']  = 0


            try:
                data[ 'ImageDirectoryEntryExport' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXPORT']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntryExport' ] = 0
            try:
                data[ 'ImageDirectoryEntryImport' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntryImport' ] = 0
            try:
                data[ 'ImageDirectoryEntryResource' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_RESOURCE']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntryResource' ] = 0
            try:
                data[ 'ImageDirectoryEntryException' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXCEPTION']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntryException' ] = 0
            try:
                data[ 'ImageDirectoryEntrySecurity' ] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_SECURITY']].VirtualAddress
            except IndexError as e:
                data[ 'ImageDirectoryEntrySecurity' ] = 0
        
            data_list.append(data)

        return [csr_matrix(pd.DataFrame(data_list).to_numpy())], non_parsed
