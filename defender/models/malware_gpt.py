import argparse
import builtins
import math
import os
import random
import shutil
import time
from typing import List
import warnings
from functools import partial
from libauc.metrics import auc_roc_score

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import lightning.pytorch as pl
import torch
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.parallel
import torch.optim
from torch.utils.data import Dataset
import torch.utils.data
import torch.utils.data.distributed
import torchmetrics
import torch.nn as nn
import torchvision.datasets as datasets
import torchvision.models as torchvision_models
import torchvision.transforms as transforms
import torchvision.transforms as T
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from defender.model import Model, ThresholdSelector

from defender.utils import create_batches, load_pickle


parser = argparse.ArgumentParser(description='')
parser.add_argument('-a', '--arch', type=str, default='attention')
parser.add_argument('-j', '--workers', default=8, type=int, metavar='N',
                    help='number of data loading workers (default: 4)')
parser.add_argument('--epochs', default=400, type=int, metavar='N',
                    help='number of total epochs to run')
parser.add_argument('-b', '--batch-size', default=526, type=int,
                    metavar='N',
                    help='mini-batch size (default: 256)')
parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,
                    metavar='LR', help='initial (base) learning rate', dest='lr')
parser.add_argument('--momentum', default=0.9, type=float, metavar='M',
                    help='momentum')
parser.add_argument('--wd', '--weight-decay', default=0., type=float,
                    metavar='W', help='weight decay (default: 1e-6)',
                    dest='weight_decay')
parser.add_argument('--resume', default=None, type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('-e', '--evaluate_every', default=10, type=int,
                    help='evaluate model on validation set every # epochs')
parser.add_argument('--seed', default=None, type=int,
                    help='seed for initializing training. ')
parser.add_argument('--debug', action='store_true', help='To debug code')
parser.add_argument('-s', '--save', type=str, default='defender/ml_classifier.pkl', help='file where to save model')

# additional configs:
# parser.add_argument('--pretrained', default='saved_models/chexpert/resnet50/dcl/pretrain/version_0/resnet50-best-epoch=99.ckpt', type=str,
#                     help='path to sogclr pretrained checkpoint')

parser.add_argument('--loss_type', default='bce', type=str,
                    help='loss type of pretrained (default: bce)')
parser.add_argument('--optimizer', default='adamw', type=str,
                    choices=['sgd', 'adamw'],
                    help='optimizer used (default: sgd)')

# dataset 
parser.add_argument('--save_dir', default='./saved_models/', type=str) 

# saving
parser.add_argument('--save_every_epochs', default=20, type=int,
                    help='number of epochs to save checkpoint')

# model
parser.add_argument("--hidden_dim", type=int, nargs='+', default=[512, 512])
parser.add_argument("--num_heads", type=int, default=3)
parser.add_argument("--dropout_rate", type=float, default=0.5)
parser.add_argument("--att_dropout", type=float, default=0.0)


class SparseDataset(Dataset):
    tmp_file = f"tmp_calc/{random.randint(0, 9999)}_tmp_x"

    def __init__(self, path, use_features=None):
        list_sparse_x, self.y = load_pickle(path)
        self.x = []
        for idx,sparse_x in enumerate(list_sparse_x):
            if use_features is not None and idx not in use_features:
                continue

            x = np.memmap(f"{self.tmp_file}_{idx}", mode="r", dtype=sparse_x.dtype, offset=0, shape=sparse_x.shape)
            for arr,start,end in create_batches(sparse_x):
                x[start:end,...] = arr.toarray()
            self.x.append(x)

    def __len__(self):
        return self.x.shape[0]

    def feature_dims(self):
        return [x.shape[1] for x in self.x]

    def __getitem__(self, index):
        return [x[index] for x in self.x], self.y[index]


class AttentionModel(nn.Module):
    def __init__(
            self,
            features_dim: List[int],
            hidden_dim: List[int],
            num_heads: int, 
            output_dim: int=1,
            dropout_rate: float=0.5,
            att_dropout: float=0.0,
        ):
        super().__init__()
        assert hidden_dim[-1] == 1

        # embeddings for features
        l = hidden_dim[0]
        assert l % len(features_dim)==0
        self.embed_features = [nn.Sequential(
            nn.BatchNorm1d(k),
            nn.Linear(k, l // len(features_dim), bias=False),
        ) for k in features_dim]

        self.layers = nn.ModuleList()
        for h in hidden_dim[1:]:
            self.layers.append(nn.Sequential(
                nn.BatchNorm1d(l),
                nn.MultiheadAttention(
                    embed_dim=l,
                    num_heads=num_heads,
                    dropout=att_dropout,
                    batch_first=True,
                ), 
            ))
            self.layers.append(nn.Sequential(
                nn.BatchNorm1d(l),
                nn.Linear(l, h),
                nn.ReLU(inplace=True),
                nn.Linear(h, h),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
            ))
            l = h

        self.output = nn.Linear(l, output_dim)

    def forward(self,x):
        x = [f(x) for f in self.embed_features]
        x = torch.concatenate(x, dim=2)
        for f in self.layers:
            x = f(x) + x
        return self.output(x)


class TorchModel(Model):
    def __init__(
            self, 
            feature_extractor,
            model,
        ):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.model = model

    def extract_features(self, pe_files: List[str]) -> List[np.array]:
        features, parsed_errors = self.feature_extractor(pe_files, enable_extractor=None, train=self.train)
        if len(parsed_errors) != 0:
            raise Exception("Some files were not parsed")
        return features

    def __call__(self, x: List[np.array]) -> np.array:
        self.model(x)

    @staticmethod
    def _init_parameters(d):
        return {'feature_extractor': d['feature_extractor'], 'model': d['model']}


class DetectorModel(pl.LightningModule):
    def __init__(
        self,
        model,
        args,
        **kwargs,
    ):
        super().__init__()
        # random input to build computational graph
        # self.example_input_array = torch.zeros((1, ))

        # save hyperparameters as attribute
        self.save_hyperparameters(ignore=['model', 'args'])
        
        self.args = args
        self.model = model

        # # infer learning rate
        # self.init_lr = self.hparams.args.lr * self.hparams.args.batch_size / 256
        # self.hparams.args.lr = self.init_lr
        # print('initial learning rate:', self.hparams.args.lr)

        ##################
        # METRICS
        ##################
        # todo! change for auc criterion
        if args.loss == 'bce':
            self.criterion = nn.BCEWithLogitsLoss()
        # elif args.loss == 'auc':
        #     self.criterion = AUCM_MultiLabel()
        else:
            raise NotImplementedError()

        self.train_auc = torchmetrics.AUROC(task='binary', max_fpr=0.01)
        self.val_auc = torchmetrics.AUROC(task='binary', max_fpr=0.01)

    def configure_optimizers(self):
        if self.hparams.args.optimizer == 'sgd':
            optimizer = torch.optim.SGD(self.parameters(), self.hparams.args.lr,
                                            weight_decay=self.hparams.args.weight_decay,
                                            momentum=self.hparams.args.momentum)
        elif self.hparams.args.optimizer == 'adamw':
            optimizer = torch.optim.AdamW(self.parameters(), self.hparams.args.lr,
                                    weight_decay=self.hparams.args.weight_decay)
        else:
            raise NotImplementedError("Optimizer not implemented")

        # todo! use reduce on plateau scheduler
        # scheduler = get_linear_schedule_with_warmup(optimizer,num_training_steps=n_epochs, num_warmup_steps=100)
        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience)
        return optimizer
        
    def forward(self, x):
        return self.model(x)

    def on_train_epoch_start(self):
        # adjust learning rate and momentum coefficient per iteration
        adjust_learning_rate(self.optimizers(), self.init_lr, self.current_epoch, self.hparams.args)

    def training_step(self, batch, batch_idx):
        # run through model
        x, target = batch
        output = self(x)
        loss = self.criterion(output, target)

        # calculate metrics
        self.train_auc(output, target.int())

        # log loss for training step and average loss for epoch
        self.log_dict({
            "train_loss": loss,
            "train_auc": self.train_auc,
        }, on_step=True, on_epoch=True, prog_bar=True, logger=True)

        # # compute gradient and do SGD step
        # optimizer.zero_grad()
        # scaler.scale(loss).backward()
        # scaler.step(optimizer)
        # scaler.update()
        
        return loss
    
    # def on_validation_start(self) -> None:
    #     # check that pretrained weights have not changed
    #     self.model.sanity_check(self.hparams.args.pretrained, verbose=False)
    #     return super().on_validation_start()

    def validation_step(self, batch, batch_idx):   
        # run through model
        x, target = batch
        output = self(x)
        loss = self.criterion(output, target)

        # calculate metrics
        y_pred = torch.sigmoid(output)
        self.val_auc(y_pred, target.int())

        # log loss for training step and average loss for epoch
        self.log_dict({
            "val_loss": loss,
            "val_auc": self.val_auc,
        }, on_step=False, on_epoch=True, prog_bar=True, logger=True)

    # def validation_epoch_end(self, outputs):
    #     fpr, tpr, thresholds = self.val_roc.compute()
    #     plot_roc(fpr, tpr, self.val_auc.compute())

    # def on_test_start(self) -> None:
    #     self.preds = []
    #     self.targets = []

    # def test_step(self, batch, batch_idx):
    #     # run through model
    #     images, target = batch
    #     output = self(images)
    #     y_pred = torch.sigmoid(output)

    #     self.preds.append(y_pred.detach().cpu())
    #     self.targets.append(target.detach().cpu())

    # def on_test_epoch_end(self) -> None:
    #     self.preds = torch.cat(self.preds, dim=0)
    #     self.targets = torch.cat(self.targets, dim=0)
    #     torch.save(self.preds, os.path.join(self.hparams.save_path, "preds.pt"))
    #     torch.save(self.targets, os.path.join(self.hparams.save_path, "targets.pt"))


def adjust_learning_rate(optimizer, init_lr, epoch, args):
    """Decay the learning rate based on schedule with half cycle cosine"""
    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))
    for param_group in optimizer.param_groups:
        param_group['lr'] = cur_lr


def main():
    ###########################
    # PARAMETERS
    ###########################
    args = parser.parse_args()

    if args.debug:
        args.workers = 0
    
    # seed for reproducibility
    if args.seed is not None:
        warnings.warn(f"You have seeded training with seed {args.seed}")
        pl.seed_everything(args.seed, workers=True)
    else:
        warnings.warn(f"You have not seeded training")

    if not torch.cuda.is_available():
        warnings.warn("No GPU available: training will be extremely slow")

    ###########################
    # DATASET
    ###########################

    train_dataset = SparseDataset('data/train.pkl')
    val_dataset = SparseDataset('data/valid.pkl')

    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=args.workers, 
        pin_memory=not args.debug,
        drop_last=not args.debug,
    )

    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.workers, 
        pin_memory=False,
        drop_last=False,
    )

    ###########################
    # MODEL
    ###########################

    base_dir = os.path.join(args.arch, args.loss_type)
    if args.resume is None:
        logger = pl.loggers.TensorBoardLogger(
            save_dir=args.save_dir,
            name=base_dir, 
            # todo! use log graph
            # log_graph=True,
        )
    else:
        logdir = args.resume.split('/')[1:-1]
        logger = pl.loggers.TensorBoardLogger(
            save_dir=args.save_dir,
            name=os.path.join(*logdir[:-1]),
            version=logdir[-1],
            # log_graph=True,
        )

    # adjust path for pretrain according to model
    # args.pretrained = os.path.join(args.save_dir, base_dir, "pretrain", args.pretrained)

    # load pretrained model
    # pretrained_model = SogModel.load_from_checkpoint(args.pretrained).model
    model = AttentionModel(
        features_dim=train_dataset.feature_dims(),
        hidden_dim=args.hidden_dim,
        num_heads=args.num_heads,
        dropout_rate=args.dropout_rate,
        att_dropout=args.att_dropout,
    )

    # task to do
    model = DetectorModel(
        model=model,
        args=args,
    )

    ###########################
    # CALLBACKS
    ###########################

    callbacks = [
        pl.callbacks.LearningRateMonitor(),
        # pl.callbacks.DeviceStatsMonitor(),  # monitors and logs device stats, useful to find memory usage
    ]

    save_path = logger.log_dir

    ## callback for saving checkpoints
    checkpoint_cb_every = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="last",
        monitor="step",
        mode="max",
        save_top_k=1,
        every_n_epochs=args.save_every_epochs,
        # save_on_train_epoch_end=True,                         # when using a training metric
        # train_time_interval=,
        # every_n_train_steps=,
        # save_last=False,                                        # save last might be useful to have
    )
    callbacks.append(checkpoint_cb_every)

    checkpoint_cb_bestk = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="best",
        save_top_k=1, 
        monitor='val_auc',
        mode='max',
        # save_on_train_epoch_end=False,     # when using a training metric
        # save_last=False,
    )
    callbacks.append(checkpoint_cb_bestk)

    # early stopping
    # early_stopping = pl.callbacks.EarlyStopping(
    #     monitor='val_loss',
    #     mode='min',
    #     patience=10,
    #     verbose=True,
    # )
    # callbacks.append(early_stopping)

    ###########################
    # TRAINER
    ###########################

    # may increase performance but lead to unstable training
    torch.set_float32_matmul_precision("high")
    trainer = pl.Trainer(
        accelerator='gpu' if not args.debug else 'cpu',
        deterministic="warn" if args.seed is not None else False,
        precision="16-mixed",   # reduce memory, can improve performance but might lead to unstable training
        
        max_epochs=args.epochs,
        max_time="00:1:00:00",
        # max_steps=,

        check_val_every_n_epoch=args.evaluate_every,
        # val_check_interval=1.0,
        logger=logger,
        log_every_n_steps=10,
        callbacks=callbacks,

        fast_dev_run=args.debug,   # for testing training and validation
        # num_sanity_val_steps=0 if not args.debug else 2,
        limit_train_batches=1.0 if not args.debug else 0.01,  # to test what happens after an epoch
        # overfit_batches=0.01,

        # profiler='simple',    # advanced profiling to check for bottlenecks
    )

    ###########################
    # RUN MODEL
    ###########################

    # todo! TUNE LR
    # call tune to find the batch size
    # tuner = pl.tuner.Tuner(trainer)
    # tuner.scale_batch_size(model, mode="power")

    # fit the model
    print("Fitting model...")
    trainer.fit(
        model=model,
        train_dataloaders=train_dataloader,
        val_dataloaders=val_dataloader,
        ckpt_path=args.resume,
    )

    # get best threshold
    model = DetectorModel.load_from_checkpoint(checkpoint_cb_bestk.best_model_path).model
    score, threshold = ThresholdSelector()(model, val_dataloader)
    print(f"Best score: {score}")
    wrapper = TorchModel(load_pickle('data/feature_extractor.pkl'), model)
    wrapper.best_threshold = threshold
    wrapper.save(args.save)

    ## test model
    # trainer.test(
    #     model=model,
    #     dataloaders=test_dataloader, 
    #     ckpt_path=os.path.join(save_path, "best.ckpt"),
    #     verbose=True,
    # )


if __name__ == '__main__':
    main()
