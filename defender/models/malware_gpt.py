import argparse
import math
import os
import random
from typing import List
import warnings
from tqdm.auto import trange

import matplotlib
import numpy as np
import numpy as np
import lightning.pytorch as pl
import torch.utils.data as data
import torch
from scipy.sparse import csr_matrix
from scipy import sparse
import torch.nn as nn
import torch.nn.parallel
import torch.optim
from torch.utils.data import Dataset
import torch.utils.data
import torch.utils.data.distributed
import torchmetrics
import torch.nn as nn
from torch.utils.data import DataLoader
from defender.model import Model, ThresholdSelector, find_best_score
import defender.models.ember_init as ember
import torch.nn.functional as F

from defender.utils import create_batches, load_pickle, split_arrays, split_generator_dataset
from libauc.losses import AUCMLoss
from libauc.optimizers import PESG


parser = argparse.ArgumentParser(description='')
parser.add_argument('-a', '--arch', type=str, default='attention')
parser.add_argument('-j', '--workers', default=0, type=int, metavar='N',
                    help='number of data loading workers (default: 4)')
parser.add_argument('--epochs', default=100, type=int, metavar='N',
                    help='number of total epochs to run')
parser.add_argument('-b', '--batch-size', default=128, type=int,
                    metavar='N',
                    help='mini-batch size (default: 256)')
parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,
                    metavar='LR', help='initial (base) learning rate', dest='lr')
parser.add_argument('--momentum', default=0.9, type=float, metavar='M',
                    help='momentum')
parser.add_argument('--wd', '--weight-decay', default=0., type=float,
                    metavar='W', help='weight decay (default: 1e-6)',
                    dest='weight_decay')
parser.add_argument('--resume', default=None, type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('--seed', default=None, type=int,
                    help='seed for initializing training. ')
parser.add_argument('--debug', action='store_true', help='To debug code')

# additional configs:
# parser.add_argument('--pretrained', default='saved_models/chexpert/resnet50/dcl/pretrain/version_0/resnet50-best-epoch=99.ckpt', type=str,
#                     help='path to sogclr pretrained checkpoint')

parser.add_argument('--loss_type', default='bce', type=str,
                    help='loss type of pretrained (default: bce)')
parser.add_argument('--optimizer', default='adamw', type=str,
                    choices=['sgd', 'adamw'],
                    help='optimizer used (default: sgd)')
parser.add_argument('--warmup-epochs', default=10, type=int, metavar='N',
                    help='number of warmup epochs')

# dataset 
parser.add_argument('--save_dir', default='./saved_models/', type=str) 

# saving
parser.add_argument('--save_every_epochs', default=20, type=int,
                    help='number of epochs to save checkpoint')
parser.add_argument('-e', '--evaluate_every', default=1, type=int,
                    help='evaluate model on validation set every # epochs')
parser.add_argument('--early_stopping_patience', default=20, type=int,
                    help='patience for early stopping')
parser.add_argument('-s', '--save', type=str, default='defender/ml_classifier.pkl', help='file where to save model')

# model
parser.add_argument("--hidden_dim", type=int, nargs='+', default=[1024, 1024])
parser.add_argument("--num_heads", type=int, default=4)
parser.add_argument("--dropout_rate", type=float, default=0.5)
parser.add_argument("--att_dropout", type=float, default=0.0)


def save_files(X, path):
    filenames = []
    for i in trange(X.shape[0]):
        f = f"{path}_{i}"
        v = torch.from_numpy(X[i,...])
        if v.isnan().sum() > 0:
            raise Exception("Input nan, no implemented solutions")
        torch.save(v, f)
        filenames.append(f)
    return filenames

class SparseDataset(Dataset):
    base_tmp_file = f"tmp_calc/torch_tmp_x"
    version = -1

    @staticmethod
    def get_datasets(paths, seed, lengths=[0.8, 0.2], use_features=None, use_ember=False):
        self_x = []
        self_y = []
        for idx_path, path in enumerate(paths):
            SparseDataset.version += 1
            self_tmp_file = f"{SparseDataset.base_tmp_file}_{idx_path}"
            # load data
            list_sparse_x, y = load_pickle(path)
            # save to files to not fill memory
            for idx,sparse_x in enumerate(list_sparse_x):
                if use_features is not None and idx not in use_features:
                    continue

                if idx_path == 0:
                    self_x.append([])

                filenames = []
                for i in trange(sparse_x.shape[0]):
                    f = f"{self_tmp_file}_{idx}_{i}"
                    v = torch.from_numpy(sparse_x[i,...].toarray()[0])
                    if v.isnan().sum() > 0:
                        raise Exception("Input nan, no implemented solutions")
                    torch.save(v, f)
                    filenames.append(f)

                self_x[idx].extend(filenames)
                self_y.append(y)
        self_y = np.concatenate(self_y, axis=0)

        # load additional data
        if use_ember:
            assert use_features == [0]
            filenames = []

            # bodmas dataset
            filename = 'data/bodmas/bodmas.npz'
            data = np.load(filename)
            X = data['X']  # all the feature vectors
            y = data['y']  # labels, 0 as benign, 1 as malicious
            self_y = np.concatenate([self_y, y], axis=0)
            del y
            del data

            # save to files
            self_x[0].extend(save_files(X, f"{self_tmp_file}_bodmas"))
            del X
            
            X_train, y_train, X_test, y_test = ember.read_vectorized_features("data/ember2018/")
            self_y = np.concatenate([self_y, y_train], axis=0)
            self_y = np.concatenate([self_y, y_test], axis=0)
            del y_train
            del y_test
            
            # save to files
            self_x[0].extend(save_files(X_train, f"{self_tmp_file}_emberTrain"))
            del X_train
            self_x[0].extend(save_files(X_test, f"{self_tmp_file}_emberTest"))
            del X_test

        # Filter unlabeled data
        train_rows = (self_y != -1)
        self_y = self_y[train_rows]
        self_x = [np.asarray(x)[train_rows] for x in self_x]

        # check that shapes match
        if self_y.shape != self_x[0].shape:
            raise Exception("Shapes do not match")

        # check that no unlabeled data
        if np.logical_and(self_y != 0, self_y !=1).sum():
            raise Exception("Unknown labels, it should be 0 or 1")

        # divide into train and validation
        xy1, xy2 = split_arrays(self_x + [self_y], lengths, seed=seed)

        return SparseDataset(xy1[:-1], xy1[-1]), SparseDataset(xy2[:-1], xy2[-1])

    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __len__(self):
        return len(self.x[0])

    def feature_dims(self):
        return [torch.load(x[0]).shape[0] for x in self.x]
        # return [x.shape[1] for x in self.x]

    def __getitem__(self, index):
        # return [x[index].astype(np.float32) for x in self.x], self.y[index]
        return [torch.load(x[index]).float() for x in self.x], self.y[index]


# class SelfAttention(nn.Module):
#     def __init__(self, num_heads, out_features):
#         super().__init__()
#         self.num_heads = num_heads
#         self.out_features = out_features

#         self.head_size = out_features // num_heads

#         self.query = nn.Linear(self.head_size, self.head_size)
#         self.key = nn.Linear(self.head_size, self.head_size)
#         self.value = nn.Linear(self.head_size, self.head_size)

#         self.output = nn.Linear(out_features, out_features)

#     def forward(self, x):
#         assert len(x.shape) == 2

#         batch_size = x.shape[0]
#         x = x.view(batch_size, self.num_heads, self.head_size)

#         query = self.query(x)
#         key = self.key(x)
#         value = self.value(x)

#         score = torch.matmul(query, key.transpose(-2, -1)) / (self.head_size ** 0.5)
#         attention = F.softmax(score, dim=-1)
#         attended = torch.matmul(attention, value)

#         attended = attended.view(batch_size, self.out_features)
#         output = self.output(attended)

#         return output


# class FeedForward(nn.Module):
#     def __init__(
#             self,
#             features_dim: List[int],
#             hidden_dim: List[int],
#             num_heads: int, 
#             output_dim: int=1,
#             dropout_rate: float=0.5,
#             att_dropout: float=0.0,
#         ):
#         super().__init__()

#         # embeddings for features
#         hidden_dim = np.asarray(hidden_dim) * len(features_dim)
#         l = hidden_dim[0]
#         assert l % len(features_dim)==0
#         self.embed_features = nn.ModuleList([nn.Sequential(
#             nn.BatchNorm1d(k),
#             nn.Linear(k, l // len(features_dim)),
#         ) for k in features_dim])

#         self.layers = []
#         for h in hidden_dim[1:]:
#             self.layers.append(self.Block(
#                 in_dim=l,
#                 out_dim=h,
#                 num_heads=num_heads,
#                 dropout_rate=dropout_rate,
#                 att_dropout=att_dropout,
#             ))
#             l = h
#         self.layers = nn.Sequential(*self.layers)

#         self.output = self.ff = nn.Sequential(
#             # nn.Linear(l, l),
#             # nn.ReLU(inplace=True),
#             nn.Linear(l, output_dim),
#         )

#     def forward(self, list_x):
#         x = [f(x.float()) for x,f in zip(list_x,self.embed_features)]
#         x = torch.concatenate(x, dim=1)     # B, dim
#         x = x[None].transpose(0,1)     # B, 1, dim
#         for f in self.layers:
#             x = f(x)
#         x = x[:,0,:]
#         return self.output(x)[:,0]


class AttentionModel(nn.Module):
    class Block(nn.Module):
        class MultiheadSelfAttention(nn.MultiheadAttention):
            def forward(self, x, *args, **kwargs):
                return super().forward(
                    query=x,
                    key=x,
                    value=x,
                    *args,
                    **kwargs,
                )
        
        def __init__(self, in_dim, out_dim, num_heads, dropout_rate, att_dropout):
            super().__init__()

            self.self_attn = self.MultiheadSelfAttention(
                embed_dim=in_dim,
                num_heads=num_heads,
                dropout=att_dropout,
                batch_first=True,
            )
            self.norm1 = nn.LayerNorm(in_dim)
            self.norm2 = nn.LayerNorm(in_dim)
            self.dropout1 = nn.Dropout(dropout_rate)

            self.ff1 = nn.Linear(in_dim, out_dim)
            self.ff2 = nn.Linear(out_dim, in_dim)

            self.norm3 = nn.LayerNorm(in_dim)
            self.dropout2 = nn.Dropout(dropout_rate)

        def forward(self,x):
            # self attention
            x = self.norm1(x)
            x2 = self.self_attn(x)[0]
            # residual connection
            x = x + self.dropout1(self.norm2(x2))

            # feedforward
            x2 = F.relu(self.ff2(F.relu(self.ff1(x))))
            
            # residual connection
            x = x + self.dropout2(self.norm3(x2))
            return x

    def __init__(
            self,
            features_dim: List[int],
            hidden_dim: List[int],
            num_heads: int, 
            output_dim: int=1,
            dropout_rate: float=0.5,
            att_dropout: float=0.0,
        ):
        super().__init__()

        # embeddings for features
        hidden_dim = np.asarray(hidden_dim) * len(features_dim)
        l = hidden_dim[0]
        assert l % len(features_dim)==0
        self.embed_features = nn.ModuleList([nn.Sequential(
            nn.BatchNorm1d(k),
            nn.Linear(k, l // len(features_dim)),
        ) for k in features_dim])

        self.layers = []
        for h in hidden_dim[1:]:
            self.layers.append(self.Block(
                in_dim=l,
                out_dim=h,
                num_heads=num_heads,
                dropout_rate=dropout_rate,
                att_dropout=att_dropout,
            ))
            l = h
        self.layers = nn.Sequential(*self.layers)

        self.output = self.ff = nn.Sequential(
            # nn.Linear(l, l),
            # nn.ReLU(inplace=True),
            nn.Linear(l, output_dim),
        )

    def forward(self, list_x):
        x = [f(x.float()) for x,f in zip(list_x,self.embed_features)]
        x = torch.concatenate(x, dim=1)     # B, dim
        x = x[None].transpose(0,1)     # B, 1, dim
        for f in self.layers:
            x = f(x)
        x = x[:,0,:]
        return self.output(x)[:,0]


class TorchModel(Model):
    # todo! run eval
    def __init__(
            self, 
            feature_extractor,
            model,
            use_features=None,
        ):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.model = model
        self.use_features = use_features

    def extract_features(self, pe_files: List[str]) -> List[np.array]:
        features, parsed_errors = self.feature_extractor(pe_files, enable_extractor=self.use_features, train=self.train)
        if len(parsed_errors) != 0:
            raise Exception("Some files were not parsed")
        return torch.as_tensor(features.toarray()).float()

    def __call__(self, x: List[np.array]) -> np.array:
        self.model(x)

    @Model.train.setter
    def train(self, v):
        super().train
        self.model.train(v)

    @torch.no_grad()
    def predict(self, x: List[np.array]):
        return super().predict(x)

    @staticmethod
    def _init_parameters(d):
        return {'feature_extractor': d['feature_extractor'], 'model': d['model']}


class DetectorModel(pl.LightningModule):
    def __init__(
        self,
        features_dim,
        args,
        **kwargs,
    ):
        super().__init__()
        # random input to build computational graph
        # self.example_input_array = torch.zeros((1, ))

        # save hyperparameters as attribute
        self.save_hyperparameters(ignore=['model'])
        
        self.args = args
        self.model = AttentionModel(
            features_dim=features_dim,
            hidden_dim=args.hidden_dim,
            num_heads=args.num_heads,
            dropout_rate=args.dropout_rate,
            att_dropout=args.att_dropout,
        )
        self.lr = self.args.lr
        self.batch_size = self.args.batch_size

        ## infer learning rate
        self.init_lr = self.lr
        self.init_lr = self.lr * self.batch_size / 256
        self.lr = self.init_lr
        print('initial learning rate:', self.args.lr)

        ##################
        # METRICS
        ##################
        # todo! change for auc criterion
        if args.loss_type == 'bce':
            self.criterion = nn.BCEWithLogitsLoss()
        elif args.loss_type == 'auc':
            self.criterion = AUCMLoss()
        else:
            raise NotImplementedError()

        self.train_auc = torchmetrics.AUROC(task='binary', max_fpr=0.01)
        self.val_auc = torchmetrics.AUROC(task='binary', max_fpr=0.01)

    def configure_optimizers(self):
        if self.args.loss_type == 'auc':
            optimizer = PESG(
                self.model, 
                loss_fn=self.criterion, 
                lr=self.args.lr, 
                momentum=self.args.momentum,
                weight_decay=self.args.weight_decay,
            )
        else:
            if self.args.optimizer == 'sgd':
                optimizer = torch.optim.SGD(self.parameters(), self.args.lr,
                                                weight_decay=self.args.weight_decay,
                                                momentum=self.args.momentum)
            elif self.args.optimizer == 'adamw':
                optimizer = torch.optim.AdamW(self.parameters(), self.args.lr,
                                        weight_decay=self.args.weight_decay)
            else:
                raise NotImplementedError("Optimizer not implemented")

        # todo! use reduce on plateau scheduler
        # scheduler = get_linear_schedule_with_warmup(optimizer,num_training_steps=n_epochs, num_warmup_steps=100)
        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience)
        return optimizer
        
    def forward(self, x):
        return self.model(x)

    def on_train_epoch_start(self):
        # adjust learning rate and momentum coefficient per iteration
        adjust_learning_rate(self.optimizers(), self.init_lr, self.current_epoch, self.args)

    def training_step(self, batch, batch_idx):
        # run through model
        x, target = batch
        output = self(x)
        if output.isnan().sum() > 0:
            warnings.warn("Nan values being generated")
            # output = self(x)
        loss = self.criterion(output, target)
        if loss.isnan():
            warnings.warn("Getting nan loss")

        # calculate metrics
        y_pred = torch.sigmoid(output)
        self.train_auc(y_pred.float(), target.int())

        # log loss for training step and average loss for epoch
        self.log_dict({
            "train_loss": loss,
            "train_auc": self.train_auc,
        }, on_step=True, on_epoch=True, prog_bar=True, logger=True)

        # # compute gradient and do SGD step
        # optimizer.zero_grad()
        # scaler.scale(loss).backward()
        # scaler.step(optimizer)
        # scaler.update()
        
        return loss
    
    # def on_validation_start(self) -> None:
    #     # check that pretrained weights have not changed
    #     self.model.sanity_check(self.args.pretrained, verbose=False)
    #     return super().on_validation_start()

    def validation_step(self, batch, batch_idx):   
        # run through model
        x, target = batch
        output = self(x)
        loss = self.criterion(output, target)

        # calculate metrics
        y_pred = torch.sigmoid(output)
        self.val_auc(y_pred.float(), target.int())
        score = find_best_score(
            target.int().cpu().numpy(), 
            y_pred.float().detach().cpu().numpy(), 
            return_threshold=False,
        )

        # log loss for training step and average loss for epoch
        self.log_dict({
            "val_loss": loss,
            "val_auc": self.val_auc,
            "score": score,
        }, on_step=False, on_epoch=True, prog_bar=True, logger=True)

    # def validation_epoch_end(self, outputs):
    #     fpr, tpr, thresholds = self.val_roc.compute()
    #     plot_roc(fpr, tpr, self.val_auc.compute())

    # def on_test_start(self) -> None:
    #     self.preds = []
    #     self.targets = []

    # def test_step(self, batch, batch_idx):
    #     # run through model
    #     images, target = batch
    #     output = self(images)
    #     y_pred = torch.sigmoid(output)

    #     self.preds.append(y_pred.detach().cpu())
    #     self.targets.append(target.detach().cpu())

    # def on_test_epoch_end(self) -> None:
    #     self.preds = torch.cat(self.preds, dim=0)
    #     self.targets = torch.cat(self.targets, dim=0)
    #     torch.save(self.preds, os.path.join(self.save_path, "preds.pt"))
    #     torch.save(self.targets, os.path.join(self.save_path, "targets.pt"))


def adjust_learning_rate(optimizer, init_lr, epoch, args):
    """Decays the learning rate with half-cycle cosine after warmup"""
    if epoch < args.warmup_epochs:
        lr = init_lr * epoch / args.warmup_epochs 
    else:
        lr = init_lr * 0.5 * (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return lr


def main():
    ###########################
    # PARAMETERS
    ###########################
    args = parser.parse_args()
    use_features = [0]

    if args.debug:
        args.workers = 0
    
    # seed for reproducibility
    if args.seed is not None:
        warnings.warn(f"You have seeded training with seed {args.seed}")
        pl.seed_everything(args.seed, workers=True)
    else:
        warnings.warn(f"You have not seeded training")

    if not torch.cuda.is_available():
        warnings.warn("No GPU available: training will be extremely slow")

    ###########################
    # DATASET
    ###########################

    # dataset1 = SparseDataset('data/train.pkl', use_features=use_features, use_ember=False)
    # dataset2 = SparseDataset('data/test.pkl', use_features=use_features, use_ember=False)
    # full_dataset = data.ConcatDataset([dataset1, dataset2])
    # train_dataset, val_dataset = split_generator_dataset(full_dataset, [0.8, 0.2], args.seed)
    train_dataset, val_dataset = SparseDataset.get_datasets(['data/train.pkl', 'data/test.pkl'], seed=args.seed, use_features=use_features, use_ember=True)

    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=args.workers, 
        pin_memory=not args.debug,
        drop_last=not args.debug,
    )

    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.workers, 
        pin_memory=False,
        drop_last=False,
    )

    ###########################
    # MODEL
    ###########################

    base_dir = os.path.join(args.arch, args.loss_type)
    if args.resume is None:
        logger = pl.loggers.TensorBoardLogger(
            save_dir=args.save_dir,
            name=base_dir, 
            # todo! use log graph
            # log_graph=True,
        )
    else:
        logdir = args.resume.split('/')[1:-1]
        logger = pl.loggers.TensorBoardLogger(
            save_dir=args.save_dir,
            name=os.path.join(*logdir[:-1]),
            version=logdir[-1],
            # log_graph=True,
        )

    # adjust path for pretrain according to model
    # args.pretrained = os.path.join(args.save_dir, base_dir, "pretrain", args.pretrained)

    # load pretrained model
    # pretrained_model = SogModel.load_from_checkpoint(args.pretrained).model

    # task to do
    model_task = DetectorModel(
        features_dim=train_dataset.feature_dims(),
        args=args,
    )

    ###########################
    # CALLBACKS
    ###########################

    callbacks = [
        pl.callbacks.LearningRateMonitor(),
        # pl.callbacks.DeviceStatsMonitor(),  # monitors and logs device stats, useful to find memory usage
    ]

    save_path = logger.log_dir

    ## callback for saving checkpoints
    checkpoint_cb_every = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="last",
        monitor="step",
        mode="max",
        save_top_k=1,
        every_n_epochs=args.save_every_epochs,
        # save_on_train_epoch_end=True,                         # when using a training metric
        # train_time_interval=,
        # every_n_train_steps=,
        # save_last=False,                                        # save last might be useful to have
    )
    callbacks.append(checkpoint_cb_every)

    checkpoint_cb_bestk = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="best",
        save_top_k=1, 
        monitor='val_auc',
        mode='max',
        # save_on_train_epoch_end=False,     # when using a training metric
        # save_last=False,
    )
    callbacks.append(checkpoint_cb_bestk)

    ## early stopping
    early_stopping = pl.callbacks.EarlyStopping(
        monitor='val_auc',
        mode='max',
        patience=args.early_stopping_patience,
        verbose=True,
    )
    callbacks.append(early_stopping)

    ###########################
    # TRAINER
    ###########################

    # may increase performance but lead to unstable training
    # torch.set_float32_matmul_precision("high")
    trainer = pl.Trainer(
        accelerator='gpu' if not args.debug else 'cpu',
        deterministic="warn" if args.seed is not None else False,
        # precision="16-mixed",   # reduce memory, can improve performance but might lead to unstable training
        
        max_epochs=args.epochs,
        # max_time="00:1:00:00",
        # max_steps=,

        check_val_every_n_epoch=args.evaluate_every,
        # val_check_interval=1.0,
        logger=logger,
        log_every_n_steps=10,
        callbacks=callbacks,

        fast_dev_run=args.debug,   # for testing training and validation
        # num_sanity_val_steps=0 if not args.debug else 2,
        limit_train_batches=1.0 if not args.debug else 0.01,  # to test what happens after an epoch
        # overfit_batches=0.01,

        # profiler='simple',    # advanced profiling to check for bottlenecks
    )

    ###########################
    # RUN MODEL
    ###########################

    # ## call tune to find lr and batch size
    # from lightning.pytorch.tuner import Tuner
    # tuner = pl.tuner.Tuner(trainer)
    # lr_finder = tuner.lr_find(model_task, train_dataloaders=train_dataloader)
    # print(lr_finder.results)
    # fig = lr_finder.plot(suggest=True)
    # fig.show()
    # # new_lr = lr_finder.suggestion()
    # # batch_size = tuner.scale_batch_size(model_task, train_dataloaders=train_dataloader)
    # return


    # fit the model
    print("Fitting model...")
    trainer.fit(
        model=model_task,
        train_dataloaders=train_dataloader,
        val_dataloaders=val_dataloader,
        ckpt_path=args.resume,
    )

    # get best threshold
    model = DetectorModel.load_from_checkpoint(checkpoint_cb_bestk.best_model_path).model
    score, threshold = ThresholdSelector()(model, val_dataloader)
    print(f"Best score: {score}")
    wrapper = TorchModel(load_pickle('data/feature_extractor.pkl'), model, use_features=use_features)
    wrapper.best_threshold = threshold
    wrapper.train = False
    wrapper.save(args.save)

    ## test model
    # trainer.test(
    #     model=model,
    #     dataloaders=test_dataloader, 
    #     ckpt_path=os.path.join(save_path, "best.ckpt"),
    #     verbose=True,
    # )


if __name__ == '__main__':
    main()
