from typing import List

import numpy as np
import torch
import torch.nn.parallel
import torch.optim
import torch.utils.data
import torch.utils.data.distributed
import torch.nn as nn
from defender.model import Model
import torch.nn.functional as F


class SelfAttention(nn.Module):
    def __init__(self, num_heads, out_features):
        super().__init__()
        self.num_heads = num_heads
        self.out_features = out_features

        self.head_size = out_features // num_heads

        self.query = nn.Linear(self.head_size, self.head_size)
        self.key = nn.Linear(self.head_size, self.head_size)
        self.value = nn.Linear(self.head_size, self.head_size)

        self.output = nn.Linear(out_features, out_features)

    def forward(self, x):
        assert len(x.shape) == 2

        batch_size = x.shape[0]
        x = x.view(batch_size, self.num_heads, self.head_size)

        query = self.query(x)
        key = self.key(x)
        value = self.value(x)

        score = torch.matmul(query, key.transpose(-2, -1)) / (self.head_size ** 0.5)
        attention = F.softmax(score, dim=-1)
        attended = torch.matmul(attention, value)

        attended = attended.view(batch_size, self.out_features)
        output = self.output(attended)

        return output


class FeedForward(nn.Module):
    def __init__(
            self,
            features_dim: List[int],
            hidden_dim: List[int],
            output_dim: int=1,
            dropout_rate: float=0.5,
        ):
        super().__init__()

        # embeddings for features
        hidden_dim = np.asarray(hidden_dim) * len(features_dim)
        l = hidden_dim[0]
        assert l % len(features_dim)==0
        self.embed_features = nn.ModuleList([nn.Sequential(
            nn.BatchNorm1d(k),
            nn.Linear(k, l // len(features_dim)),
        ) for k in features_dim])

        self.layers = []
        for h in hidden_dim[1:]:
            self.layers.append(nn.Sequential(
                nn.Linear(l,l),
                nn.ReLU(),
                nn.Linear(l,h),
                nn.ReLU(),
                nn.Linear(h,h),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
            ))
            l = h
        self.layers = nn.Sequential(*self.layers)

        self.output = self.ff = nn.Sequential(
            # nn.Linear(l, l),
            # nn.ReLU(inplace=True),
            nn.Linear(l, output_dim),
        )

    def forward(self, list_x):
        x = [f(x.float()) for x,f in zip(list_x,self.embed_features)]
        x = torch.concatenate(x, dim=1)     # B, dim
        x = x[None].transpose(0,1)     # B, 1, dim
        for f in self.layers:
            x = f(x)
        x = x[:,0,:]
        return self.output(x)[:,0]


class AttentionModelv2(nn.Module):
    class Block(nn.Module):
        class MultiheadSelfAttention(nn.MultiheadAttention):
            def forward(self, x, *args, **kwargs):
                return super().forward(
                    query=x,
                    key=x,
                    value=x,
                    *args,
                    **kwargs,
                )
        
        def __init__(self, in_dim, num_heads, dropout_rate, att_dropout, intermediate_dim=None):
            super().__init__()

            self.self_attn = self.MultiheadSelfAttention(
                embed_dim=in_dim,
                num_heads=num_heads,
                dropout=att_dropout,
                batch_first=True,
            )
            self.norm1 = nn.LayerNorm(in_dim)
            self.norm2 = nn.LayerNorm(in_dim)
            self.dropout1 = nn.Dropout(dropout_rate)
            self.dropout1_res = nn.Dropout(dropout_rate)

            if intermediate_dim is None:
                intermediate_dim = 2*in_dim
            self.ff1 = nn.Linear(in_dim, intermediate_dim)
            self.ff2 = nn.Linear(intermediate_dim, in_dim)
            # self.ff1 = nn.Conv1d(in_dim, intermediate_dim, kernel_size=1, bias=True)
            # self.ff2 = nn.Conv1d(intermediate_dim, in_dim, kernel_size=1, bias=True)

            self.norm3 = nn.LayerNorm(in_dim)
            self.dropout2 = nn.Dropout(dropout_rate)
            self.dropout2_res = nn.Dropout(dropout_rate)

        def forward(self,x):
            ## layer norm input
            x = self.norm1(x)       # B, n_features, hidden
            # x = F.tanh(x)
            ## self attention layer with dropout
            x2 = self.self_attn(x)[0]   # B, n_features, hidden
            x2 = self.dropout1(x2)
            ## residual connection
            # x = x + self.dropout1(self.norm2(x2))
            # x = self.norm2(self.dropout1_res(x) + self.dropout1(x2))
            x = self.norm2(self.dropout1_res(x) + x2) 
            # x = self.norm2(x + x2)

            ## feedforward
            x2 = self.ff2(F.gelu(self.ff1(x)))      # B, n_features, hidden
            x2 = self.dropout2(x2)
            
            ## residual connection
            # x = x + self.dropout2(self.norm3(x2))
            x = self.norm3(self.dropout2_res(x) + x2)
            # x = self.dropout2(self.norm3(x + x2))
            return x                            # B, n_features, hidden

    def __init__(
            self,
            features_dim: List[int],
            textual_dim: List[int],
            hidden_dim: int,
            n_layers: int,
            num_heads: int, 
            intermediate_dim: int = None,
            output_dim: int=1,
            dropout_rate: float=0.1,
            att_dropout: float=0.1,
            max_features_dim = 512,
            **kwargs,
        ):
        super().__init__()
        assert n_layers > 0
        assert hidden_dim % num_heads == 0

        features_dim = np.sum(features_dim)
        self.norm = nn.BatchNorm1d(features_dim)
        # MaxAbsScaler

        if features_dim > max_features_dim:
            in_features = features_dim
            features_dim = max_features_dim
            ## to reduce lenght of features through non-linear projection
            self.reduce = nn.Sequential(
                nn.Linear(in_features,features_dim * 2),
                nn.ReLU(inplace=True),
                nn.Linear(features_dim * 2,features_dim),
            )
        else:
            self.reduce = None

        ## embeddings for features
        # self.embed_features = nn.Sequential(
        #     nn.Conv1d(1, hidden_dim, kernel_size=1, bias=True),
        #     nn.Dropout(dropout_rate),
        # )
        self.embed_features = nn.Sequential(
            nn.Linear(1, hidden_dim, bias=True),
            nn.Dropout(dropout_rate),
        )

        # todo add textual embeddings
        # self.embed = nn.ModuleList()
        # for k in range(feature_embed):
        #     self.embed.append(nn.Embedding(features_dim[k], hidden_dim))

        # self.embed_features = nn.ModuleList([nn.Sequential(
        #     # todo layer norm to give same importance to all features
        #     # nn.BatchNorm1d(k),
        #     nn.Linear(1, hidden_dim),
        #     nn.Dropout(dropout_rate),
        # ) for k in range(features_dim)])
        # self.embed_features = nn.ModuleList([nn.Sequential(
        #     nn.LayerNorm(features_dim),
        #     nn.Linear(features_dim, features_dim * hidden_dim),
        #     nn.Dropout(dropout_rate),
        # )])

        self.layers = nn.ModuleList()
        for idx in range(n_layers):
            self.layers.append(self.Block(
                in_dim=hidden_dim,
                intermediate_dim=intermediate_dim,
                num_heads=num_heads,
                dropout_rate=dropout_rate,
                att_dropout=att_dropout,
                # dropout_rate=dropout_rate if idx != n_layers-1 else 0.0,
                # att_dropout=att_dropout if idx != n_layers-1 else 0.0,
            ))

        self.output = nn.Sequential(
            nn.Linear(hidden_dim * features_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, output_dim),
        )

    def forward(self, list_x, list_text_x=None, **kwargs):
        x = torch.concat(list_x, dim=1).float()     # B, n_features
        x = self.norm(x)        
        if self.reduce is not None:
            x = self.reduce(x)
        x = self.embed_features(x[...,None])      # B, n_features, hidden

        # x = [f(x.float()) for x,f in zip(list_x,self.embed_features)]
        # x = torch.concatenate(x, dim=1)     # B, dim
        # x = x[None].transpose(0,1)     # B, 1, dim

        for f in self.layers:
            x = f(x)             # B, n_features, hidden
        # x = x[:,0,:]
        x = x.flatten(1)
        return self.output(x)[:,0]


# todo! wrong implementation, kept for saved models (works as FFN)
class AttentionModel(nn.Module):
    class Block(nn.Module):
        class MultiheadSelfAttention(nn.MultiheadAttention):
            def forward(self, x, *args, **kwargs):
                return super().forward(
                    query=x,
                    key=x,
                    value=x,
                    *args,
                    **kwargs,
                )
        
        def __init__(self, in_dim, h_dim, num_heads, dropout_rate, att_dropout):
            super().__init__()

            self.self_attn = self.MultiheadSelfAttention(
                embed_dim=in_dim,
                num_heads=num_heads,
                dropout=att_dropout,
                batch_first=True,
            )
            self.norm1 = nn.LayerNorm(in_dim)
            self.norm2 = nn.LayerNorm(in_dim)
            self.dropout1 = nn.Dropout(dropout_rate)
            self.dropout1_res = nn.Dropout(dropout_rate)

            self.ff1 = nn.Linear(in_dim, 2*h_dim)
            self.ff2 = nn.Linear(2*h_dim, in_dim)

            self.norm3 = nn.LayerNorm(in_dim)
            self.dropout2 = nn.Dropout(dropout_rate)
            self.dropout2_res = nn.Dropout(dropout_rate)

        def forward(self,x):
            ## layer norm input
            x = self.norm1(x)
            # x = F.tanh(x)
            ## self attention layer with dropout
            x2 = self.self_attn(x)[0]
            x2 = self.dropout1(x2)
            ## residual connection
            # x = x + self.dropout1(self.norm2(x2))
            # x = self.norm2(self.dropout1_res(x) + self.dropout1(x2))
            x = self.norm2(self.dropout1_res(x) + x2)
            # x = self.norm2(x + x2)

            ## feedforward
            x2 = self.ff2(F.gelu(self.ff1(x)))
            x2 = self.dropout2(x2)
            
            ## residual connection
            # x = x + self.dropout2(self.norm3(x2))
            x = self.norm3(self.dropout2_res(x) + x2)
            # x = self.dropout2(self.norm3(x + x2))
            return x

    def __init__(
            self,
            features_dim: List[int],
            hidden_dim: List[int],
            num_heads: int, 
            output_dim: int=1,
            dropout_rate: float=0.5,
            att_dropout: float=0.0,
            **kwargs,
        ):
        super().__init__()

        # embeddings for features
        # hidden_dim = np.asarray(hidden_dim) * len(features_dim)
        features_dim = [np.asarray(features_dim).sum()]
        l = hidden_dim[0]
        assert l % len(features_dim)==0
        self.embed_features = nn.ModuleList([nn.Sequential(
            nn.BatchNorm1d(k),
            nn.Linear(k, l // len(features_dim)),
            nn.Dropout(dropout_rate),
        ) for k in features_dim])

        self.layers = []
        for idx, h in enumerate(hidden_dim[1:]):
            self.layers.append(self.Block(
                in_dim=l,
                h_dim=h,
                num_heads=num_heads,
                dropout_rate=dropout_rate if idx != len(hidden_dim)-2 else 0.0,
                att_dropout=att_dropout,
            ))
            # l = h
        self.layers = nn.Sequential(*self.layers)

        self.output = self.ff = nn.Sequential(
            # nn.Linear(l, l),
            # nn.ReLU(inplace=True),
            nn.Linear(l, output_dim),
        )

    def forward(self, list_x, **kwargs):
        list_x = [torch.concat(list_x, dim=1)]
        x = [f(x.float()) for x,f in zip(list_x,self.embed_features)]
        x = torch.concatenate(x, dim=1)     # B, dim
        x = x[None].transpose(0,1)     # B, 1, dim
        for f in self.layers:
            x = f(x)
        x = x[:,0,:]
        return self.output(x)[:,0]


class TorchModel(Model):
    def __init__(
            self, 
            feature_extractor,
            model,
            use_features=None,
        ):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.model = model
        self.use_features = use_features

    def extract_features(self, pe_files: List[str]) -> List[np.array]:
        features, parsed_errors = self.feature_extractor(pe_files, enable_extractor=self.use_features, train=self.train)
        if len(parsed_errors) != 0:
            raise Exception("Some files were not parsed")
        return [torch.as_tensor(x.toarray()).float() for x in features]

    def __call__(self, x: List[np.array]) -> np.array:
        return self.model(x)

    @Model.train.setter
    def train(self, v):
        super().train(v)
        self.model.train(v)

    @torch.no_grad()
    def predict(self, x: List[np.array]):
        p = self(x).detach().cpu().numpy()
        return (p >= self.best_threshold).astype(int)

    @staticmethod
    def _init_parameters(d):
        return {'feature_extractor': d['feature_extractor'], 'model': d['model']}
