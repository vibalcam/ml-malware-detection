from typing import List

import numpy as np
import torch
import torch.nn.parallel
import torch.optim
import torch.utils.data
import torch.utils.data.distributed
import torch.nn as nn
from defender.model import Model
import torch.nn.functional as F



# class SelfAttention(nn.Module):
#     def __init__(self, num_heads, out_features):
#         super().__init__()
#         self.num_heads = num_heads
#         self.out_features = out_features

#         self.head_size = out_features // num_heads

#         self.query = nn.Linear(self.head_size, self.head_size)
#         self.key = nn.Linear(self.head_size, self.head_size)
#         self.value = nn.Linear(self.head_size, self.head_size)

#         self.output = nn.Linear(out_features, out_features)

#     def forward(self, x):
#         assert len(x.shape) == 2

#         batch_size = x.shape[0]
#         x = x.view(batch_size, self.num_heads, self.head_size)

#         query = self.query(x)
#         key = self.key(x)
#         value = self.value(x)

#         score = torch.matmul(query, key.transpose(-2, -1)) / (self.head_size ** 0.5)
#         attention = F.softmax(score, dim=-1)
#         attended = torch.matmul(attention, value)

#         attended = attended.view(batch_size, self.out_features)
#         output = self.output(attended)

#         return output


# class FeedForward(nn.Module):
#     def __init__(
#             self,
#             features_dim: List[int],
#             hidden_dim: List[int],
#             num_heads: int, 
#             output_dim: int=1,
#             dropout_rate: float=0.5,
#             att_dropout: float=0.0,
#         ):
#         super().__init__()

#         # embeddings for features
#         hidden_dim = np.asarray(hidden_dim) * len(features_dim)
#         l = hidden_dim[0]
#         assert l % len(features_dim)==0
#         self.embed_features = nn.ModuleList([nn.Sequential(
#             nn.BatchNorm1d(k),
#             nn.Linear(k, l // len(features_dim)),
#         ) for k in features_dim])

#         self.layers = []
#         for h in hidden_dim[1:]:
#             self.layers.append(self.Block(
#                 in_dim=l,
#                 out_dim=h,
#                 num_heads=num_heads,
#                 dropout_rate=dropout_rate,
#                 att_dropout=att_dropout,
#             ))
#             l = h
#         self.layers = nn.Sequential(*self.layers)

#         self.output = self.ff = nn.Sequential(
#             # nn.Linear(l, l),
#             # nn.ReLU(inplace=True),
#             nn.Linear(l, output_dim),
#         )

#     def forward(self, list_x):
#         x = [f(x.float()) for x,f in zip(list_x,self.embed_features)]
#         x = torch.concatenate(x, dim=1)     # B, dim
#         x = x[None].transpose(0,1)     # B, 1, dim
#         for f in self.layers:
#             x = f(x)
#         x = x[:,0,:]
#         return self.output(x)[:,0]


class AttentionModel(nn.Module):
    class Block(nn.Module):
        class MultiheadSelfAttention(nn.MultiheadAttention):
            def forward(self, x, *args, **kwargs):
                return super().forward(
                    query=x,
                    key=x,
                    value=x,
                    *args,
                    **kwargs,
                )
        
        def __init__(self, in_dim, h_dim, num_heads, dropout_rate, att_dropout):
            super().__init__()

            self.self_attn = self.MultiheadSelfAttention(
                embed_dim=in_dim,
                num_heads=num_heads,
                dropout=att_dropout,
                batch_first=True,
            )
            self.norm1 = nn.LayerNorm(in_dim)
            self.norm2 = nn.LayerNorm(in_dim)
            self.dropout1 = nn.Dropout(dropout_rate)

            self.ff1 = nn.Linear(in_dim, h_dim)
            self.ff2 = nn.Linear(h_dim, in_dim)

            self.norm3 = nn.LayerNorm(in_dim)
            self.dropout2 = nn.Dropout(dropout_rate)

        def forward(self,x):
            # self attention
            x = self.norm1(x)
            # x = F.tanh(x)
            x2 = self.self_attn(x)[0]
            # residual connection
            # x = x + self.dropout1(self.norm2(x2))
            # x = self.norm2(x + self.dropout1(x2))
            x = self.norm2(x + x2)

            # feedforward
            x2 = self.ff2(F.gelu(self.ff1(x)))
            
            # residual connection
            # x = x + self.dropout2(self.norm3(x2))
            # x = self.norm3(x + self.dropout2(x2))
            x = self.dropout2(self.norm3(x + x2))
            return x

    def __init__(
            self,
            features_dim: List[int],
            hidden_dim: List[int],
            num_heads: int, 
            output_dim: int=1,
            dropout_rate: float=0.5,
            att_dropout: float=0.0,
        ):
        super().__init__()

        # embeddings for features
        # hidden_dim = np.asarray(hidden_dim) * len(features_dim)
        features_dim = [np.asarray(features_dim).sum()]
        l = hidden_dim[0]
        assert l % len(features_dim)==0
        self.embed_features = nn.ModuleList([nn.Sequential(
            nn.BatchNorm1d(k),
            nn.Linear(k, l // len(features_dim)),
        ) for k in features_dim])

        self.layers = []
        for h in hidden_dim[1:]:
            self.layers.append(self.Block(
                in_dim=l,
                h_dim=h,
                num_heads=num_heads,
                dropout_rate=dropout_rate,
                att_dropout=att_dropout,
            ))
            l = h
        self.layers = nn.Sequential(*self.layers)

        self.output = self.ff = nn.Sequential(
            nn.Linear(l, l),
            nn.ReLU(inplace=True),
            nn.Linear(l, output_dim),
        )

    def forward(self, list_x):
        list_x = [torch.concat(list_x, dim=1)]
        x = [f(x.float()) for x,f in zip(list_x,self.embed_features)]
        x = torch.concatenate(x, dim=1)     # B, dim
        x = x[None].transpose(0,1)     # B, 1, dim
        for f in self.layers:
            x = f(x)
        x = x[:,0,:]
        return self.output(x)[:,0]


class TorchModel(Model):
    def __init__(
            self, 
            feature_extractor,
            model,
            use_features=None,
        ):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.model = model
        self.use_features = use_features

    def extract_features(self, pe_files: List[str]) -> List[np.array]:
        features, parsed_errors = self.feature_extractor(pe_files, enable_extractor=self.use_features, train=self.train)
        if len(parsed_errors) != 0:
            raise Exception("Some files were not parsed")
        return [torch.as_tensor(x.toarray()).float() for x in features]

    def __call__(self, x: List[np.array]) -> np.array:
        return self.model(x)

    @Model.train.setter
    def train(self, v):
        super().train
        self.model.train(v)

    @torch.no_grad()
    def predict(self, x: List[np.array]):
        p = self(x).detach().cpu().numpy()
        return (p >= self.best_threshold).astype(int)

    @staticmethod
    def _init_parameters(d):
        return {'feature_extractor': d['feature_extractor'], 'model': d['model']}
