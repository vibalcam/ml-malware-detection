from typing import List
import warnings

import numpy as np
import torch
import torch.nn.parallel
import torch.optim
import torch.utils.data
import torch.utils.data.distributed
import torch.nn as nn
from defender.model import Model
import torch.nn.functional as F


class SelfAttention(nn.Module):
    def __init__(self, num_heads, out_features):
        super().__init__()
        self.num_heads = num_heads
        self.out_features = out_features

        self.head_size = out_features // num_heads

        self.query = nn.Linear(self.head_size, self.head_size)
        self.key = nn.Linear(self.head_size, self.head_size)
        self.value = nn.Linear(self.head_size, self.head_size)

        self.output = nn.Linear(out_features, out_features)

    def forward(self, x):
        assert len(x.shape) == 2

        batch_size = x.shape[0]
        x = x.view(batch_size, self.num_heads, self.head_size)

        query = self.query(x)
        key = self.key(x)
        value = self.value(x)

        score = torch.matmul(query, key.transpose(-2, -1)) / (self.head_size ** 0.5)
        attention = F.softmax(score, dim=-1)
        attended = torch.matmul(attention, value)

        attended = attended.view(batch_size, self.out_features)
        output = self.output(attended)

        return output


class FeedForward(nn.Module):
    def __init__(
            self,
            features_dim: List[int],
            hidden_dim: List[int],
            output_dim: int=1,
            dropout_rate: float=0.5,
        ):
        super().__init__()

        # embeddings for features
        hidden_dim = np.asarray(hidden_dim) * len(features_dim)
        l = hidden_dim[0]
        assert l % len(features_dim)==0
        self.embed_features = nn.ModuleList([nn.Sequential(
            nn.BatchNorm1d(k),
            nn.Linear(k, l // len(features_dim)),
        ) for k in features_dim])

        self.layers = []
        for h in hidden_dim[1:]:
            self.layers.append(nn.Sequential(
                nn.Linear(l,l),
                nn.ReLU(),
                nn.Linear(l,h),
                nn.ReLU(),
                nn.Linear(h,h),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
            ))
            l = h
        self.layers = nn.Sequential(*self.layers)

        self.output = self.ff = nn.Sequential(
            # nn.Linear(l, l),
            # nn.ReLU(inplace=True),
            nn.Linear(l, output_dim),
        )

    def forward(self, list_x):
        x = [f(x.float()) for x,f in zip(list_x,self.embed_features)]
        x = torch.concatenate(x, dim=1)     # B, dim
        x = x[None].transpose(0,1)     # B, 1, dim
        for f in self.layers:
            x = f(x)
        x = x[:,0,:]
        return self.output(x)[:,0]


class AttentionModelv2(nn.Module):
    class Block(nn.Module):
        class MultiheadSelfAttention(nn.MultiheadAttention):
            def forward(self, x, *args, **kwargs):
                return super().forward(
                    query=x,
                    key=x,
                    value=x,
                    *args,
                    **kwargs,
                )
        
        def __init__(self, in_dim, num_heads, dropout_rate, att_dropout, intermediate_dim=None):
            super().__init__()

            self.self_attn = self.MultiheadSelfAttention(
                embed_dim=in_dim,
                num_heads=num_heads,
                dropout=att_dropout,
                batch_first=True,
            )
            self.norm1 = nn.LayerNorm(in_dim)
            self.dropout1 = nn.Dropout(dropout_rate)
            # self.dropout1_res = nn.Dropout(dropout_rate)

            self.norm2 = nn.LayerNorm(in_dim)
            self.dropout2 = nn.Dropout(dropout_rate)
            if intermediate_dim is None:
                intermediate_dim = 2*in_dim
            self.ff1 = nn.Linear(in_dim, intermediate_dim)
            self.ff2 = nn.Linear(intermediate_dim, in_dim)
            # self.ff1 = nn.Conv1d(in_dim, intermediate_dim, kernel_size=1, bias=True)
            # self.ff2 = nn.Conv1d(intermediate_dim, in_dim, kernel_size=1, bias=True)

            # self.norm3 = nn.LayerNorm(in_dim)
            # self.dropout2_res = nn.Dropout(dropout_rate)

        def forward(self,x):
            res = x
            x = self.norm1(x)       # B, n_features, hidden

            x = self.self_attn(x)[0]   # B, n_features, hidden
            x = self.dropout1(x)
            x = x + res

            res = x
            x = self.norm2(x)
            x = self.ff2(F.gelu(self.ff1(x)))      # B, n_features, hidden
            x = self.dropout2(x)
            x = x + res

            return x                            # B, n_features, hidden

    def __init__(
            self,
            features_dim: List[int],
            use_features: List[int],
            textual_idx: List[int],
            hidden_dim: int,
            n_layers: int,
            num_heads: int, 
            intermediate_dim: int = None,
            output_dim: int=1,
            dropout_rate: float=0.1,
            att_dropout: float=0.1,
            max_num_features = 300,
            # max_textual_len = 200,
            **kwargs,
        ):
        super().__init__()
        self.use_features = use_features
        self.textual_idx = textual_idx if textual_idx is not None else []
        self.is_numeric = np.asarray([k not in textual_idx for k in use_features])

        assert n_layers > 0
        assert hidden_dim % num_heads == 0
        assert len(features_dim) == len(use_features)

        features_dim = np.asarray(features_dim)
        num_features_dim = np.sum(features_dim[self.is_numeric])

        self.num_norm = nn.BatchNorm1d(num_features_dim)
        # self.num_norm = nn.LayerNorm(num_features_dim)

        # reduce lenght of features through non-linear projection
        # or use PCA
        if num_features_dim > max_num_features:
            in_features = num_features_dim
            num_features_dim = max_num_features
            inter = (in_features - num_features_dim) // 2
            self.reduce = nn.Sequential(
                nn.Linear(in_features, inter),
                nn.ReLU(inplace=True),
                nn.Linear(inter, num_features_dim),
            )
        else:
            self.reduce = None

        ## embeddings for features
        # self.embed_features = nn.Sequential(
        #     nn.Conv1d(1, hidden_dim, kernel_size=1, bias=True),
        #     nn.Dropout(dropout_rate),
        # )

        # todo try without linear
        ## embed features
        ## numeric embedding will repeat its value or use linear
        ## textual will use embeddings
        # self.embed_num = None
        self.embed_num = nn.ModuleList()
        for _ in range(num_features_dim):
            self.embed_num.append(nn.Sequential(
                nn.Linear(1, hidden_dim, bias=True),
                # nn.Dropout(dropout_rate),
            ))

        self.embed_textual = nn.ModuleList()
        for dim in features_dim[~self.is_numeric]:
            self.embed_textual.append(
                nn.Embedding(dim+1, hidden_dim, padding_idx=0),
            )
            # self.textual_red = nn.LSTM(
            #     max_textual_len
            # )

        self.embed_type = nn.Embedding((~self.is_numeric).sum()+2, hidden_dim)
        self.embed_ln = nn.LayerNorm(hidden_dim)

        ## attention layers
        self.layers = nn.ModuleList()
        for _ in range(n_layers):
            self.layers.append(self.Block(
                in_dim=hidden_dim,
                intermediate_dim=intermediate_dim,
                num_heads=num_heads,
                dropout_rate=dropout_rate,
                att_dropout=att_dropout,
                # dropout_rate=dropout_rate if idx != n_layers-1 else 0.0,
                # att_dropout=att_dropout if idx != n_layers-1 else 0.0,
            ))

        self.out_ln = nn.LayerNorm(hidden_dim)
        self.output = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, output_dim),
        )

    # Workaround to get device
    @property
    def device(self):
        return next(self.parameters()).device

    def forward(self, list_x, given_idx=False, **kwargs):
        # todo limit number of features (libraries...)

        ## input numeric
        x_num = torch.concat([k for idx,k in enumerate(list_x) if self.is_numeric[idx]], dim=1).float()    # B, num_features
        x_num = self.num_norm(x_num)
        if self.reduce is not None:
            x_num = self.reduce(x_num)
        
        ## input textual
        if given_idx:
            x_tex = [list_x[k].int() for k in (~self.is_numeric).nonzero()[0]]
        else:
            x_tex = []
            for idx,k in enumerate(list_x):     # list(B, length)
                if self.is_numeric[idx]:
                    continue
                if k.is_sparse:
                    k = k.to_dense()
                x_tex.append(get_nonzero_idx(k, self.device))

        ## embed numeric
        x_num = x_num[...,None]     # B, num_features, 1
        x_num = torch.stack([f(x_num[:,idx,:]) for idx,f in enumerate(self.embed_num)], dim=1)      # B, num_features, hidden
        ## embed textual
        x_tex = [f(x_tex[idx]) for idx,f in enumerate(self.embed_textual)]          #list(B,length,hidden)
        
        ## type embedding like positional embedding        
        batch = x_num.shape[0]
        x_res = self.embed_type(torch.zeros(batch,1, dtype=torch.long, device=self.device))
        x_num = x_num + self.embed_type(torch.ones(batch, x_num.shape[1], dtype=torch.long, device=self.device))
        x_tex = [k + (idx+2) * self.embed_type(torch.ones(batch, k.shape[1], dtype=torch.long, device=self.device)) for idx,k in enumerate(x_tex)]
        
        ## combine all
        x_tex.append(x_num)
        x_tex.append(x_res)
        x = torch.concat(x_tex, dim=1)      # B, length, hidden
        x = self.embed_ln(x)

        if self.training and x.isnan().any():
            warnings.warn("Nan values before att")

        for f in self.layers:
            x = f(x)             # B, length, hidden
            if self.training and x.isnan().any():
                warnings.warn(f"Nan values during att")

        ## use last element to make predictions
        x = self.out_ln(x[:,-1,:])
        x = self.output(x)
        if self.training and x.isnan().any():
            warnings.warn("Nan values on ouput")
        return x[:,0]


# todo! provide values to weight
def get_nonzero_idx(k: torch.Tensor, device=None):
    t = k>0
    idx_nonzero = t.nonzero()
    # padding is idx 0, so we add 1 to all values
    idx_nonzero[:, 1] += 1
    t = t.sum(1).max()
    row_indices = [idx_nonzero[idx_nonzero[:, 0] == i][:, 1] for i in range(k.shape[0])]
    row_indices = [torch.cat((torch.zeros(t - len(ri), dtype=torch.long, device=device), ri)) for ri in row_indices]
    return torch.stack(row_indices)


# todo! wrong implementation, kept for saved models (works as FFN)
class AttentionModel(nn.Module):
    class Block(nn.Module):
        class MultiheadSelfAttention(nn.MultiheadAttention):
            def forward(self, x, *args, **kwargs):
                return super().forward(
                    query=x,
                    key=x,
                    value=x,
                    *args,
                    **kwargs,
                )
        
        def __init__(self, in_dim, h_dim, num_heads, dropout_rate, att_dropout):
            super().__init__()

            self.self_attn = self.MultiheadSelfAttention(
                embed_dim=in_dim,
                num_heads=num_heads,
                dropout=att_dropout,
                batch_first=True,
            )
            self.norm1 = nn.LayerNorm(in_dim)
            self.norm2 = nn.LayerNorm(in_dim)
            self.dropout1 = nn.Dropout(dropout_rate)
            self.dropout1_res = nn.Dropout(dropout_rate)

            self.ff1 = nn.Linear(in_dim, 2*h_dim)
            self.ff2 = nn.Linear(2*h_dim, in_dim)

            self.norm3 = nn.LayerNorm(in_dim)
            self.dropout2 = nn.Dropout(dropout_rate)
            self.dropout2_res = nn.Dropout(dropout_rate)

        def forward(self,x):
            ## layer norm input
            x = self.norm1(x)
            # x = F.tanh(x)
            ## self attention layer with dropout
            x2 = self.self_attn(x)[0]
            x2 = self.dropout1(x2)
            ## residual connection
            # x = x + self.dropout1(self.norm2(x2))
            # x = self.norm2(self.dropout1_res(x) + self.dropout1(x2))
            x = self.norm2(self.dropout1_res(x) + x2)
            # x = self.norm2(x + x2)

            ## feedforward
            x2 = self.ff2(F.gelu(self.ff1(x)))
            x2 = self.dropout2(x2)
            
            ## residual connection
            # x = x + self.dropout2(self.norm3(x2))
            x = self.norm3(self.dropout2_res(x) + x2)
            # x = self.dropout2(self.norm3(x + x2))
            return x

    def __init__(
            self,
            features_dim: List[int],
            hidden_dim: List[int],
            num_heads: int, 
            output_dim: int=1,
            dropout_rate: float=0.5,
            att_dropout: float=0.0,
            **kwargs,
        ):
        super().__init__()

        # embeddings for features
        # hidden_dim = np.asarray(hidden_dim) * len(features_dim)
        features_dim = [np.asarray(features_dim).sum()]
        l = hidden_dim[0]
        assert l % len(features_dim)==0
        self.embed_features = nn.ModuleList([nn.Sequential(
            nn.BatchNorm1d(k),
            nn.Linear(k, l // len(features_dim)),
            nn.Dropout(dropout_rate),
        ) for k in features_dim])

        self.layers = []
        for idx, h in enumerate(hidden_dim[1:]):
            self.layers.append(self.Block(
                in_dim=l,
                h_dim=h,
                num_heads=num_heads,
                dropout_rate=dropout_rate if idx != len(hidden_dim)-2 else 0.0,
                att_dropout=att_dropout,
            ))
            # l = h
        self.layers = nn.Sequential(*self.layers)

        self.output = self.ff = nn.Sequential(
            # nn.Linear(l, l),
            # nn.ReLU(inplace=True),
            nn.Linear(l, output_dim),
        )

    def forward(self, list_x, **kwargs):
        list_x = [torch.concat(list_x, dim=1)]
        x = [f(x.float()) for x,f in zip(list_x,self.embed_features)]
        x = torch.concatenate(x, dim=1)     # B, dim
        x = x[None].transpose(0,1)     # B, 1, dim
        for f in self.layers:
            x = f(x)
        x = x[:,0,:]
        return self.output(x)[:,0]


class TorchModel(Model):
    def __init__(
            self, 
            feature_extractor,
            model,
            use_features=None,
        ):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.model = model
        self.use_features = use_features

    def extract_features(self, pe_files: List[str]) -> List[np.array]:
        features, parsed_errors = self.feature_extractor(pe_files, enable_extractor=self.use_features, train=self.train)
        if len(parsed_errors) != 0:
            raise Exception("Some files were not parsed")
        
        # todo! check if textual unknown features are present to return malware

        return [torch.as_tensor(x.toarray()).float() for x in features]

    def __call__(self, x: List[np.array]) -> np.array:
        x = [k.to(self.model.device) for k in x]
        return self.model(x)

    @Model.train.setter
    def train(self, v):
        self._train = v
        self.model.train(v)

    @torch.no_grad()
    def predict(self, x: List[np.array]):
        p = self(x).detach().cpu()
        p = torch.sigmoid(p)
        return (p >= self.best_threshold).int()

    # def save(self, path):
    #     torch.save(self, path)
    
    # @staticmethod
    # def load(path, cls=None):
    #     obj = torch.load(path)
    #     obj.model = obj.model.cpu()
    #     obj.train = False

    @staticmethod
    def _init_parameters(d):
        return {'feature_extractor': d['feature_extractor'], 'model': d['model']}
    