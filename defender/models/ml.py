import glob
from typing import List, Union
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from tqdm.auto import tqdm
from sklearn.model_selection import KFold, GridSearchCV, LeaveOneOut, RandomizedSearchCV
import os
import defender.models.ember_init as ember
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA, IncrementalPCA
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MaxAbsScaler, StandardScaler
from defender.model import Model, competition_scorer, find_best_score
from sklearn.metrics import classification_report, confusion_matrix

from defender.utils import batched_fit_transform, create_batches, load_pickle
from sklearn.metrics import make_scorer
import warnings
import random
import lightgbm as lgb
import xgboost
from scipy import sparse


import argparse
parser = argparse.ArgumentParser()

parser.add_argument('-n', '--pca_components', type=int, nargs='+', default=[200,200,200,200,200], help='number of components for PCA')
parser.add_argument('--scaler', type=int, nargs='+', default=[0,1,2,3,4], help='input features to scale')
# parser.add_argument('-f', '--use_features', nargs='+', type=int, default=[0], help='features to use from the feature extractor')
parser.add_argument('-f', '--use_features', type=int, nargs='+', default=[0,1,2,3,4], help='features to use from the feature extractor')
parser.add_argument('-s', '--save', type=str, default='defender/rf.pkl', help='file where to save model')
parser.add_argument('-e', '--evaluate', type=str, default=None, help='model to evaluate')

# python -m defender.models.ml && python test.py -m defender/ml.pkl

"""
try no program
try more malware
try with rito
Try MLSEC
"""

class MLClassifier(Model):
    tmp_file = f"tmp_calc/rf_tmp_x"

    def log(self, s):
        if self.verbose:
            print(s)

    @staticmethod
    def _init_parameters(d):
        return {'feature_extractor': d['feature_extractor']}
    
    def __init__(
            self, 
            feature_extractor, 
            scale_input: Union[bool, List[int]] = True,
            use_features: List[int] = [0],
            pca_components: Union[int, List[int]] = None,
            verbose=False,
            args = None,
        ) -> None:
        super().__init__()

        self.feature_extractor = feature_extractor
        self.use_features = use_features
        self.verbose = verbose
        self.args = args

        # scaler_type = MaxAbsScaler
        scaler_type = StandardScaler
        if isinstance(scale_input, bool):
            self.scaler = {k: scaler_type() for k in use_features}
        else:
            self.scaler = {k: scaler_type() for k in scale_input}
        
        if pca_components is not None:
            if isinstance(pca_components, int):
                self.pca = {k: IncrementalPCA(n_components=pca_components, batch_size=pca_components) for k in use_features}
            else:
                self.pca = {idx: IncrementalPCA(n_components=k, batch_size=k) for idx,k in enumerate(pca_components)}
        else:
            self.pca = {}

    def extract_features(self, pe_files: List[str]):
        # return self.feature_extractor(pe_files, train=self.train)
        # todo fix enable extractor
        features, parsed_errors = self.feature_extractor(pe_files, enable_extractor=self.use_features, train=self.train)
        if len(parsed_errors) != 0:
            raise Exception("Some files were not parsed")
        return features

    def __call__(self, x) -> np.array:
        x = self.pre_process(x)
        return self.pipe_grid.predict_proba(x)[:,1]
    
    def pre_process(self, sparse_x):
        warnings.warn(f"Training mode is set to {self.train}")

        x = []
        for idx, k in enumerate(sparse_x):
            # use memmap to handle large dataset during training
            if self.train:
                if isinstance(k, sparse.csr_matrix):
                    x_k = np.memmap(self.tmp_file + str(idx), mode="w+", dtype=k.dtype, offset=0, shape=k.shape)
                    for arr,start,end in create_batches(k):
                        x_k[start:end,...] = arr.toarray()
                else:
                    x_k = k
            else:
                x_k = k.toarray()

            # scale data
            scaler = self.scaler.get(idx, None)
            if scaler is not None:
                self.log("Scaling data...")
                if self.train:
                    x_k = batched_fit_transform(x_k, scaler, batch_size=2000)
                else:
                    x_k = scaler.transform(x_k)

            # apply pca
            pca = self.pca.get(idx, None) if self.pca is not None else self.pca
            if pca is not None:
                self.log("Apply pca...")
                
                # if it has more features than required, apply PCA
                if x_k.shape[1] > pca.n_components:
                    if self.train:
                        x_k = batched_fit_transform(x_k, pca, batch_size=max(pca.n_components, 400), combine_last=True)
                        self.log(pca.explained_variance_ratio_.sum())

                        # n_components = (self.pca.explained_variance_ratio_.cumsum() > 0.9).nonzero()[0][0]
                        # x = x[:, :n_components]
                        # self.pca.n_components = n_components
                    else:
                        x_k = pca.transform(x_k)
            
            x.append(x_k)

        if len(x) == 1:
            return np.asarray(x[0])
        else: 
            return np.concatenate(x, axis=1)

    def fit(self, x, y, n_jobs=2, cv=3):
        super().fit()

        self.train = True
        x = self.pre_process(x)

        # Define hyperparameters grid for all classifiers
        pipeline = Pipeline([('classifier', None)])
        params = [
            # {
            #     'classifier': [xgboost.XGBClassifier()],
            #     'classifier__objective': ['binary:logistic'],
            #     'classifier__booster': ['gbtree', 'dart'],
            #     'classifier__n_estimators': [500, 900, 1200],
            #     'classifier__learning_rate': [0.01, 0.1],
            #     'classifier__max_depth': [40, 50, 60],
            #     'classifier__subsample': [0.5, 0.8, 1.0],
            #     'classifier__colsample_bytree': [0.5, 0.8, 1.0],
            #     'classifier__gamma': [0, 1, 5],
            #     'classifier__reg_alpha': [0, 0.1, 1],
            #     'classifier__reg_lambda': [0, 0.1, 1],
            # },
            # {
            #     'classifier': [lgb.LGBMClassifier()],
            #     'classifier__boosting_type': ['gbdt'],
            #     'classifier__objective': ['binary'],
            #     'classifier__num_iterations': [500, 1000],
            #     'classifier__learning_rate': [0.005, 0.05],
            #     'classifier__num_leaves': [512, 1024, 2048],
            #     'classifier__feature_fraction': [0.5, 0.8, 1.0],
            #     'classifier__bagging_fraction': [0.5, 0.8, 1.0],
            # },
            # {
            #     'classifier': [RandomForestClassifier(max_depth=None, random_state=1234)],
            #     'classifier__n_estimators': [600, 800, 1000, 1200],
            #     'classifier__max_depth': [40, 50, 60],
            #     'classifier__max_features': ['log2', 'sqrt'],
            # },
            {
                'classifier': [RandomForestClassifier(max_depth=None, random_state=1234)],
                'classifier__n_estimators': [1200, 1400],
                'classifier__max_depth': [40, 50, 60],
                # 'classifier__criterion': ['gini', 'entropy'],
                # 'classifier__min_samples_split': [2, 5],
                # 'classifier__min_samples_leaf': [1, 2],
                'classifier__max_features': ['log2'],
                # 'classifier__bootstrap': [True, False],
            },
        ] 

        # cross validation search
        self.log("Starting training...")
        self.pipe_grid = RandomForestClassifier(
            n_estimators=600,
            n_jobs=-1,
            # max_depth=50,
            # oob_score=True,
            # n_estimators=600,
            random_state=44444,
        ).fit(x,y)

        """
        Best currently
        ---------------
        estimators 600
        using features 1 2 3 4
        scaling MaxAbsScaler
        pca 200 200 200 200
        """

        # folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)
        # self.pipe_grid = GridSearchCV(
        #     pipeline, 
        #     params, 
        #     scoring=make_scorer(find_best_score, greater_is_better=True, needs_proba=True), 
        #     cv=StratifiedKFold(cv, shuffle=True, random_state=1234), 
        #     refit=True,
        #     verbose=4,
        #     n_jobs=n_jobs,
        # ).fit(x, y)

        # # Print results
        # cols = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
        # self.log(pd.DataFrame(self.pipe_grid.cv_results_)[cols].sort_values(by='rank_test_score').head().T)
        # self.log(f"Best parameters: {self.pipe_grid.best_params_}")
        # self.log(f"Best Matthews correlation coefficient: {self.pipe_grid.best_score_}")

        # choose best threshold
        best_score, self.best_threshold = find_best_score(
            y_true=y, 
            y_pred=self.pipe_grid.predict_proba(x)[:,1], 
            return_threshold=True,
        )
        self.log(f"Best threshold {self.best_threshold} with score {best_score}")


# todo! can only be used with one feature set
def get_datasets(paths, use_ember=True, use_features=[0]):
    assert len(use_features) == 1

    self_tmp_file = 'tmp_calc/ml'
    self_x = []
    self_y = []
    for idx_path, path in enumerate(paths):
        # load data
        list_sparse_x, y = load_pickle(path)
        for f in use_features:
            sparse_x = list_sparse_x[f]
            # save to files to not fill memory
            x_k = np.memmap(f"{self_tmp_file}_{idx_path}", mode="w+", dtype=sparse_x.dtype, offset=0, shape=sparse_x.shape)
            for arr,start,end in create_batches(sparse_x):
                x_k[start:end,...] = arr.toarray()
            self_x.append(x_k)
            self_y.append(y)
            del sparse_x
            del y

    # load additional data
    if use_ember:
        assert use_features == [0]
        # bodmas dataset
        filename = 'data/bodmas/bodmas.npz'
        data = np.load(filename)
        X = data['X']  # all the feature vectors
        y = data['y']  # labels, 0 as benign, 1 as malicious
        self_y.append(y)
        del y
        del data

        # save to files
        self_x.append(X)
        del X
        
        X_train, y_train, X_test, y_test = ember.read_vectorized_features("data/ember2018/")
        self_y.append(y_train)
        self_y.append(y_test)
        del y_train
        del y_test
        
        # save to files
        self_x.append(X_train)
        del X_train
        self_x.append(X_test)
        del X_test
    
    self_y = np.concatenate(self_y, axis=0)
    # x_fin = self_x[0]
    x = np.memmap(f"{self_tmp_file}", mode="w+", dtype=self_x[0].dtype, offset=0, shape=(np.sum([len(k) for k in self_x]), self_x[0].shape[1]))
    k = 0
    for x_k in self_x:
        for arr,start,end in create_batches(x_k):
            x[start+k:end+k,...] = arr
        k += len(x_k)

    x_fin = x

    # todo! needs fixing
    # # Filter unlabeled data
    # train_rows = (self_y != -1)
    # self_y = self_y[train_rows]
    # x_fin = np.memmap(f"{self_tmp_file}_final", mode="w+", dtype=x.dtype, offset=0, shape=(train_rows.sum(), x.shape[1]))
    # k = 0
    # for arr,start,end in create_batches(train_rows):
    #     x_batch = x[start:end][arr]
    #     x_fin[k:k+len(x_batch),...] = x_batch

    # check that shapes match
    assert len(self_y) == len(x_fin), "Shapes do not match"

    # check that no unlabeled data
    assert np.logical_and(self_y != 0, self_y != 1).sum() == 0, Exception("Unknown labels, it should be 0 or 1")

    return [x_fin], self_y


if __name__ == '__main__':
    args = parser.parse_args()

    if args.pca_components is not None and len(args.pca_components) == 1:
        args.pca_components = args.pca_components[0]

    if args.evaluate is None:
        # train and save model
        train_sparse_x, train_y = load_pickle('data_features_combined/full_train.pkl')
        train_sparse_x = [k for idx,k in enumerate(train_sparse_x) if idx in args.use_features]
        # test_sparse_x, test_y = load_pickle('data_features_combined/full_test.pkl')
        # test_sparse_x = [k for idx,k in enumerate(test_sparse_x) if idx in args.use_features]
        # sparse_x = [sparse.vstack([v1,v2]) for v1,v2 in zip(train_sparse_x, test_sparse_x)]
        # y = np.concatenate([train_y, test_y], axis=0)
        sparse_x = train_sparse_x
        y = train_y
        print(str(sparse_x))


        # files = []
        # for k in ['dike*', 'benign*', 'malwareZ*']:
        #     for t in ['train', 'test']:
        #         l = list(glob.glob(f"data_features_combined/**/{k}_{t}.pkl", recursive=True))
        #         l = [f for f in l if os.path.isfile(f)]
        #         if len(l) == 0:
        #             raise Exception("Some files are emtpy")
        #         files.extend(l)
        # # files = [f for f in files if os.path.isfile(f)]
        # sparse_x, y = get_datasets(files, False, args.use_features)
        
        feature_extractor = load_pickle('data_features_combined/full_feature_extractor.pkl')
        model = MLClassifier(
            feature_extractor, 
            scale_input=args.scaler,
            use_features=args.use_features,
            pca_components=args.pca_components,
            verbose=True,
            args=args,
        )
        model.fit(sparse_x,y, cv=2, n_jobs=None)
        model.save(args.save)

        args.evaluate = args.save

    # test results
    # sparse_x, y = load_pickle('data/test.pkl')
    # model = Model.load(args.evaluate)
    # model.train = False
    # print(model.evaluate(sparse_x,y))
    # model.save(args.evaluate)

    """
    Best parameters: {'classifier': RandomForestClassifier(max_depth=40, max_features='log2', n_estimators=1000, random_state=1234), 'classifier__max_depth': 40, 'classifier__max_features': 'log2', 'classifier__n_estimators': 1000}
    Best Matthews correlation coefficient: 0.9972280527186372
    Best threshold 0.36 with score 0.9999999988851729
    """


    # cols = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
    # print(results[cols].to_latex(index=False))

    # todo augment with packers
