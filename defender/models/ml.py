from typing import List, Union
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from tqdm.auto import tqdm
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold, GridSearchCV, LeaveOneOut, RandomizedSearchCV
import os
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA, IncrementalPCA
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from defender.model import Model, competition_scorer, find_best_score
from sklearn.metrics import classification_report, confusion_matrix

from defender.utils import DikePEDataset, batched_fit_transform, create_batches, load_pickle
from sklearn.metrics import make_scorer
import warnings
import random


class MLClassifier(Model):
    tmp_file = f"tmp_calc/{random.randint(0, 9999)}_tmp_x"

    @staticmethod
    def _init_parameters(d):
        return {'feature_extractor': d['feature_extractor']}
    
    def __init__(
            self, 
            feature_extractor, 
            scale_input: Union[bool, List[int]] = True,
            use_features: List[int] = [0],
            pca_components: Union[int, List[int]] = None,
        ) -> None:
        super().__init__()

        self.feature_extractor = feature_extractor
        self.use_features = use_features

        if isinstance(scale_input, bool):
            self.scaler = {k: StandardScaler() for k in use_features}
        else:
            self.scaler = {k: StandardScaler() for k in scale_input}
        
        if pca_components is not None:
            if isinstance(pca_components, int):
                self.pca = {k: IncrementalPCA(n_components=pca_components, batch_size=pca_components) for k in use_features}
            else:
                self.pca = {idx: IncrementalPCA(n_components=k, batch_size=k) for idx,k in enumerate(pca_components)}
        else:
            self.pca = {}

    def extract_features(self, pe_files: List[str]):
        # return self.feature_extractor(pe_files, train=self.train)
        # todo fix enable extractor
        features, parsed_errors = self.feature_extractor(pe_files, enable_extractor=None, train=self.train)
        if len(parsed_errors) != 0:
            raise Exception("Some files were not parsed")
        return features

    def __call__(self, x) -> np.array:
        x = self.pre_process(x)
        return self.pipe_grid.best_estimator_.predict_proba(x)[:,1]
    
    def pre_process(self, sparse_x):
        warnings.warn(f"Training mode is set to {self.train}")

        x = []
        for idx, k in enumerate(sparse_x):
            if idx not in self.use_features:
                continue

            # use memmap to handle large dataset during training
            if self.train:
                x_k = np.memmap(self.tmp_file + str(idx), mode="w+", dtype=k.dtype, offset=0, shape=k.shape)
                for arr,start,end in create_batches(k):
                    x_k[start:end,...] = arr.toarray()
            else:
                x_k = k.toarray()

            # scale data
            scaler = self.scaler.get(idx, None)
            if scaler is not None:
                print("Scaling data...")
                if self.train:
                    x_k = batched_fit_transform(x_k, scaler, batch_size=2000)
                else:
                    x_k = scaler.transform(x_k)

            # apply pca
            pca = self.pca.get(idx, None) if self.pca is not None else self.pca
            if pca is not None:
                print("Apply pca...")
                
                # if it has more features than required, apply PCA
                if x_k.shape[1] > pca.n_components:
                    if self.train:
                        x_k = batched_fit_transform(x_k, pca, batch_size=max(pca.n_components, 400), combine_last=True)
                        print(pca.explained_variance_ratio_.sum())

                        # n_components = (self.pca.explained_variance_ratio_.cumsum() > 0.9).nonzero()[0][0]
                        # x = x[:, :n_components]
                        # self.pca.n_components = n_components
                    else:
                        x_k = pca.transform(x_k)
            
            x.append(x_k)

        return np.concatenate(x, axis=1)

    def fit(self, x, y, n_jobs=-1):
        super().fit()

        self.train = True
        x = self.pre_process(x)

        # Define hyperparameters grid for all classifiers
        pipeline = Pipeline([('classifier', None)])
        params = [
            {
                'classifier': [RandomForestClassifier(max_depth=None)],
                'classifier__n_estimators': [600, 800, 1000, 1200],
                'classifier__max_depth': [40, 50, 60],
                'classifier__max_features': ['log2', 'sqrt'],
            },
        ] 

        # cross validation search
        print("Starting training...")
        # folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)
        self.pipe_grid = GridSearchCV(
            pipeline, 
            params, 
            scoring=make_scorer(find_best_score, greater_is_better=True, needs_proba=True), 
            cv=3, 
            refit=True,
            verbose=4,
            n_jobs=n_jobs,
        ).fit(x, y)

        # Print results
        cols = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
        print(pd.DataFrame(self.pipe_grid.cv_results_)[cols].sort_values(by='rank_test_score').head().T)
        print("Best parameters: ", self.pipe_grid.best_params_)
        print("Best Matthews correlation coefficient: ", self.pipe_grid.best_score_)

        # choose best threshold
        best_score, self.best_threshold = find_best_score(
            y_true=y, 
            y_pred=self.pipe_grid.predict_proba(x)[:,1], 
            return_threshold=True,
        )
        print(f"Best threshold {self.best_threshold} with score {best_score}")
        

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('-n', '--pca_components', type=int, nargs='+', default=None, help='number of components for PCA')
    parser.add_argument('--scaler', type=int, nargs='+', default=[0], help='input features to scale')
    parser.add_argument('-f', '--use_features', nargs='+', type=int, default=[0], help='features to use from the feature extractor')
    parser.add_argument('-s', '--save', type=str, default='defender/ml_classifier.pkl', help='file where to save model')
    parser.add_argument('-e', '--evaluate', type=str, default=None, help='model to evaluate')
    args = parser.parse_args()

    if args.pca_components is not None and len(args.pca_components) == 1:
        args.pca_components = args.pca_components[0]

    if args.evaluate is None:
        # train and save model
        sparse_x, y = load_pickle('data/train.pkl')
        feature_extractor = load_pickle('data/feature_extractor.pkl')
        model = MLClassifier(
            feature_extractor, 
            scale_input=args.scaler,
            use_features=args.use_features,
            pca_components=args.pca_components,
        )
        model.fit(sparse_x,y)
        model.save(args.save)

        args.evaluate = args.save

    # test results
    sparse_x, y = load_pickle('data/test.pkl')
    model = Model.load(args.evaluate)
    model.train = False
    print(model.evaluate(sparse_x,y))
    model.save(args.evaluate)

    '''
    Current best model (test scores: mcc, f1, fpr, tpr)
    ----------------------------------------------------
    0.9935454284892195, 0.99581356405247, 0.9998363959242529, 0.004567600487210719, 1.0
    "fp": 0.3633217993079585,
    "fn": 0.1806853582554517,1

    '''

    # cols = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
    # print(results[cols].to_latex(index=False))

    # todo augment with packers
    # todo use transformers to train model
    # todo? libauc to train
