from typing import List
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from tqdm.auto import tqdm
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold, GridSearchCV, LeaveOneOut, RandomizedSearchCV
import os
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA, IncrementalPCA
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from models.Model import Model, competition_scorer, find_best_score
from sklearn.metrics import classification_report, confusion_matrix

from models.utils import DikePEDataset, batched_fit_transform, create_batches, load_pickle
from sklearn.metrics import make_scorer


class MLClassifier(Model):
    def __new__(cls):
        obj = super().__new__(cls)
        obj.pca = None
        obj.train = False
        return obj

    def __init__(
            self, 
            feature_extractor, 
            pca_components: int = None,
        ) -> None:
        super().__init__(feature_extractor)

        self.pca_components = pca_components
        self.scaler = StandardScaler()
        if self.pca_components is not None:
            self.pca = IncrementalPCA(n_components=self.pca_components, batch_size=self.pca_components)
        else:
            self.pca = None

    def __call__(self, x) -> np.array:
        x = self.pre_process(x)
        return self.pipe_grid.best_estimator_.predict_proba(x)[:,1]
    
    def extract_features(self, pe_files: List[str]):
        # todo? make this better
        return self.feature_extractor(pe_files, enable_extractor=[0], train=self.train)
    
    def pre_process(self, sparse_x):
        # sparse_x = sparse.hstack(sparse_x)
        # todo! use more feature information

        # use only one set of features
        sparse_x = sparse_x[0]

        # use memmap to handle large dataset
        if self.train:
            x = np.memmap(self.tmp_file, mode="w+", dtype=sparse_x.dtype, offset=0, shape=sparse_x.shape)
            for arr,start,end in create_batches(sparse_x):
                x[start:end,...] = arr.toarray()
        else:
            x = sparse_x.toarray()

        # scale data
        print("Scaling data...")
        if self.train:
            x = batched_fit_transform(x, self.scaler, batch_size=2000)
        else:
            x = self.scaler.transform(x)

        # apply pca
        # todo! maybe use pca
        if self.pca is not None:
            print("Apply pca...")
            if self.train:
                x = batched_fit_transform(x, self.pca, batch_size=self.pca_components, combine_last=True)
                # print(self.pca.explained_variance_ratio_.sum())

                # n_components = (self.pca.explained_variance_ratio_.cumsum() > 0.9).nonzero()[0][0]
                # x = x[:, :n_components]
                # self.pca.n_components = n_components
            else:
                x = self.pca.transform(x)

        return x

    def fit(self, x, y, n_jobs=6):
        if not self.train:
            raise Exception("Cannot train since model is not in train mode")

        x = self.pre_process(x)

        # Define hyperparameters grid for all classifiers
        pipeline = Pipeline(
            [('classifier', None)],
            verbose=False,
        )
        params = [
            # {
            #     'classifier': [SVC(probability=True)],
            #     'classifier__C': [0.1, 1, 10],
            #     'classifier__kernel': ['linear', 'rbf', 'poly'],
            # },
            {
                'classifier': [RandomForestClassifier(max_depth=None)],
                'classifier__n_estimators': [600, 800, 1000, 1200],
                'classifier__max_depth': [None, 40, 50, 60],
                'classifier__max_features': [None, 'log2', 'sqrt'],
            },
            # {
            #     'classifier': [KNeighborsClassifier()],
            #     'classifier__n_neighbors': [3, 5, 10, 100],
            #     'classifier__weights': ['uniform', 'distance'],
            # }
        ] 

        # cross validation search
        print("Starting training...")
        # folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)
        self.pipe_grid = GridSearchCV(
            pipeline, 
            params, 
            scoring=make_scorer(find_best_score, greater_is_better=True, needs_proba=True), 
            cv=3, 
            refit=True,
            verbose=4,
            n_jobs=n_jobs,
        ).fit(x, y)

        # Print results
        cols = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
        print(pd.DataFrame(self.pipe_grid.cv_results_)[cols].sort_values(by='rank_test_score').head())
        print("Best parameters: ", self.pipe_grid.best_params_)
        print("Best Matthews correlation coefficient: ", self.pipe_grid.best_score_)

        # choose best threshold
        scores = []
        y_pred = self.pipe_grid.predict_proba(x)[:,1]
        thresholds = np.linspace(1, 10, 101)/100
        for k in thresholds:
            score = competition_scorer(
                y_true=y, 
                y_pred=y_pred,
                threshold=k,
            )
            scores.append(score)
        best_idx = np.argmax(scores)
        self.best_threshold = thresholds[best_idx]
        print(f"Best threshold {self.best_threshold} with score {scores[best_idx]}")


if __name__ == '__main__':
    # dataset
    # sparse_x, y = load_pickle('data/train.pkl')

    # # train and save model
    # feature_extractor = load_pickle('data/feature_extractor.pkl')
    # model = MLClassifier(feature_extractor)
    # model.fit(sparse_x,y)
    # model.save('ml_classifier.pkl')

    # test results
    sparse_x, y = load_pickle('data/test.pkl')
    model = Model.load('models/ml_classifier.pkl')
    model.train = False
    print(model.evaluate(sparse_x,y))


    '''
    Current best model
    ------------------

    '''

    # cols = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
    # print(results[cols].to_latex(index=False))


    # todo! incorporate into docker app
    # todo! refractor to make consistent

    

    # todo augment with packers
    # todo use transformers to train model
    # todo? libauc to train
