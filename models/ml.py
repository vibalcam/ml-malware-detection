import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from tqdm.auto import tqdm
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold, GridSearchCV, LeaveOneOut
import os
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA, IncrementalPCA
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from models.Model import Model, competition_scorer, find_best_score
from sklearn.metrics import classification_report, confusion_matrix

from models.utils import DikePEDataset, load_pickle
from sklearn.metrics import make_scorer


class MLClassifier(Model):
    def __init__(self, batch_size:int = 2000) -> None:
        super().__init__()

        self.batch_size = batch_size

    def __call__(self, x) -> np.array:
        return self.pipe_grid.best_estimator_.predict(x)

    def train(self, x, y, n_jobs=None):
        # create pipeline
        self.scaler = StandardScaler()
        self.pca = IncrementalPCA(n_components=50, batch_size=self.batch_size)
        x = self.scaler.fit_transform(x)

        # batched pca gets filename
        filename = 'tmp.npy'
        np.save(filename, x)
        shape = x.shape
        dtype = x.dtype
        del x

        # Load the data using memmap so its not all loaded into memory
        # todo! dont use offset, just slice dataset
        x = np.memmap(filename, mode="r", dtype=dtype, offset=0, shape=shape)
        x = self.pca.fit_transform(x)
        print(self.pca.explained_variance_ratio_.cumsum())

        # Define hyperparameters grid for all classifiers
        pipeline = Pipeline(
            [('classifier', None)],
            verbose=False,
        )
        params = [
            {
                'classifier': [SVC()],
                'classifier__C': [0.1, 1, 10],
                'classifier__kernel': ['linear', 'rbf', 'poly'],
            },
            {
                'classifier': [RandomForestClassifier()],
                'classifier__n_estimators': [100, 250, 400],
                'classifier__max_depth': [5, 10, 20],
            },
            {
                'classifier': [KNeighborsClassifier()],
                'classifier__n_neighbors': [3, 5, 10],
                'classifier__weights': ['uniform', 'distance'],
            }
        ] 

        # cross validation search
        folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)
        self.pipe_grid = GridSearchCV(
            pipeline, 
            params, 
            scoring=make_scorer(find_best_score, greater_is_better=True, needs_proba=True), 
            cv=folds, 
            verbose=4,
            n_jobs=n_jobs,
        ).fit(x, y)

        # Print results
        print(pd.DataFrame(self.pipe_grid.cv_results_).sort_values(by='rank_test_score').head())
        print("Best parameters: ", self.pipe_grid.best_params_)
        print("Best Matthews correlation coefficient: ", self.pipe_grid.best_score_)

        # todo! choose threshold


if __name__ == '__main__':
    # dataset
    x, y = load_pickle('data/train.pkl')
    # concatenate all features
    x = np.concatenate(x, axis=1)

    # for debugging
    x = x[:200,...]
    y = y[:200,...]

    model = MLClassifier().train(x,y)

    # cols = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
    # print(results[cols].to_latex(index=False))

    # todo! incorporate into docker app

    # todo! use also files from own system Windows folder as benign
    # todo! augment with packers
    # todo? libauc to train
