import pickle
import random
from pathlib import Path
from typing import List, Dict, Tuple, Union, Optional
import os
import numpy as np
import torch
import pathlib
from torch.utils.data import random_split
import pandas as pd
import glob

from models.feature_extractor import FeatureExtractor, Features


class DikePEDataset:

    def __init__(
        self,
        data_path = 'data/DikeDataset-main',
        feature_extractors: List[Features] = None, 
        train=True,
        debug=False,
        # save_features=True,
    ):
        self.data_path = data_path
        self.feature_extractor = FeatureExtractor(feature_extractors)
        self.train = train

        # load labels
        benign_labels = pd.read_csv(os.path.join(data_path, "labels", "benign.csv"))
        malware_labels = pd.read_csv(os.path.join(data_path, "labels", "malware.csv"))
        
        # label as malware or benign and concat
        benign_labels['malware'] = 0
        malware_labels['malware'] = 1
        labels = pd.concat([benign_labels, malware_labels], axis=0, ignore_index=True)

        # keep only PE files
        labels = labels[labels.type == 0]
        labels.drop('type', axis=1, inplace=True)
        # set hash as index
        labels.set_index('hash', inplace=True)

        # use for debugging
        if debug:
            labels = labels.iloc[:100,:]

        # extract features from files from dataset and benign files from system
        filenames = [os.path.join(self.data_path, 'files', 'malware' if k[1].malware else 'benign', f"{k[0]}.exe") for k in labels.iterrows()]
        system_files = glob.glob('/mnt/c/Windows/system32/**/*.exe', recursive=True) \
            + glob.glob('/mnt/c/Windows/system32/**/*.dll', recursive=True) \
            + glob.glob('/mnt/c/Program Files/**/*.exe', recursive=True) \
            + glob.glob('/mnt/c/Program Files/**/*.dll', recursive=True)
        filenames.extend(system_files)
        self.data = self.feature_extractor(filenames, train=True)

        # ignore all labels except whether it is malware
        self.labels = labels.malware.to_numpy()
        # add benign labels for system
        self.labels = np.concatenate((self.labels, np.zeros(len(system_files))))
        
    def split(self, lengths, save='data'):
        arrays = self.data.copy()
        arrays.append(self.labels)
        splits = split_arrays(arrays, lengths)

        if len(lengths)==2:
            names = ['train', 'test']
        else:
            names = ['train', 'valid', 'test']

        datasets = []
        for arr, name in zip(splits, names):
            x, y = arr[:-1], arr[-1]
            d = (x,y)
            if save is not None:
                # todo! instead save wrapper to reconstruct numpy array, that allows loading batches
                # save_pickle(d, os.path.join(save, f"{name}.pkl"))
                np.save(d, os.path.join(save, f"{name}.npy"))

            datasets.append(d)

            _, counts = np.unique(y, return_counts=True)
            ratio = counts / counts.sum()
            print(f"Benign: {counts[0]}/{ratio[0]}%    Malware: {counts[1]}/{ratio[1]}%")

        return datasets


def split_arrays(
    arrays,
    lengths,
    seed : int = None,
):
    # get lengths for datasets
    n_samples = arrays[0].shape[0]
    split_length = np.floor(np.cumsum(lengths) * n_samples).astype(int)
    split_length[-1] = n_samples

    # shuffle indices
    if seed is not None:
        np.random.seed(seed)
    idx = np.random.permutation(n_samples)

    # split dataset
    splits = []
    start = 0
    for l in split_length:
        arrs = [k[idx[start:l]] for k in arrays]
        start = l
        splits.append(arrs)
    splits.append([k[idx[start:]] for k in arrays])

    return splits


def split_generator_dataset(
    dataset,
    lengths,
    seed : int,
):
    # get lengths for datasets
    lengths = np.floor(np.asarray(lengths) * len(dataset))
    lengths[-1] = len(dataset) - np.sum(lengths[:-1])
    # random split 
    subsets = random_split(
        dataset, 
        lengths.astype(int).tolist(),
        torch.Generator().manual_seed(seed),
    )
    return subsets


# ----------------------------------------------------------------------------------

# add confusion matrix

# ----------------------------------------------------------------------------------


MODEL_CLASS_KEY = 'model_class'
FOLDER_PATH_KEY = 'path_name'

# to dynamically import
# import importlib
# module = importlib.import_module(module_name)
# class_ = getattr(module, class_name)
# instance = class_()


def save_checkpoint(
    path: str, 
    name: str, 
    epoch: int, 
    optimizer: torch.optim.Optimizer, 
    **kwargs
):
    '''Save a checkpoint of the training parameters
    
    Parameters
    ----------
    path : str
        The path to save the checkpoint to.
    name : str
        The name of the model.
    epoch : int
        The current epoch number
    optimizer : torch.optim.Optimizer
        torch.optim.Optimizer
    loss : float
    '''
    folder_path = f"{path}/{name}"
    pathlib.Path(folder_path).mkdir(parents=True, exist_ok=True)

    d = {
        'epoch': epoch,
        'optimizer': optimizer.state_dict(),
    }
    d.update(kwargs)
    torch.save(d, f"{folder_path}/{name}_checkpoint.pt")


def load_checkpoint(path: str, optimizer: torch.optim.Optimizer):
    '''It loads a checkpoint, and then loads the optimizer state
    
    Parameters
    ----------
    path : str
        the path to the folder where the checkpoint is stored
    name : str
        the name of the model
    optimizer : torch.optim.Optimizer
        torch.optim.Optimizer
    
    Returns
    -------
        The dictionary d is being returned.
    
    '''
    folder_path = pathlib.Path(path)
    path = f"{folder_path.absolute()}/{folder_path.name}"
    d = torch.load(f"{path}_checkpoint.pt", map_location='cpu')
    
    # load optimizer state
    optimizer.load_state_dict(d['optimizer'])
    d['optimizer'] = optimizer
    return d


def save_model(
        model: torch.nn.Module,
        folder: Union[pathlib.Path, str],
        model_name: str,
        models_dict:Dict,
        param_dicts: Dict = None,
        save_model: bool = True,
) -> None:
    """
    Saves the model so it can be loaded after

    :param model_name: name of the model to be saved (non including extension)
    :param folder: path of the folder where to save the model
    :param param_dicts: dictionary of the model parameters that can later be used to load it
    :param model: model to be saved
    :param save_model: If true the model and dictionary will be saved, otherwise only the dictionary will be saved
    """
    # create folder if it does not exist
    folder_path = f"{folder}/{model_name}"
    pathlib.Path(folder_path).mkdir(parents=True, exist_ok=True)

    # save model
    if save_model:
        torch.save(model.state_dict(), f"{folder_path}/{model_name}.th")

    # save dict
    if param_dicts is None:
        param_dicts = {}

    # get class of the model
    model_class = None
    for k, v in models_dict.items():
        if isinstance(model, v):
            model_class = k
            break
    if model_class is None:
        raise Exception("Model class unknown")
    param_dicts[MODEL_CLASS_KEY] = model_class

    # save the dictionary as plain text and pickle
    save_dict(param_dicts, f"{folder_path}/{model_name}.dict", as_str=True)
    save_dict(param_dicts, f"{folder_path}/{model_name}.dict.pickle", as_str=False)


def load_model(
    folder_path: str, 
    models_dict:Dict,
    model_class: Optional[str] = None,
) -> Tuple[torch.nn.Module, Dict]:
    """
    Loads a model that has been previously saved using its name (model th and dict must have that same name)

    :param folder_path: folder path of the model to be loaded
    :param model_class: one of the model classes in `models_dict` dict. If none, it is obtained from the dictionary
    :return: the loaded model and the dictionary of parameters
    """
    folder_path = pathlib.Path(folder_path)
    path = f"{folder_path.absolute()}/{folder_path.name}"
    # use pickle dictionary
    dict_model = load_dict(f"{path}.dict.pickle")

    # get model class
    if model_class is None:
        model_class = dict_model.get(MODEL_CLASS_KEY)

    # set folder path
    dict_model[FOLDER_PATH_KEY] = str(folder_path)

    return load_model_data(models_dict[model_class](**dict_model), f"{path}.th"), dict_model


def load_model_data(model: torch.nn.Module, model_path: str) -> torch.nn.Module:
    """
    Loads a model than has been previously saved

    :param model_path: path from where to load model
    :param model: model into which to load the saved model
    :return: the loaded model
    """
    model.load_state_dict(torch.load(model_path, map_location='cpu'))
    return model


def save_pickle(obj, path: Union[str, Path]):
    """
    Saves an object with pickle

    :param obj: object to be saved
    :param save_path: path to the file where it will be saved
    """
    with open(path, 'wb') as f:
        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)


def load_pickle(path: Union[str, Path]):
    """
    Loads an object with pickle from a file

    :param path: path to the file where the object is stored
    """
    with open(path, 'rb') as f:
        return pickle.load(f)


def dict_to_str(d: Dict):
    str = '{\n'
    for k,v in d.items():
        if isinstance(v, dict):
            v = dict_to_str(v)
        str += f"'{k}':{v},\n"
    return str + '}'


def save_dict(d: Dict, path: str, as_str: bool = False) -> None:
    """
    Saves a dictionary to a file in plain text
    :param d: dictionary to save
    :param path: path of the file where the dictionary will be saved
    :param as_str: If true, it will save as a string. If false, it will use pickle
    """
    if as_str:
        with open(path, 'w', encoding="utf-8") as file:
            # file.write(str(d))
            file.write(dict_to_str(d))
    else:
        save_pickle(d, path)


def load_dict(path: str) -> Dict:
    """
    Loads a dictionary from a file (plain text or pickle)

    :param path: path where the dictionary was saved
    :return: the loaded dictionary
    """
    try:
        return load_pickle(path)
    except pickle.UnpicklingError as e:
        # print(e)
        pass

    with open(path, 'r', encoding="utf-8") as file:
        from ast import literal_eval
        s = file.read()
        return dict(literal_eval(s))


def log(s):
    """
    Helper method to log

    Parameters
    ----------
    s : str
        string to log
    """
    print(s)


# ----------------------------------------------------------------------------------


def set_seed(seed: int) -> None:
    """
    This function sets a seed and ensure a deterministic behavior

    :param seed: seed for the random generators
    """
    # set seed in numpy and random
    np.random.seed(seed)
    random.seed(seed)

    # set seed and deterministic algorithms for torch
    torch.manual_seed(seed)
    torch.use_deterministic_algorithms(True)

    # Ensure all operations are deterministic on GPU
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # make deterministic
        torch.backends.cudnn.determinstic = True
        torch.backends.cudnn.benchmark = False

        # for deterministic behavior on cuda >= 10.2
        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"


class dotdict(dict):
    """dot.notation access to dictionary attributes"""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

    def __getstate__(self): 
        return self.__dict__
    def __setstate__(self, d): 
        self.__dict__.update(d)

    def copy(self):
        return dotdict(super().copy())


def datetime_to_timestamp(dt):
    return np.array(dt, dtype='datetime64').view('int64') * 1e-9


# ----------------------------------------------------------------------------------


if __name__ == '__main__':
    dataset = DikePEDataset()
    dataset.split([0.85, 0.15])
