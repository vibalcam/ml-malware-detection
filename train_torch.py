import argparse
import glob
import math
import os
import warnings
from tqdm.auto import trange

import numpy as np
import lightning.pytorch as pl
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim
from torch.utils.data import Dataset
import torch.utils.data
import torch.utils.data.distributed
from libauc.losses.auc import pAUC_CVaR_Loss, tpAUC_KL_Loss
from libauc.optimizers import SOPA, SOTAs
import torchmetrics
from torch.utils.data import DataLoader
from defender.model import ThresholdSelector, competition_scorer, find_best_score
import defender.models.ember_init as ember

from defender.utils import load_pickle, split_arrays
from libauc.losses import AUCMLoss
from libauc.optimizers import PESG
from defender.models.malware_torch import AttentionModel, FeedForward, TorchModel, AttentionModelv2


parser = argparse.ArgumentParser(description='')
parser.add_argument('-a', '--arch', type=str, default='attention')
parser.add_argument('-j', '--workers', default=0, type=int, metavar='N',
                    help='number of data loading workers (default: 4)')
parser.add_argument('--epochs', default=100, type=int, metavar='N',
                    help='number of total epochs to run')
parser.add_argument('-b', '--batch-size', default=64, type=int,
                    metavar='N',
                    help='mini-batch size (default: 256)')
parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,
                    metavar='LR', help='initial (base) learning rate', dest='lr')
parser.add_argument('--momentum', default=0.9, type=float, metavar='M',
                    help='momentum')
parser.add_argument('--wd', '--weight-decay', default=0., type=float,
                    metavar='W', help='weight decay (default: 1e-6)',
                    dest='weight_decay')
parser.add_argument('--resume', default=None, type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('--seed', default=None, type=int,
                    help='seed for initializing training. ')
parser.add_argument('--debug', action='store_true', help='To debug code')

# additional configs:
# parser.add_argument('--pretrained', default='saved_models/chexpert/resnet50/dcl/pretrain/version_0/resnet50-best-epoch=99.ckpt', type=str,
#                     help='path to sogclr pretrained checkpoint')

parser.add_argument('--loss_type', default='bce', type=str,
                    help='loss type of pretrained (default: bce)')
parser.add_argument('--optimizer', default='adamw', type=str,
                    choices=['sgd', 'adamw'],
                    help='optimizer used (default: sgd)')
parser.add_argument('--warmup-epochs', default=10, type=int, metavar='N',
                    help='number of warmup epochs')

# dataset 
parser.add_argument('--save_dir', default='./saved_models/', type=str) 

# saving
parser.add_argument('--save_every_epochs', default=20, type=int,
                    help='number of epochs to save checkpoint')
parser.add_argument('-e', '--evaluate_every', default=1, type=int,
                    help='evaluate model on validation set every # epochs')
parser.add_argument('--early_stopping_patience', default=20, type=int,
                    help='patience for early stopping')
parser.add_argument('-s', '--save', type=str, default='defender/ml_classifier.pkl', help='file where to save model')

# model
# parser.add_argument("--hidden_dim", type=int, default=768)
parser.add_argument("--hidden_dim", type=int, nargs='+', default=[512, 512], help="Hidden dimension. Only the first number will be used for attention")
parser.add_argument("--n_layers", type=int, default=12, help="Number of attention layers")
parser.add_argument("--num_heads", type=int, default=4)
parser.add_argument("--dropout_rate", type=float, default=0.5)
parser.add_argument("--att_dropout", type=float, default=0.0)
parser.add_argument("--n_tries", type=int, default=100, help="Number of thresholds to test")

parser.add_argument("--skip_training", type=str, default=None)


class PEDataset(Dataset):
    def __init__(self, p, extractor, use_features):
        x = []
        y = []
        for t in [('gw', 0), ('mw', 1)]:
            files = list(glob.glob(os.path.join(p, f"{t[0]}*", "**", '*'), recursive=True))
            files = [f for f in files if os.path.isfile(f)]
            y.append(np.ones(len(files)) * t[1])
            x.extend(files)

        self.y=np.concatenate(y, axis=0)
        self.x, non_parsed = extractor(x, enable_extractor=use_features, train=False)
        self.x = [k.toarray() for k in self.x]
        # remove unparsed labels
        correct = ~np.isin(np.arange(self.y.shape[0]), non_parsed)
        self.y = self.y[correct]

        # check no nan
        assert np.sum([np.isnan(k).sum() for k in self.x]) == 0
        # check that shapes match
        assert len(self.y) == len(self.x[0]), "Shapes do not match"
        # check that no unlabeled data
        assert np.logical_and(self.y != 0, self.y !=1).sum() == 0, "Unknown labels, it should be 0 or 1"

    def __len__(self):
        return len(self.x[0])

    def feature_dims(self):
        return [x.shape[1] for x in self.x]

    def __getitem__(self, index):
        if self.y[index] == 1:
            index_p = (self.y[:index+1] == 1).sum()-1
        else:
            index_p = -(self.y[:index+1] == 0).sum()
        return [x[index].astype(np.float32) for x in self.x], self.y[index], index_p


def save_files(X, path):
    filenames = []
    for i in trange(X.shape[0]):
        f = f"{path}_{i}"
        v = torch.from_numpy(X[i,...])
        if v.isnan().sum() > 0:
            raise Exception("Input nan, no implemented solutions")
        torch.save(v, f)
        filenames.append(f)
    return filenames


class SparseDataset(Dataset):
    base_tmp_file = f"tmp_calc/torch_tmp_x"
    version = -1

    @staticmethod
    def get_datasets(paths, seed, lengths=[0.8, 0.2], use_features=None, use_ember=False, use_bodmas=False):
        self_x = []
        self_y = []
        indices = {}
        for idx_path, path in enumerate(paths):
            SparseDataset.version += 1
            self_tmp_file = f"{SparseDataset.base_tmp_file}_{idx_path}"
            # load data
            list_sparse_x, y = load_pickle(path)
            # save to files to not fill memory
            for idx,sparse_x in enumerate(list_sparse_x):
                if use_features is not None and idx not in use_features:
                    continue

                if idx_path == 0:
                    indices[idx] = len(self_x)
                    self_x.append([])

                filenames = []
                for i in trange(sparse_x.shape[0]):
                    f = f"{self_tmp_file}_{idx}_{i}"
                    v = torch.from_numpy(sparse_x[i,...].toarray()[0])
                    if v.isnan().sum() > 0:
                        raise Exception("Input nan, no implemented solutions")
                    torch.save(v, f)
                    filenames.append(f)

                self_x[indices[idx]].extend(filenames)
            self_y.append(y)

        # load additional data
        if use_bodmas:
            assert use_features == [0]
            if len(self_x) == 0:
                indices[0] = len(self_x)
                self_x.append([])

            filenames = []

            # bodmas dataset
            filename = 'data/bodmas/bodmas.npz'
            data = np.load(filename)
            X = data['X']  # all the feature vectors
            y = data['y']  # labels, 0 as benign, 1 as malicious
            self_y.append(y)
            del y
            del data

            # save to files
            self_x[indices[0]].extend(save_files(X, f"{SparseDataset.base_tmp_file}_bodmas"))
            del X
        
        if use_ember:
            assert use_features == [0]
            if len(self_x) == 0:
                indices[0] = len(self_x)
                self_x.append([])

            X_train, y_train, X_test, y_test = ember.read_vectorized_features("data/ember2018/")
            self_y.append(y_train)
            self_y.append(y_test)
            del y_train
            del y_test
            
            # save to files
            self_x[indices[0]].extend(save_files(X_train, f"{SparseDataset.base_tmp_file}_emberTrain"))
            del X_train
            self_x[indices[0]].extend(save_files(X_test, f"{SparseDataset.base_tmp_file}_emberTest"))
            del X_test
        self_y = np.concatenate(self_y, axis=0)

        # Filter unlabeled data
        train_rows = (self_y != -1)
        self_y = self_y[train_rows]
        self_x = [np.asarray(x)[train_rows] for x in self_x]

        # # check no nan
        # assert torch.sum([np.isnan(k).sum() for k in self_x]) == 0
        # check that shapes match
        assert len(self_y) == len(self_x[0]), "Shapes do not match"
        # check that no unlabeled data
        assert np.logical_and(self_y != 0, self_y !=1).sum() == 0, "Unknown labels, it should be 0 or 1"

        # divide into train and validation
        xy1, xy2 = split_arrays(self_x + [self_y], lengths, seed=seed)

        return SparseDataset(xy1[:-1], xy1[-1]), SparseDataset(xy2[:-1], xy2[-1])

    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __len__(self):
        return len(self.x[0])

    def feature_dims(self):
        return [torch.load(x[0]).shape[0] for x in self.x]
        # return [x.shape[1] for x in self.x]

    def __getitem__(self, index):
        # return [x[index].astype(np.float32) for x in self.x], self.y[index]
        if self.y[index] == 1:
            index_p = (self.y[:index+1] == 1).sum()-1
        else:
            index_p = -(self.y[:index+1] == 0).sum()
        return [torch.load(x[index]).float() for x in self.x], self.y[index], index_p


class DetectorModel(pl.LightningModule):
    def __init__(
        self,
        features_dim,
        args,
        num_epochs,
        pos_samples=None,
        neg_samples=None,
        textual_dim=None,
        **kwargs,
    ):
        super().__init__()
        # random input to build computational graph
        # self.example_input_array = torch.zeros((1, ))

        # save hyperparameters as attribute
        self.save_hyperparameters(ignore=['model'])
        self.pos_samples = pos_samples
        self.neg_samples = neg_samples
        self.threshold = 0.5
        self.num_epochs = num_epochs
        
        self.args = args
        if args.arch == 'attention':
            self.model = AttentionModelv2(
                features_dim=features_dim,
                textual_dim=textual_dim,
                hidden_dim=args.hidden_dim[0],
                intermediate_dim=None,
                n_layers=args.n_layers,
                num_heads=args.num_heads,
                dropout_rate=args.dropout_rate,
                att_dropout=args.att_dropout,
            )
        elif args.arch == 'ff':
            self.model = FeedForward(
                features_dim=features_dim,
                hidden_dim=args.hidden_dim,
                dropout_rate=args.dropout_rate,
            )
        else:
            raise NotImplementedError()
        print(f"Using model: {self.model}")

        self.lr = self.args.lr
        self.batch_size = self.args.batch_size
        ## infer learning rate
        self.init_lr = self.lr
        self.init_lr = self.lr * self.batch_size / 256
        self.lr = self.init_lr
        print('initial learning rate:', self.args.lr)

        ##################
        # METRICS
        ##################
        if args.loss_type == 'bce':
            self.criterion = nn.BCEWithLogitsLoss()
        elif args.loss_type == 'mse':
            self.criterion = nn.MSELoss()
        elif args.loss_type == 'auc':
            self.criterion = AUCMLoss()
        elif args.loss_type == 'pauc':
            # self.criterion = pAUC_CVaR_Loss(self.pos_samples, self.neg_samples, beta=0.001)
            self.criterion = tpAUC_KL_Loss(self.pos_samples, Lambda=0.5, tau=1.0)
        else:
            raise NotImplementedError()

        self.train_auc = torchmetrics.AUROC(task='binary', max_fpr=0.01)#, thresholds=self.args.n_tries)
        self.val_auc = torchmetrics.AUROC(task='binary', max_fpr=0.01)#, thresholds=self.args.n_tries)

    def configure_optimizers(self):
        if self.args.loss_type == 'auc':
            optimizer = PESG(
                self.model, 
                loss_fn=self.criterion, 
                lr=self.args.lr, 
                momentum=self.args.momentum,
                weight_decay=self.args.weight_decay,
            )
        elif self.args.loss_type == 'pauc':
            # optimizer = SOPA(
            #     self.model, 
            #     loss_fn=self.criterion, 
            #     lr=self.args.lr, 
            #     mode='adam',
            #     eta=1e1,
            #     weight_decay=self.args.weight_decay,
            # )
            optimizer = SOTAs(
                self.parameters(), 
                loss_fn=self.criterion, 
                lr=self.args.lr, 
                mode='adam',
                gammas=(0.5, 0.5),
                weight_decay=self.args.weight_decay,
            )
        else:
            if self.args.optimizer == 'sgd':
                optimizer = torch.optim.SGD(self.parameters(), self.args.lr,
                                                weight_decay=self.args.weight_decay,
                                                momentum=self.args.momentum)
            elif self.args.optimizer == 'adamw':
                optimizer = torch.optim.AdamW(self.parameters(), self.args.lr,
                                        weight_decay=self.args.weight_decay)
            else:
                raise NotImplementedError("Optimizer not implemented")

        # todo! use reduce on plateau scheduler
        # scheduler = get_linear_schedule_with_warmup(optimizer,num_training_steps=n_epochs, num_warmup_steps=100)
        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience)
        return optimizer
        
    def forward(self, x):
        return self.model(x)

    # def on_train_epoch_start(self):
    #     # adjust learning rate and momentum coefficient per iteration
    #     adjust_learning_rate(self.optimizers(), self.init_lr, self.current_epoch, self.args)

    def training_step(self, batch, batch_idx):
        adjust_learning_rate(self.optimizers(), self.init_lr, self.current_epoch + batch_idx/self.num_epochs, self.args)

        # run through model
        x, target, index = batch

        output = self(x)
        if output.isnan().sum() > 0:
            warnings.warn("Nan values being generated")
            # output = self(x)
        if self.args.loss_type=='pauc':
            loss = self.criterion(torch.sigmoid(output).float(), target, index.long())
        elif self.args.loss_type=='mse':
            loss = self.criterion(torch.sigmoid(output).float(), target.float())
        else:
            loss = self.criterion(output, target)
        if loss.isnan():
            warnings.warn("Getting nan loss")

        # calculate metrics
        y_pred = torch.sigmoid(output)
        self.train_auc(y_pred.float(), target.int())
        # score = competition_scorer(
        #     y_true=target.int().detach().cpu().numpy(), 
        #     y_pred=y_pred.detach().cpu().numpy(),
        #     threshold=self.threshold,
        #     max_fpr=0.001, 
        #     min_tpr=0.95,
        # )[0]
        if batch_idx == 0:
            self.train_score = find_best_score(
                target.int().detach().cpu().numpy(), 
                y_pred.float().detach().cpu().numpy(), 
                return_threshold=False,
            )

        # log loss for training step and average loss for epoch
        self.log_dict({
            "train_loss": loss,
            "train_auc": self.train_auc,
            "train_score": self.train_score,
        }, on_step=True, on_epoch=True, prog_bar=True, logger=True)

        # # compute gradient and do SGD step
        # optimizer.zero_grad()
        # scaler.scale(loss).backward()
        # scaler.step(optimizer)
        # scaler.update()
        
        return loss
    
    # def on_validation_start(self) -> None:
    #     # check that pretrained weights have not changed
    #     self.model.sanity_check(self.args.pretrained, verbose=False)
    #     return super().on_validation_start()

    def validation_step(self, batch, batch_idx):   
        # run through model
        x, target, index = batch
        output = self(x)

        # if self.args.loss_type=='pauc':
        #     loss = self.criterion(output, target, index)
        # elif self.args.loss_type=='mse':
        #     loss = self.criterion(torch.sigmoid(output).float(), target.float())
        # else:
        #     loss = self.criterion(output, target)

        # calculate metrics
        y_pred = torch.sigmoid(output)
        self.val_auc(y_pred.float(), target.int())
        score, self.threshold, full_scores = find_best_score(
            target.int().cpu().numpy(), 
            y_pred.float().detach().cpu().numpy(), 
            return_threshold=True,
            return_full_scores=True,
        )
        # log loss for training step and average loss for epoch
        self.log_dict({
            # "val_loss": loss,
            "val_auc": self.val_auc,
            "val_score": score,
            "val_f1": full_scores[1],
            "val_fpr": full_scores[2],
            "val_tpr": full_scores[3],
        }, on_step=False, on_epoch=True, prog_bar=True, logger=True)

    # def validation_epoch_end(self, outputs):
    #     fpr, tpr, thresholds = self.val_roc.compute()
    #     plot_roc(fpr, tpr, self.val_auc.compute())

    # def on_test_start(self) -> None:
    #     self.preds = []
    #     self.targets = []

    # def test_step(self, batch, batch_idx):
    #     # run through model
    #     images, target = batch
    #     output = self(images)
    #     y_pred = torch.sigmoid(output)

    #     self.preds.append(y_pred.detach().cpu())
    #     self.targets.append(target.detach().cpu())

    # def on_test_epoch_end(self) -> None:
    #     self.preds = torch.cat(self.preds, dim=0)
    #     self.targets = torch.cat(self.targets, dim=0)
    #     torch.save(self.preds, os.path.join(self.save_path, "preds.pt"))
    #     torch.save(self.targets, os.path.join(self.save_path, "targets.pt"))


def adjust_learning_rate(optimizer, init_lr, epoch, args):
    """Decays the learning rate with half-cycle cosine after warmup"""
    if epoch < args.warmup_epochs:
        lr = init_lr * epoch / args.warmup_epochs 
    else:
        lr = init_lr * 0.5 * (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return lr


def main():
    ###########################
    # PARAMETERS
    ###########################
    args = parser.parse_args()
    # os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

    if args.debug:
        args.workers = 0
    
    # seed for reproducibility
    if args.seed is not None:
        warnings.warn(f"You have seeded training with seed {args.seed}")
        pl.seed_everything(args.seed, workers=True)
    else:
        warnings.warn(f"You have not seeded training")

    if not torch.cuda.is_available():
        warnings.warn("No GPU available: training will be extremely slow")

    ###########################
    # DATASET
    ###########################

    # dataset1 = SparseDataset('data/train.pkl', use_features=use_features, use_ember=False)
    # dataset2 = SparseDataset('data/test.pkl', use_features=use_features, use_ember=False)
    # full_dataset = data.ConcatDataset([dataset1, dataset2])
    # train_dataset, val_dataset = split_generator_dataset(full_dataset, [0.8, 0.2], args.seed)

    # files = list(glob.glob('data_features_combined/**/*_train.pkl', recursive=True))
    # files.extend(list(glob.glob('data_features_combined/**/*_test.pkl', recursive=True)))
    # files = [f for f in files if os.path.isfile(f)]
    files = []
    for k in ['small']:
        for t in ['train', 'test']:
            l = list(glob.glob(f"data_features_combined/**/{k}_{t}.pkl", recursive=True))
            l = [f for f in l if os.path.isfile(f)]
            if len(l) == 0:
                raise Exception("Some files are emtpy")
            files.extend(l)

    use_features = [1]
    use_textual = [2,3]
    data_paths = files
    use_ember = False
    use_bodmas = False

    train_dataset, val_dataset = SparseDataset.get_datasets(
        data_paths, 
        seed=args.seed, 
        use_features=use_features, 
        use_ember=use_ember, 
        use_bodmas=use_bodmas
    )

    feature_extractor = load_pickle('data_features_combined/small_feature_extractor.pkl')
    test_dataset = PEDataset('data/testData', feature_extractor, use_features)

    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=args.workers, 
        pin_memory=not args.debug,
        drop_last=not args.debug,
    )

    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.workers, 
        pin_memory=False,
        drop_last=False,
    )

    test_dataloader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.workers, 
        pin_memory=False,
        drop_last=False,
    )

    ###########################
    # MODEL
    ###########################

    base_dir = os.path.join(args.arch, args.loss_type, str(use_features))
    if args.resume is None:
        logger = pl.loggers.TensorBoardLogger(
            save_dir=args.save_dir,
            name=base_dir, 
            # todo! use log graph
            # log_graph=True,
        )
    else:
        logdir = args.resume.split('/')[1:-1]
        logger = pl.loggers.TensorBoardLogger(
            save_dir=args.save_dir,
            name=os.path.join(*logdir[:-1]),
            version=logdir[-1],
            # log_graph=True,
        )

    # adjust path for pretrain according to model
    # args.pretrained = os.path.join(args.save_dir, base_dir, "pretrain", args.pretrained)

    # load pretrained model
    # pretrained_model = SogModel.load_from_checkpoint(args.pretrained).model

    num_pos = (train_dataset.y==1).sum()
    num_neg = (train_dataset.y==0).sum()
    print("-"*10)
    print(f"Total: {num_pos + num_neg}")
    print(f"Positive {num_pos/(num_pos + num_neg)}")
    print(f"Negative {num_neg/(num_pos + num_neg)}")
    print("-"*10)
    # task to do
    dims = np.asarray(train_dataset.feature_dims())
    model_task = DetectorModel(
        features_dim=dims[use_features],
        textual_dim=dims[use_textual],
        args=args,
        pos_samples=num_pos,
        neg_samples=num_neg,
        num_epochs=len(train_dataloader),
    )

    ###########################
    # CALLBACKS
    ###########################

    callbacks = [
        pl.callbacks.LearningRateMonitor(),
        # pl.callbacks.DeviceStatsMonitor(),  # monitors and logs device stats, useful to find memory usage
    ]

    save_path = logger.log_dir

    ## callback for saving checkpoints
    checkpoint_cb_every = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="last",
        monitor="step",
        mode="max",
        save_top_k=1,
        every_n_epochs=args.save_every_epochs,
        # save_on_train_epoch_end=True,                         # when using a training metric
        # train_time_interval=,
        # every_n_train_steps=,
        # save_last=False,                                        # save last might be useful to have
    )
    callbacks.append(checkpoint_cb_every)

    checkpoint_cb_bestk = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="best_auc",
        save_top_k=1, 
        monitor='val_auc',
        mode='max',
        # save_on_train_epoch_end=False,     # when using a training metric
        # save_last=False,
    )
    callbacks.append(checkpoint_cb_bestk)

    checkpoint_cb_bestk = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="best",
        save_top_k=2, 
        monitor='val_score',
        mode='max',
        # save_on_train_epoch_end=False,     # when using a training metric
        # save_last=False,
    )
    callbacks.append(checkpoint_cb_bestk)

    ## early stopping
    # early_stopping = pl.callbacks.EarlyStopping(
    #     monitor='val_auc',
    #     mode='max',
    #     patience=args.early_stopping_patience,
    #     verbose=True,
    # )
    # callbacks.append(early_stopping)

    ###########################
    # TRAINER
    ###########################

    # may increase performance but lead to unstable training
    torch.set_float32_matmul_precision("high")
    trainer = pl.Trainer(
        accelerator='gpu' if not args.debug else 'cpu',
        deterministic="warn" if args.seed is not None else False,
        precision="16-mixed",   # reduce memory, can improve performance but might lead to unstable training
        
        max_epochs=args.epochs,
        # max_time="00:1:00:00",
        # max_steps=,

        check_val_every_n_epoch=args.evaluate_every,
        # val_check_interval=1.0,
        logger=logger,
        log_every_n_steps=10,
        callbacks=callbacks,

        fast_dev_run=args.debug,   # for testing training and validation
        # num_sanity_val_steps=0 if not args.debug else 2,
        limit_train_batches=1.0 if not args.debug else 0.01,  # to test what happens after an epoch
        # overfit_batches=0.01,

        # profiler='simple',    # advanced profiling to check for bottlenecks
    )

    ###########################
    # RUN MODEL
    ###########################

    # ## call tune to find lr and batch size
    # from lightning.pytorch.tuner import Tuner
    # tuner = pl.tuner.Tuner(trainer)
    # lr_finder = tuner.lr_find(model_task, train_dataloaders=train_dataloader)
    # print(lr_finder.results)
    # fig = lr_finder.plot(suggest=True)
    # fig.show()
    # # new_lr = lr_finder.suggestion()
    # # batch_size = tuner.scale_batch_size(model_task, train_dataloaders=train_dataloader)
    # return

    # fit the model
    if args.skip_training is None:
        print("Fitting model...")
        trainer.fit(
            model=model_task,
            train_dataloaders=train_dataloader,
            val_dataloaders=val_dataloader,
            ckpt_path=args.resume,
        )

    # get best threshold
    chk_path = checkpoint_cb_bestk.best_model_path if args.skip_training is None else args.skip_training
    model = DetectorModel.load_from_checkpoint(chk_path).model
    score, threshold = ThresholdSelector()(model, val_dataloader, n_tries=args.n_tries)
    print(f"Best score: {score}")
    print(f"Best threshold: {threshold}")
    wrapper = TorchModel(feature_extractor, model, use_features=use_features)
    wrapper.best_threshold = threshold
    wrapper.train = False
    wrapper.save(args.save)

    ## test model
    # trainer.test(
    #     model=model,
    #     dataloaders=test_dataloader, 
    #     ckpt_path=os.path.join(save_path, "best.ckpt"),
    #     verbose=True,
    # )


if __name__ == '__main__':
    main()
