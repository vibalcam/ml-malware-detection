import argparse
from collections import defaultdict
import glob
import math
import os
import warnings
from matplotlib import pyplot as plt
from tqdm.auto import trange
from libauc.sampler import DualSampler # data resampling (for binary class)

import numpy as np
import lightning.pytorch as pl
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim
from torch.utils.data import Dataset
import torch.utils.data
import torch.utils.data.distributed
from libauc.losses.auc import pAUC_CVaR_Loss, tpAUC_KL_Loss
from libauc.optimizers import SOPA, SOTAs
import torchmetrics
from torch.utils.data import DataLoader
from defender.model import ThresholdSelector, competition_scorer, find_best_score
import defender.models.ember_init as ember

from defender.utils import load_pickle, save_pickle, split_arrays
from libauc.losses import AUCMLoss
from libauc.optimizers import PESG
from defender.models.malware_torch import AttentionModel, FeedForward, TorchModel, AttentionModelv2, get_nonzero_idx


parser = argparse.ArgumentParser(description='')
parser.add_argument('-a', '--arch', type=str, default='attention')
parser.add_argument('-j', '--workers', default=0, type=int, metavar='N',
                    help='number of data loading workers (default: 4)')
parser.add_argument('--epochs', default=100, type=int, metavar='N',
                    help='number of total epochs to run')
parser.add_argument('-b', '--batch-size', default=64, type=int,
                    metavar='N',
                    help='mini-batch size (default: 256)')
parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,
                    metavar='LR', help='initial (base) learning rate', dest='lr')
parser.add_argument('--momentum', default=0.9, type=float, metavar='M',
                    help='momentum')
parser.add_argument('--wd', '--weight-decay', default=0., type=float,
                    metavar='W', help='weight decay (default: 1e-6)',
                    dest='weight_decay')
parser.add_argument('--resume', default=None, type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('--seed', default=None, type=int,
                    help='seed for initializing training. ')
parser.add_argument('--debug', action='store_true', help='To debug code')

# additional configs:
# parser.add_argument('--pretrained', default='saved_models/chexpert/resnet50/dcl/pretrain/version_0/resnet50-best-epoch=99.ckpt', type=str,
#                     help='path to sogclr pretrained checkpoint')

parser.add_argument('--loss_type', default='bce', type=str,
                    help='loss type of pretrained (default: bce)')
parser.add_argument('--gammas', default=[0.5,0.5], type=float, nargs='+')
parser.add_argument('--lbda', default=0.5, type=float)
parser.add_argument('--tau', default=1.0, type=float)
parser.add_argument('--optimizer', default='adamw', type=str,
                    choices=['sgd', 'adamw'],
                    help='optimizer used (default: sgd)')
parser.add_argument('--warmup-epochs', default=0.1, type=float, help='number of warmup epochs')

# dataset 
parser.add_argument('--save_dir', default='./saved_models/', type=str) 

# saving
parser.add_argument('--save_every_epochs', default=20, type=int,
                    help='number of epochs to save checkpoint')
parser.add_argument('-e', '--evaluate_every', default=1, type=int,
                    help='evaluate model on validation set every # epochs')
parser.add_argument('--early_stopping_patience', default=20, type=int,
                    help='patience for early stopping')
parser.add_argument('-s', '--save', type=str, default='defender/ml_classifier.pkl', help='file where to save model')

# model
# parser.add_argument("--hidden_dim", type=int, default=768)
parser.add_argument("--hidden_dim", type=int, nargs='+', default=[512, 512], help="Hidden dimension. Only the first number will be used for attention")
parser.add_argument("--n_layers", type=int, default=8, help="Number of attention layers")
parser.add_argument("--num_heads", type=int, default=8)
parser.add_argument("--dropout_rate", type=float, default=0.5)
parser.add_argument("--att_dropout", type=float, default=0.0)
# parser.add_argument("--n_tries", type=int, default=100, help="Number of thresholds to test")

parser.add_argument("--skip_training", type=str, default=None)


class PEDataset(Dataset):
    tmp_file = f"tmp_calc/torch_test"

    def __init__(self, p, extractor, use_features, cache=True, textual_idx=[]):
        if cache and os.path.exists(self.tmp_file):
            self.filenames, self.y = load_pickle(self.tmp_file)
            return

        x = []
        y = []
        for t in [('gw', 0), ('mw', 1)]:
            files = list(glob.glob(os.path.join(p, f"{t[0]}*", "**", '*'), recursive=True))
            files = [f for f in files if os.path.isfile(f)]
            y.append(np.ones(len(files)) * t[1])
            x.extend(files)

        self.y=np.concatenate(y, axis=0)
        x, non_parsed = extractor(x, enable_extractor=use_features, train=False)
        x = [k.toarray() for k in x]
        # remove unparsed labels
        correct = ~np.isin(np.arange(self.y.shape[0]), non_parsed)
        self.y = self.y[correct]

        # check no nan
        assert np.sum([np.isnan(k).sum() for k in x]) == 0
        # check that shapes match
        assert len(self.y) == len(x[0]), "Shapes do not match"
        # check that no unlabeled data
        assert np.logical_and(self.y != 0, self.y !=1).sum() == 0, "Unknown labels, it should be 0 or 1"

        for idx in textual_idx:
            x[idx] = get_nonzero_idx(x[idx])

        self.filenames = []
        for i in trange(len(self.y)):
            f = f"{self.tmp_file}_{i}"
            v = [k[i] for k in x]
            torch.save(v, f)
            self.filenames.append(f)
        
        save_pickle((self.filenames, self.y), self.tmp_file)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, index):
        return torch.load(self.filenames[index]).float(), self.y[index]


def save_files(X, path):
    filenames = []
    for i in trange(X.shape[0]):
        f = f"{path}_{i}"
        v = torch.from_numpy(X[i,...])
        if v.isnan().sum() > 0:
            raise Exception("Input nan, no implemented solutions")
        torch.save(v, f)
        filenames.append(f)
    return filenames


class SparseDataset(Dataset):
    base_tmp_file = f"tmp_calc/torch_tmp_x"
    version = -1

    @staticmethod
    def get_datasets(paths, seed, lengths=[0.8, 0.2], use_features=None, use_ember=False, use_bodmas=False, textual_idx=[]):
        self_x = []
        self_y = []
        indices = {}
        max_nonzero = defaultdict(lambda: 0)
        vocab_dim = {}
        for idx_path, path in enumerate(paths):
            SparseDataset.version += 1
            self_tmp_file = f"{SparseDataset.base_tmp_file}_{idx_path}"
            # load data
            list_sparse_x, y = load_pickle(path)
            # save to files to not fill memory
            for idx,sparse_x in enumerate(list_sparse_x):
                if use_features is not None and idx not in use_features:
                    continue
                to_sparse = idx in textual_idx

                if idx_path == 0:
                    indices[idx] = len(self_x)
                    self_x.append([])

                filenames = []
                for i in trange(sparse_x.shape[0]):
                    f = f"{self_tmp_file}_{idx}_{i}"
                    v = torch.from_numpy(sparse_x[i,...].toarray()[0])
                    if v.isnan().any():
                        raise Exception("Input nan, no implemented solutions")
                    if to_sparse:
                        vocab_dim[idx] = v.shape[0]
                        v = (v>0).nonzero()[:,0] + 1
                        max_nonzero[idx] = max(len(v), max_nonzero[idx])
                    torch.save(v, f)
                    filenames.append(f)

                self_x[indices[idx]].extend(filenames)
            self_y.append(y)

        # load additional data
        if use_bodmas:
            assert use_features == [0]
            if len(self_x) == 0:
                indices[0] = len(self_x)
                self_x.append([])

            filenames = []

            # bodmas dataset
            filename = 'data/bodmas/bodmas.npz'
            data = np.load(filename)
            X = data['X']  # all the feature vectors
            y = data['y']  # labels, 0 as benign, 1 as malicious
            self_y.append(y)
            del y
            del data

            # save to files
            self_x[indices[0]].extend(save_files(X, f"{SparseDataset.base_tmp_file}_bodmas"))
            del X
        
        if use_ember:
            assert use_features == [0]
            if len(self_x) == 0:
                indices[0] = len(self_x)
                self_x.append([])

            X_train, y_train, X_test, y_test = ember.read_vectorized_features("data/ember2018/")
            self_y.append(y_train)
            self_y.append(y_test)
            del y_train
            del y_test
            
            # save to files
            self_x[indices[0]].extend(save_files(X_train, f"{SparseDataset.base_tmp_file}_emberTrain"))
            del X_train
            self_x[indices[0]].extend(save_files(X_test, f"{SparseDataset.base_tmp_file}_emberTest"))
            del X_test
        self_y = np.concatenate(self_y, axis=0)

        # Filter unlabeled data
        train_rows = (self_y != -1)
        self_y = self_y[train_rows]
        self_x = [np.asarray(x)[train_rows] for x in self_x]

        # # check no nan
        # assert torch.sum([np.isnan(k).sum() for k in self_x]) == 0
        # check that shapes match
        assert len(self_y) == len(self_x[0]), "Shapes do not match"
        # check that no unlabeled data
        assert np.logical_and(self_y != 0, self_y !=1).sum() == 0, "Unknown labels, it should be 0 or 1"

        # divide into train and validation
        xy1, xy2 = split_arrays(self_x + [self_y], lengths, seed=seed)

        return SparseDataset(xy1[:-1], xy1[-1], max_nonzero,vocab_dim, use_features), SparseDataset(xy2[:-1], xy2[-1], max_nonzero,vocab_dim, use_features)

    def __init__(self, x, y, max_nonzero, vocab_dim, use_features):
        self.x = x
        self.targets = y
        self.max_nonzero = max_nonzero
        self.vocab_dim = vocab_dim
        self.use_features = use_features

    def __len__(self):
        return len(self.x[0])

    def feature_dims(self):
        r = [torch.load(x[0]).shape[0] for x in self.x]
        r = [k if idx not in self.vocab_dim else self.vocab_dim[idx] for k,idx in zip(r, self.use_features)]
        return r

    def __getitem__(self, index):
        # return [x[index].astype(np.float32) for x in self.x], self.targets[index]
        if self.targets[index] == 1:
            index_p = (self.targets[:index+1] == 1).sum()-1
        else:
            index_p = -(self.targets[:index+1] == 0).sum()
        x = [torch.load(x[index]).float() for x in self.x]
        x = [torch.nn.functional.pad(k, (max(0, self.max_nonzero[self.use_features[idx]] - len(k)), 0)) for idx,k in enumerate(x)]
        
        return x, self.targets[index], index_p


class DetectorModel(pl.LightningModule):
    def __init__(
        self,
        use_features,
        features_dim,
        args,
        num_epochs,
        pos_samples=None,
        textual_idx=None,
        **kwargs,
    ):
        super().__init__()
        # random input to build computational graph
        # self.example_input_array = torch.zeros((1, ))

        # save hyperparameters as attribute
        self.save_hyperparameters(ignore=['model'])
        self.pos_samples = pos_samples
        self.threshold = 0.5
        self.num_epochs = num_epochs
        self.min_tpr = 0.95
        self.max_fpr = 0.01
        self.first = False
        
        self.args = args
        if args.arch == 'attention':
            self.model = AttentionModelv2(
                features_dim=features_dim,
                use_features=use_features,
                textual_idx=textual_idx,
                hidden_dim=args.hidden_dim[0],
                intermediate_dim=None,
                n_layers=args.n_layers,
                num_heads=args.num_heads,
                dropout_rate=args.dropout_rate,
                att_dropout=args.att_dropout,
            )
        elif args.arch == 'ff':
            self.model = FeedForward(
                features_dim=features_dim,
                hidden_dim=args.hidden_dim,
                dropout_rate=args.dropout_rate,
            )
        else:
            raise NotImplementedError()
        print(f"Using model: {self.model.__class__.__name__}")

        self.batch_size = self.args.batch_size
        ## infer learning rate
        self.init_lr = self.args.lr
        # self.init_lr = self.args.lr * self.batch_size / 256
        self.lr = self.init_lr
        print('initial learning rate:', self.args.lr)

        ##################
        # METRICS
        ##################
        if args.loss_type == 'bce':
            self.criterion = nn.BCEWithLogitsLoss()
        elif args.loss_type == 'mse':
            self.criterion = nn.MSELoss()
        elif args.loss_type == 'auc':
            self.criterion = AUCMLoss()
        elif args.loss_type == 'pauc':
            # todo check values
            self.criterion = tpAUC_KL_Loss(self.pos_samples, Lambda=args.lbda, tau=args.tau)
        else:
            raise NotImplementedError()

        self.train_auc = torchmetrics.AUROC(task='binary', max_fpr=0.01)#, thresholds=self.args.n_tries)
        self.train_roc = torchmetrics.ROC(task='binary')
        self.val_auc = torchmetrics.AUROC(task='binary', max_fpr=0.01)#, thresholds=self.args.n_tries)
        self.val_roc = torchmetrics.ROC(task='binary')

    def configure_optimizers(self):
        ## optimizer
        if self.args.loss_type == 'auc':
            optimizer = PESG(
                self.model, 
                loss_fn=self.criterion, 
                lr=self.args.lr, 
                momentum=self.args.momentum,
                weight_decay=self.args.weight_decay,
            )
        elif self.args.loss_type == 'pauc':
            optimizer = SOTAs(
                self.parameters(), 
                loss_fn=self.criterion, 
                lr=self.args.lr, 
                mode='adam',
                gammas=self.args.gammas,
                weight_decay=self.args.weight_decay,
            )
        else:
            if self.args.optimizer == 'sgd':
                optimizer = torch.optim.SGD(self.parameters(), self.args.lr,
                                                weight_decay=self.args.weight_decay,
                                                momentum=self.args.momentum)
            elif self.args.optimizer == 'adamw':
                optimizer = torch.optim.AdamW(self.parameters(), self.args.lr,
                                        weight_decay=self.args.weight_decay)
            else:
                raise NotImplementedError("Optimizer not implemented")

        ## lr scheduler
        # scheduler = get_linear_schedule_with_warmup(optimizer,num_training_steps=n_epochs, num_warmup_steps=100)
        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=scheduler_patience)
        # scheduler = torch.optim.lr_scheduler.LambdaLR()
        # lr_scheduler_config = {
        #     "scheduler": lr_scheduler,
        #     "frequency": 1,
        #     "monitor": "val_loss",
        #     # If set to `True`, will enforce that the value specified 'monitor'
        #     # is available when the scheduler is updated, thus stopping
        #     # training if not found. If set to `False`, it will only produce a warning
        #     "strict": True,
        #     "name": None,
        # }

        return {
            'optimizer': optimizer,
            # 'lr_scheduler': lr_scheduler_config,
        }
    
    # def lr_scheduler_step(self, scheduler, metric):
    #     pass
        
    def forward(self, x):
        return self.model(x, given_idx=True)

    # def on_train_epoch_start(self):
    #     # adjust learning rate and momentum coefficient per iteration
    #     adjust_learning_rate(self.optimizers(), self.init_lr, self.current_epoch, self.args)

    def training_step(self, batch, batch_idx):
        adjust_learning_rate(self.optimizers(), self.init_lr, self.current_epoch + batch_idx/self.num_epochs, self.args)

        # run through model
        x, target, index = batch

        # if not self.first:
        #     self.logger.log_graph(model=self.model, input_array=x)
        #     self.first = True

        output = self(x)
        if output.isnan().any():
            warnings.warn("Nan values being generated")
            # output = self(x)
        
        if self.args.loss_type=='pauc':
            # loss = self.criterion(torch.sigmoid(output).float(), target, index.long())
            loss = self.criterion(torch.sigmoid(output).float(), target, index[:self.pos_samples].long())
        elif self.args.loss_type=='mse':
            loss = self.criterion(torch.sigmoid(output).float(), target.float())
        else:
            loss = self.criterion(output, target)
        if loss.isnan():
            warnings.warn("Getting nan loss")

        # calculate metrics
        y_pred = torch.sigmoid(output)
        self.train_auc(y_pred.float(), target.int())
        # score = competition_scorer(
        #     y_true=target.int().detach().cpu().numpy(), 
        #     y_pred=y_pred.detach().cpu().numpy(),
        #     threshold=self.threshold,
        #     max_fpr=0.001, 
        #     min_tpr=0.95,
        # )[0]
        if self.current_epoch * self.num_epochs + batch_idx % 20 == 0:
            self.train_score = find_best_score(
                target.int().detach().cpu().numpy(), 
                y_pred.float().detach().cpu().numpy(), 
                return_threshold=False,
            )

        # log loss for training step and average loss for epoch
        self.log_dict({
            "train_loss": loss,
            "train_auc": self.train_auc,
            "train_score": self.train_score,
        }, on_step=True, on_epoch=True, prog_bar=True, logger=True)

        ## compute gradient and do SGD step
        ## automatically done by lightning
        ## can be disabled (https://lightning.ai/docs/pytorch/stable/model/manual_optimization.html)
        # optimizer.zero_grad()
        # outputs = model(input)
        # loss = loss_f(output, labels)
        # loss.backward()
        # optimizer.step()
        
        return loss
    
    # def on_validation_start(self) -> None:
    #     # check that pretrained weights have not changed
    #     self.model.sanity_check(self.args.pretrained, verbose=False)
    #     return super().on_validation_start()

    def validation_step(self, batch, batch_idx):   
        # run through model
        x, target, index = batch
        output = self(x)

        # if self.args.loss_type=='pauc':
        #     loss = self.criterion(output, target, index)
        # elif self.args.loss_type=='mse':
        #     loss = self.criterion(torch.sigmoid(output).float(), target.float())
        # else:
        #     loss = self.criterion(output, target)

        # calculate metrics
        y_pred = torch.sigmoid(output)
        self.val_auc.update(y_pred.float(), target.int())
        self.val_roc.update(y_pred.float(), target.int())
        # score, self.threshold, full_scores = find_best_score(
        #     target.int().cpu().numpy(), 
        #     y_pred.float().detach().cpu().numpy(), 
        #     return_threshold=True,
        #     return_full_scores=True,
        # )

        ## log loss for training step and average loss for epoch
        self.log_dict({
            # "val_loss": loss,
            "val_auc": self.val_auc,
            # "val_score": score,
            # "val_f1": full_scores[1],
            # "val_fpr": full_scores[2],
            # "val_tpr": full_scores[3],
        }, on_step=False, on_epoch=True, prog_bar=True, logger=True)

    def on_validation_epoch_end(self):
        fpr, tpr, thresholds = [k.detach().cpu() for k in self.val_roc.compute()]
        self.logger.experiment.add_figure('ROC Curve', plot_roc(tpr, fpr), global_step=self.global_step)

        score = ((tpr - self.min_tpr) + (self.max_fpr - fpr)) / (1 - self.min_tpr + self.max_fpr)
        idx = score.argmax()
        self.threshold = thresholds[idx]

        self.log_dict({
            "val_score": score[idx],
            "val_fpr": fpr[idx],
            "val_tpr": tpr[idx],
        }, on_step=False, on_epoch=True, prog_bar=True, logger=True)

    # def on_test_start(self) -> None:
    #     self.preds = []
    #     self.targets = []

    # def test_step(self, batch, batch_idx):
    #     # run through model
    #     images, target = batch
    #     output = self(images)
    #     y_pred = torch.sigmoid(output)

    #     self.preds.append(y_pred.detach().cpu())
    #     self.targets.append(target.detach().cpu())

    # def on_test_epoch_end(self) -> None:
    #     self.preds = torch.cat(self.preds, dim=0)
    #     self.targets = torch.cat(self.targets, dim=0)
    #     torch.save(self.preds, os.path.join(self.save_path, "preds.pt"))
    #     torch.save(self.targets, os.path.join(self.save_path, "targets.pt"))


def adjust_learning_rate(optimizer, init_lr, epoch, args, min_lr=0):
    """Decays the learning rate with half-cycle cosine after warmup"""
    warmup_epochs = args.warmup_epochs
    if warmup_epochs < 1:
        warmup_epochs *= args.epochs

    if epoch < warmup_epochs:
        lr = (init_lr-min_lr) * epoch / warmup_epochs  + min_lr
    else:
        lr = init_lr * 0.5 * (1. + math.cos(math.pi * (epoch - warmup_epochs) / (args.epochs - warmup_epochs)))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return lr


def plot_roc(tpr,fpr):
    fig = plt.figure()
    plt.plot(fpr, tpr)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    return fig


def main():
    ###########################
    # PARAMETERS
    ###########################
    args = parser.parse_args()
    # os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

    if args.debug:
        args.workers = 0
    
    # seed for reproducibility
    if args.seed is not None:
        warnings.warn(f"You have seeded training with seed {args.seed}")
        pl.seed_everything(args.seed, workers=True)
    else:
        warnings.warn(f"You have not seeded training")

    if not torch.cuda.is_available():
        warnings.warn("No GPU available: training will be extremely slow")

    ###########################
    # DATASET
    ###########################

    # dataset1 = SparseDataset('data/train.pkl', use_features=use_features, use_ember=False)
    # dataset2 = SparseDataset('data/test.pkl', use_features=use_features, use_ember=False)
    # full_dataset = data.ConcatDataset([dataset1, dataset2])
    # train_dataset, val_dataset = split_generator_dataset(full_dataset, [0.8, 0.2], args.seed)

    # files = list(glob.glob('data_features_combined/**/*_train.pkl', recursive=True))
    # files.extend(list(glob.glob('data_features_combined/**/*_test.pkl', recursive=True)))
    # files = [f for f in files if os.path.isfile(f)]
    files = []
    for k in ['full']:
        for t in ['train', 'test']:
            l = list(glob.glob(f"data_features_combined/**/{k}_{t}.pkl", recursive=True))
            l = [f for f in l if os.path.isfile(f)]
            if len(l) == 0:
                raise Exception("Some files are emtpy")
            files.extend(l)

    use_features = [1,2,3]
    use_textual = [2,3,4]
    # use_features = [2]
    # use_textual = [2]
    data_paths = files
    feature_extractor = load_pickle('data_features_combined/full_feature_extractor.pkl')
    use_ember = False
    use_bodmas = False

    train_dataset, val_dataset = SparseDataset.get_datasets(
        data_paths, 
        seed=args.seed, 
        use_features=use_features, 
        use_ember=use_ember, 
        use_bodmas=use_bodmas,
        lengths=[0.9,0.1],
        textual_idx=use_textual,
    )

    test_dataset = PEDataset('data/testData', feature_extractor, use_features)

    sampler = DualSampler(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, sampling_rate=0.5)
    train_dataloader = DataLoader(
        train_dataset, 
        sampler=sampler,
        batch_size=args.batch_size, 
        # shuffle=True,
        shuffle=False,
        num_workers=args.workers, 
        # pin_memory=not args.debug,
        drop_last=not args.debug,
    )

    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.workers, 
        pin_memory=False,
        drop_last=False,
    )

    test_dataloader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.workers, 
        pin_memory=False,
        drop_last=False,
    )

    ###########################
    # MODEL
    ###########################

    base_dir = os.path.join(args.arch, args.loss_type, str(use_features))
    if args.resume is None:
        logger = pl.loggers.TensorBoardLogger(
            save_dir=args.save_dir,
            name=base_dir, 
            # todo! use log graph
            # log_graph=True,
        )
    else:
        logdir = args.resume.split('/')[1:-1]
        logger = pl.loggers.TensorBoardLogger(
            save_dir=args.save_dir,
            name=os.path.join(*logdir[:-1]),
            version=logdir[-1],
            # log_graph=True,
        )

    # adjust path for pretrain according to model
    # args.pretrained = os.path.join(args.save_dir, base_dir, "pretrain", args.pretrained)

    # load pretrained model
    # pretrained_model = SogModel.load_from_checkpoint(args.pretrained).model

    num_pos = (train_dataset.targets==1).sum()
    num_neg = (train_dataset.targets==0).sum()
    print("-"*10)
    print(f"Total: {num_pos + num_neg}")
    print(f"Positive {num_pos/(num_pos + num_neg)}")
    print(f"Negative {num_neg/(num_pos + num_neg)}")
    print("-"*10)
    print(train_dataset.feature_dims())
    print("-"*10)
    # task to do
    model_task = DetectorModel(
        use_features=use_features,
        features_dim=train_dataset.feature_dims(),
        textual_idx=use_textual,
        args=args,
        # pos_samples=num_pos,
        pos_samples=sampler.pos_len,
        num_epochs=len(train_dataloader),
    )

    ###########################
    # CALLBACKS
    ###########################

    callbacks = [
        pl.callbacks.LearningRateMonitor(),
        # pl.callbacks.DeviceStatsMonitor(),  # monitors and logs device stats, useful to find memory usage
    ]

    save_path = logger.log_dir

    ## callback for saving checkpoints
    checkpoint_cb_every = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="last",
        monitor="step",
        mode="max",
        save_top_k=1,
        every_n_epochs=args.save_every_epochs,
        # save_on_train_epoch_end=True,                         # when using a training metric
        # train_time_interval=,
        # every_n_train_steps=,
        # save_last=False,                                        # save last might be useful to have
    )
    callbacks.append(checkpoint_cb_every)

    checkpoint_cb_bestk = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="best_auc",
        save_top_k=1, 
        monitor='val_auc',
        mode='max',
        # save_on_train_epoch_end=False,     # when using a training metric
        # save_last=False,
    )
    callbacks.append(checkpoint_cb_bestk)

    checkpoint_cb_bestk = pl.callbacks.ModelCheckpoint(
        dirpath=save_path, 
        filename="best",
        save_top_k=2, 
        monitor='val_score',
        mode='max',
        # save_on_train_epoch_end=False,     # when using a training metric
        # save_last=False,
    )
    callbacks.append(checkpoint_cb_bestk)

    ## early stopping
    # early_stopping = pl.callbacks.EarlyStopping(
    #     monitor='val_auc',
    #     mode='max',
    #     patience=args.early_stopping_patience,
    #     verbose=True,
    # )
    # callbacks.append(early_stopping)

    ###########################
    # TRAINER
    ###########################

    # may increase performance but lead to unstable training
    torch.set_float32_matmul_precision("high")
    trainer = pl.Trainer(
        accelerator='gpu' if not args.debug else 'cpu',
        deterministic="warn" if args.seed is not None else False,
        precision="16-mixed",   # reduce memory, can improve performance but might lead to unstable training
        
        max_epochs=args.epochs,
        # max_time="00:1:00:00",
        # max_steps=,

        check_val_every_n_epoch=args.evaluate_every,
        # val_check_interval=1.0,
        logger=logger,
        log_every_n_steps=10,
        callbacks=callbacks,

        fast_dev_run=args.debug,   # for testing training and validation
        num_sanity_val_steps=0,
        limit_train_batches=1.0 if not args.debug else 0.01,  # to test what happens after an epoch
        # overfit_batches=0.01,

        # profiler='simple',    # advanced profiling to check for bottlenecks
    )

    ###########################
    # RUN MODEL
    ###########################

    # ## call tune to find lr and batch size
    # from lightning.pytorch.tuner import Tuner
    # tuner = pl.tuner.Tuner(trainer)
    # lr_finder = tuner.lr_find(model_task, train_dataloaders=train_dataloader)
    # print(lr_finder.results)
    # fig = lr_finder.plot(suggest=True)
    # fig.show()
    # # new_lr = lr_finder.suggestion()
    # # batch_size = tuner.scale_batch_size(model_task, train_dataloaders=train_dataloader)
    # return

    # fit the model
    if args.skip_training is None:
        print("Fitting model...")
        trainer.fit(
            model=model_task,
            train_dataloaders=train_dataloader,
            # todo! change
            # val_dataloaders=val_dataloader,
            val_dataloaders=test_dataloader,
            ckpt_path=args.resume,
        )

    # get best threshold
    chk_path = checkpoint_cb_bestk.best_model_path if args.skip_training is None else args.skip_training
    model = DetectorModel.load_from_checkpoint(chk_path).model
    score, threshold = ThresholdSelector()(model, val_dataloader, n_tries=args.n_tries)
    print(f"Best score: {score}")
    print(f"Best threshold: {threshold}")
    wrapper = TorchModel(feature_extractor, model, use_features=use_features)
    wrapper.best_threshold = threshold
    wrapper.train = False
    wrapper.save(args.save)

    ## test model
    # trainer.test(
    #     model=model,
    #     dataloaders=test_dataloader, 
    #     ckpt_path=os.path.join(save_path, "best.ckpt"),
    #     verbose=True,
    # )


if __name__ == '__main__':
    main()
